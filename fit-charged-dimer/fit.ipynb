{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998cf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83178f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06b72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cace\n",
    "from cace.representations.cace_representation import Cace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b78f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read,write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4afc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xyz_dir = '../lode-datasets/train-id0.xyz'\n",
    "test_xyz_dir = '../lode-datasets/test-id0.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ccc8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ase_xyz = read(train_xyz_dir, ':')\n",
    "test_ase_xyz = read(test_xyz_dir, ':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f191877",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_list = cace.tools.get_unique_atomic_number(train_ase_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a746a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = cace.tasks.get_dataset_from_xyz(train_path=train_xyz_dir,\n",
    "                                 valid_path=test_xyz_dir,\n",
    "                                cutoff=cutoff,\n",
    "                                 data_key={'energy': 'inter_energy', \n",
    "                                           'forces': 'forces',\n",
    "                                          'distance': 'distance'},\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41391e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = cace.tasks.load_data_loader(collection=collection,\n",
    "                              data_type='train',\n",
    "                              batch_size=batch_size,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b676f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = cace.tasks.load_data_loader(collection=collection,\n",
    "                              data_type='valid',\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc6163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cace.tools.init_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e59cc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb2e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4034789d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(atomic_numbers=[69], batch=[69], cell=[9, 3], distance=[3], edge_index=[2, 804], energy=[3], forces=[69, 3], positions=[69, 3], ptr=[4], shifts=[804, 3], unit_shifts=[804, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e7c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.modules import CosineCutoff, MollifierCutoff, PolynomialCutoff\n",
    "from cace.modules import BesselRBF, GaussianRBF, GaussianRBFCentered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "692c8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_basis = BesselRBF(cutoff=cutoff, n_rbf=6, trainable=False)\n",
    "cutoff_fn = PolynomialCutoff(cutoff=cutoff, p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f97e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cace_representation = Cace(\n",
    "    zs=element_list,\n",
    "    n_atom_basis=3,\n",
    "    cutoff=cutoff,\n",
    "    cutoff_fn=cutoff_fn,\n",
    "    radial_basis=radial_basis,\n",
    "    n_radial_basis=8,\n",
    "    max_l=2,\n",
    "    max_nu=2,\n",
    "    num_message_passing=1,\n",
    "    device=device,\n",
    "    timeit=False,\n",
    "    forward_features=['atomic_charge']\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9cb765c-9e5e-4d00-b407-176b9298624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = cace.modules.Atomwise(\n",
    "    n_layers=3,\n",
    "    n_hidden=[24,12],\n",
    "    n_out=1,\n",
    "    feature_key = ['node_feats'], \n",
    "    per_atom_output_key='q',\n",
    "    output_key = 'tot_q',\n",
    "    residual=False,\n",
    "    add_linear_nn=False,\n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a92760-6b92-4adc-882f-1c764df6afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.modules import EwaldPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e4a718d-fa8a-47df-9323-f940395b1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = EwaldPotential(dl=3.,\n",
    "                    sigma=1.0,\n",
    "                    feature_key='q',\n",
    "                    output_key='ewald_potential',\n",
    "                    aggregation_mode='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d242ea9-0e90-4117-a7d2-b0fe468ca589",
   "metadata": {},
   "outputs": [],
   "source": [
    "forces_lr = cace.modules.Forces(energy_key='ewald_potential',\n",
    "                                    forces_key='ewald_forces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54142124-26c8-4748-ac65-c74e03b2dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models.atomistic import NeuralNetworkPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6789ace-6f39-4898-a992-3c32146d2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp_lr = NeuralNetworkPotential(\n",
    "    input_modules=None,\n",
    "    representation=cace_representation,\n",
    "    output_modules=[q, ep, forces_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8336abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nnp_lr(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c971a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 12878\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in nnp_lr.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c3823d2-301c-45b7-adcd-61feefe2823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69, 8, 3, 9, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cace_representation(sampled_data)['node_feats'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13195a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomwise = cace.modules.Atomwise(\n",
    "    n_layers=3,\n",
    "    n_hidden=[24,12],\n",
    "    n_out=1,\n",
    "    output_key='CACE_energy_intra',\n",
    "    residual=False,\n",
    "    add_linear_nn=True,\n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f70ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "forces = cace.modules.Forces(energy_key='CACE_energy_intra',\n",
    "                                    forces_key='CACE_forces_intra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cea892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models.atomistic import NeuralNetworkPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09165246",
   "metadata": {},
   "outputs": [],
   "source": [
    "cace_nnp_intra = NeuralNetworkPotential(\n",
    "    input_modules=None,\n",
    "    representation=cace_representation,\n",
    "    output_modules=[atomwise, forces]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1369dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cace_nnp_intra(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "877e1400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(atomic_numbers=[69], batch=[69], cell=[9, 3], distance=[3], edge_index=[2, 804], energy=[3], forces=[69, 3], positions=[69, 3], ptr=[4], shifts=[804, 3], unit_shifts=[804, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7225b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models import CombinePotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "222dab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot1 = {'CACE_energy': 'ewald_potential', \n",
    "        'CACE_forces': 'ewald_forces',\n",
    "        'weight': 1.\n",
    "       }\n",
    "\n",
    "pot2 = {'CACE_energy': 'CACE_energy_intra', \n",
    "        'CACE_forces': 'CACE_forces_intra',\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd937dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_p = CombinePotential([nnp_lr, cace_nnp_intra], [pot1,pot2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bdc3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tasks import GetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d782978",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff4cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_2 = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2100797",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_3 = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7564e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_loss = GetLoss(\n",
    "    target_name='forces',\n",
    "    predict_name= 'CACE_forces',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a667d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_loss_2 = GetLoss(\n",
    "    target_name='forces',\n",
    "    predict_name= 'CACE_forces',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "294f2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tools import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddc2c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_metric = Metrics(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    name='e',\n",
    "    per_atom=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a23ccecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metric = Metrics(\n",
    "    target_name='forces',\n",
    "    predict_name='CACE_forces',\n",
    "    name='f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cc083f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dict = sampled_data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c25f5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_result = combo_p(sampled_dict, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4100cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2423, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_loss(sampled_data_result, sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "248757a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(81.1265, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_loss(sampled_data_result, sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ccfcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tasks.train import TrainingTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "642ab2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 39.5620, Val Loss: 24.3555\n",
      "train_e_mae: 2.459101\n",
      "train_e_rmse: 2.757242\n",
      "train_f_mae: 0.207418\n",
      "train_f_rmse: 0.276471\n",
      "val_e_mae: 2.113706\n",
      "val_e_rmse: 2.114202\n",
      "val_f_mae: 0.109361\n",
      "val_f_rmse: 0.141016\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 17.3320, Val Loss: 28.5694\n",
      "train_e_mae: 0.911952\n",
      "train_e_rmse: 1.114156\n",
      "train_f_mae: 0.117878\n",
      "train_f_rmse: 0.160707\n",
      "val_e_mae: 1.223272\n",
      "val_e_rmse: 1.224121\n",
      "val_f_mae: 0.131490\n",
      "val_f_rmse: 0.164532\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 13.7019, Val Loss: 21.1129\n",
      "train_e_mae: 0.432585\n",
      "train_e_rmse: 0.535124\n",
      "train_f_mae: 0.078556\n",
      "train_f_rmse: 0.119676\n",
      "val_e_mae: 1.178235\n",
      "val_e_rmse: 1.179112\n",
      "val_f_mae: 0.112035\n",
      "val_f_rmse: 0.140437\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 12.8875, Val Loss: 15.7866\n",
      "train_e_mae: 0.364060\n",
      "train_e_rmse: 0.445614\n",
      "train_f_mae: 0.067482\n",
      "train_f_rmse: 0.110534\n",
      "val_e_mae: 1.134327\n",
      "val_e_rmse: 1.135233\n",
      "val_f_mae: 0.095752\n",
      "val_f_rmse: 0.120407\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 10.7675, Val Loss: 11.6467\n",
      "train_e_mae: 0.341500\n",
      "train_e_rmse: 0.416820\n",
      "train_f_mae: 0.062457\n",
      "train_f_rmse: 0.106187\n",
      "val_e_mae: 1.102706\n",
      "val_e_rmse: 1.103632\n",
      "val_f_mae: 0.080341\n",
      "val_f_rmse: 0.102121\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 10.0718, Val Loss: 8.8028\n",
      "train_e_mae: 0.334335\n",
      "train_e_rmse: 0.399133\n",
      "train_f_mae: 0.058285\n",
      "train_f_rmse: 0.101547\n",
      "val_e_mae: 1.068886\n",
      "val_e_rmse: 1.069836\n",
      "val_f_mae: 0.067826\n",
      "val_f_rmse: 0.087512\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 8.5349, Val Loss: 6.8338\n",
      "train_e_mae: 0.330222\n",
      "train_e_rmse: 0.391484\n",
      "train_f_mae: 0.055472\n",
      "train_f_rmse: 0.095528\n",
      "val_e_mae: 1.037938\n",
      "val_e_rmse: 1.038912\n",
      "val_f_mae: 0.057440\n",
      "val_f_rmse: 0.075858\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 6.7653, Val Loss: 5.6434\n",
      "train_e_mae: 0.325476\n",
      "train_e_rmse: 0.385749\n",
      "train_f_mae: 0.053496\n",
      "train_f_rmse: 0.085279\n",
      "val_e_mae: 0.995401\n",
      "val_e_rmse: 0.996415\n",
      "val_f_mae: 0.051183\n",
      "val_f_rmse: 0.068195\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 6.0710, Val Loss: 4.8090\n",
      "train_e_mae: 0.338609\n",
      "train_e_rmse: 0.400821\n",
      "train_f_mae: 0.050227\n",
      "train_f_rmse: 0.075534\n",
      "val_e_mae: 0.969909\n",
      "val_e_rmse: 0.970952\n",
      "val_f_mae: 0.045761\n",
      "val_f_rmse: 0.062179\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 4.3848, Val Loss: 4.3344\n",
      "train_e_mae: 0.346201\n",
      "train_e_rmse: 0.406474\n",
      "train_f_mae: 0.045511\n",
      "train_f_rmse: 0.068356\n",
      "val_e_mae: 0.949773\n",
      "val_e_rmse: 0.950843\n",
      "val_f_mae: 0.042446\n",
      "val_f_rmse: 0.058569\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 3.6818, Val Loss: 4.0793\n",
      "train_e_mae: 0.342641\n",
      "train_e_rmse: 0.402421\n",
      "train_f_mae: 0.041610\n",
      "train_f_rmse: 0.062043\n",
      "val_e_mae: 0.932837\n",
      "val_e_rmse: 0.933933\n",
      "val_f_mae: 0.040280\n",
      "val_f_rmse: 0.056631\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 3.2564, Val Loss: 3.9496\n",
      "train_e_mae: 0.346654\n",
      "train_e_rmse: 0.402576\n",
      "train_f_mae: 0.039186\n",
      "train_f_rmse: 0.057509\n",
      "val_e_mae: 0.915283\n",
      "val_e_rmse: 0.916409\n",
      "val_f_mae: 0.038835\n",
      "val_f_rmse: 0.055766\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 3.0226, Val Loss: 3.8706\n",
      "train_e_mae: 0.346788\n",
      "train_e_rmse: 0.402188\n",
      "train_f_mae: 0.036702\n",
      "train_f_rmse: 0.053895\n",
      "val_e_mae: 0.899182\n",
      "val_e_rmse: 0.900338\n",
      "val_f_mae: 0.037565\n",
      "val_f_rmse: 0.055317\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 2.5884, Val Loss: 3.7954\n",
      "train_e_mae: 0.343929\n",
      "train_e_rmse: 0.398866\n",
      "train_f_mae: 0.034069\n",
      "train_f_rmse: 0.050531\n",
      "val_e_mae: 0.877454\n",
      "val_e_rmse: 0.878650\n",
      "val_f_mae: 0.036579\n",
      "val_f_rmse: 0.054985\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 2.4088, Val Loss: 3.7120\n",
      "train_e_mae: 0.339326\n",
      "train_e_rmse: 0.391916\n",
      "train_f_mae: 0.031927\n",
      "train_f_rmse: 0.047476\n",
      "val_e_mae: 0.857180\n",
      "val_e_rmse: 0.858415\n",
      "val_f_mae: 0.035713\n",
      "val_f_rmse: 0.054544\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 2.1140, Val Loss: 3.6141\n",
      "train_e_mae: 0.329728\n",
      "train_e_rmse: 0.381883\n",
      "train_f_mae: 0.030236\n",
      "train_f_rmse: 0.045324\n",
      "val_e_mae: 0.830229\n",
      "val_e_rmse: 0.831515\n",
      "val_f_mae: 0.035048\n",
      "val_f_rmse: 0.054062\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 2.0102, Val Loss: 3.5339\n",
      "train_e_mae: 0.321559\n",
      "train_e_rmse: 0.371890\n",
      "train_f_mae: 0.029001\n",
      "train_f_rmse: 0.043581\n",
      "val_e_mae: 0.804147\n",
      "val_e_rmse: 0.805486\n",
      "val_f_mae: 0.034333\n",
      "val_f_rmse: 0.053713\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 1.9671, Val Loss: 3.4698\n",
      "train_e_mae: 0.314565\n",
      "train_e_rmse: 0.364385\n",
      "train_f_mae: 0.028102\n",
      "train_f_rmse: 0.042738\n",
      "val_e_mae: 0.775345\n",
      "val_e_rmse: 0.776742\n",
      "val_f_mae: 0.033774\n",
      "val_f_rmse: 0.053539\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 1.8725, Val Loss: 3.4200\n",
      "train_e_mae: 0.307460\n",
      "train_e_rmse: 0.357191\n",
      "train_f_mae: 0.027408\n",
      "train_f_rmse: 0.042033\n",
      "val_e_mae: 0.748496\n",
      "val_e_rmse: 0.749951\n",
      "val_f_mae: 0.033276\n",
      "val_f_rmse: 0.053456\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 1.7958, Val Loss: 3.3747\n",
      "train_e_mae: 0.301737\n",
      "train_e_rmse: 0.348165\n",
      "train_f_mae: 0.026615\n",
      "train_f_rmse: 0.041252\n",
      "val_e_mae: 0.723019\n",
      "val_e_rmse: 0.724530\n",
      "val_f_mae: 0.033023\n",
      "val_f_rmse: 0.053383\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 1.7240, Val Loss: 3.3296\n",
      "train_e_mae: 0.292125\n",
      "train_e_rmse: 0.338640\n",
      "train_f_mae: 0.026066\n",
      "train_f_rmse: 0.040637\n",
      "val_e_mae: 0.698819\n",
      "val_e_rmse: 0.700384\n",
      "val_f_mae: 0.032872\n",
      "val_f_rmse: 0.053283\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 1.7046, Val Loss: 3.2828\n",
      "train_e_mae: 0.282312\n",
      "train_e_rmse: 0.328144\n",
      "train_f_mae: 0.025666\n",
      "train_f_rmse: 0.040109\n",
      "val_e_mae: 0.675303\n",
      "val_e_rmse: 0.676919\n",
      "val_f_mae: 0.032693\n",
      "val_f_rmse: 0.053147\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 1.6138, Val Loss: 3.2383\n",
      "train_e_mae: 0.273183\n",
      "train_e_rmse: 0.315528\n",
      "train_f_mae: 0.025088\n",
      "train_f_rmse: 0.039521\n",
      "val_e_mae: 0.656835\n",
      "val_e_rmse: 0.658490\n",
      "val_f_mae: 0.032323\n",
      "val_f_rmse: 0.052959\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 1.5999, Val Loss: 3.1993\n",
      "train_e_mae: 0.262378\n",
      "train_e_rmse: 0.303178\n",
      "train_f_mae: 0.024669\n",
      "train_f_rmse: 0.038983\n",
      "val_e_mae: 0.640723\n",
      "val_e_rmse: 0.642407\n",
      "val_f_mae: 0.031917\n",
      "val_f_rmse: 0.052788\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 1.5244, Val Loss: 3.1733\n",
      "train_e_mae: 0.250350\n",
      "train_e_rmse: 0.289423\n",
      "train_f_mae: 0.024132\n",
      "train_f_rmse: 0.038325\n",
      "val_e_mae: 0.628440\n",
      "val_e_rmse: 0.630139\n",
      "val_f_mae: 0.031625\n",
      "val_f_rmse: 0.052690\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 1.4655, Val Loss: 3.1688\n",
      "train_e_mae: 0.235146\n",
      "train_e_rmse: 0.271515\n",
      "train_f_mae: 0.023849\n",
      "train_f_rmse: 0.037718\n",
      "val_e_mae: 0.619022\n",
      "val_e_rmse: 0.620722\n",
      "val_f_mae: 0.031717\n",
      "val_f_rmse: 0.052759\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 1.4135, Val Loss: 3.1870\n",
      "train_e_mae: 0.223488\n",
      "train_e_rmse: 0.256802\n",
      "train_f_mae: 0.023322\n",
      "train_f_rmse: 0.037038\n",
      "val_e_mae: 0.611887\n",
      "val_e_rmse: 0.613575\n",
      "val_f_mae: 0.032223\n",
      "val_f_rmse: 0.053015\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 1.3631, Val Loss: 3.2294\n",
      "train_e_mae: 0.210881\n",
      "train_e_rmse: 0.242499\n",
      "train_f_mae: 0.022973\n",
      "train_f_rmse: 0.036448\n",
      "val_e_mae: 0.606250\n",
      "val_e_rmse: 0.607916\n",
      "val_f_mae: 0.032835\n",
      "val_f_rmse: 0.053478\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 1.3215, Val Loss: 3.2923\n",
      "train_e_mae: 0.199760\n",
      "train_e_rmse: 0.228948\n",
      "train_f_mae: 0.022518\n",
      "train_f_rmse: 0.035949\n",
      "val_e_mae: 0.601617\n",
      "val_e_rmse: 0.603250\n",
      "val_f_mae: 0.033429\n",
      "val_f_rmse: 0.054115\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 1.3179, Val Loss: 3.3717\n",
      "train_e_mae: 0.188019\n",
      "train_e_rmse: 0.215473\n",
      "train_f_mae: 0.022392\n",
      "train_f_rmse: 0.035628\n",
      "val_e_mae: 0.597304\n",
      "val_e_rmse: 0.598897\n",
      "val_f_mae: 0.034087\n",
      "val_f_rmse: 0.054891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 5.7771, Val Loss: 5.2805\n",
      "train_e_mae: 0.240880\n",
      "train_e_rmse: 0.299557\n",
      "train_f_mae: 0.054151\n",
      "train_f_rmse: 0.077889\n",
      "val_e_mae: 0.859829\n",
      "val_e_rmse: 0.860391\n",
      "val_f_mae: 0.044241\n",
      "val_f_rmse: 0.067382\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 5.4716, Val Loss: 3.1659\n",
      "train_e_mae: 0.293359\n",
      "train_e_rmse: 0.358200\n",
      "train_f_mae: 0.039894\n",
      "train_f_rmse: 0.059030\n",
      "val_e_mae: 0.543939\n",
      "val_e_rmse: 0.544821\n",
      "val_f_mae: 0.039882\n",
      "val_f_rmse: 0.053564\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 6.7491, Val Loss: 3.0127\n",
      "train_e_mae: 0.341858\n",
      "train_e_rmse: 0.415304\n",
      "train_f_mae: 0.044034\n",
      "train_f_rmse: 0.061343\n",
      "val_e_mae: 0.535997\n",
      "val_e_rmse: 0.536893\n",
      "val_f_mae: 0.037950\n",
      "val_f_rmse: 0.052196\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 2.2281, Val Loss: 2.8468\n",
      "train_e_mae: 0.387364\n",
      "train_e_rmse: 0.475973\n",
      "train_f_mae: 0.044585\n",
      "train_f_rmse: 0.061578\n",
      "val_e_mae: 0.511163\n",
      "val_e_rmse: 0.512097\n",
      "val_f_mae: 0.036047\n",
      "val_f_rmse: 0.050838\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 2.6619, Val Loss: 2.8286\n",
      "train_e_mae: 0.309734\n",
      "train_e_rmse: 0.375930\n",
      "train_f_mae: 0.044407\n",
      "train_f_rmse: 0.061439\n",
      "val_e_mae: 0.511871\n",
      "val_e_rmse: 0.512796\n",
      "val_f_mae: 0.035330\n",
      "val_f_rmse: 0.050653\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 2.1323, Val Loss: 2.7885\n",
      "train_e_mae: 0.234837\n",
      "train_e_rmse: 0.288534\n",
      "train_f_mae: 0.034099\n",
      "train_f_rmse: 0.048055\n",
      "val_e_mae: 0.498266\n",
      "val_e_rmse: 0.499211\n",
      "val_f_mae: 0.034125\n",
      "val_f_rmse: 0.050391\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 1.8484, Val Loss: 2.7706\n",
      "train_e_mae: 0.220855\n",
      "train_e_rmse: 0.266272\n",
      "train_f_mae: 0.028627\n",
      "train_f_rmse: 0.042801\n",
      "val_e_mae: 0.486600\n",
      "val_e_rmse: 0.487553\n",
      "val_f_mae: 0.033083\n",
      "val_f_rmse: 0.050327\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 1.5106, Val Loss: 2.7664\n",
      "train_e_mae: 0.190318\n",
      "train_e_rmse: 0.222513\n",
      "train_f_mae: 0.024931\n",
      "train_f_rmse: 0.039045\n",
      "val_e_mae: 0.472041\n",
      "val_e_rmse: 0.472999\n",
      "val_f_mae: 0.032341\n",
      "val_f_rmse: 0.050425\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 1.3329, Val Loss: 2.7647\n",
      "train_e_mae: 0.167253\n",
      "train_e_rmse: 0.193764\n",
      "train_f_mae: 0.023035\n",
      "train_f_rmse: 0.036989\n",
      "val_e_mae: 0.461018\n",
      "val_e_rmse: 0.461960\n",
      "val_f_mae: 0.031792\n",
      "val_f_rmse: 0.050511\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 1.3365, Val Loss: 2.7424\n",
      "train_e_mae: 0.138900\n",
      "train_e_rmse: 0.162347\n",
      "train_f_mae: 0.022760\n",
      "train_f_rmse: 0.036939\n",
      "val_e_mae: 0.446319\n",
      "val_e_rmse: 0.447236\n",
      "val_f_mae: 0.031224\n",
      "val_f_rmse: 0.050422\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 1.1819, Val Loss: 2.7186\n",
      "train_e_mae: 0.115965\n",
      "train_e_rmse: 0.134869\n",
      "train_f_mae: 0.021119\n",
      "train_f_rmse: 0.035041\n",
      "val_e_mae: 0.431395\n",
      "val_e_rmse: 0.432276\n",
      "val_f_mae: 0.030791\n",
      "val_f_rmse: 0.050316\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 1.2389, Val Loss: 2.6996\n",
      "train_e_mae: 0.088680\n",
      "train_e_rmse: 0.104057\n",
      "train_f_mae: 0.020536\n",
      "train_f_rmse: 0.034261\n",
      "val_e_mae: 0.416973\n",
      "val_e_rmse: 0.417805\n",
      "val_f_mae: 0.030511\n",
      "val_f_rmse: 0.050250\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 1.0563, Val Loss: 2.6728\n",
      "train_e_mae: 0.065638\n",
      "train_e_rmse: 0.078746\n",
      "train_f_mae: 0.019918\n",
      "train_f_rmse: 0.033332\n",
      "val_e_mae: 0.401166\n",
      "val_e_rmse: 0.401942\n",
      "val_f_mae: 0.030315\n",
      "val_f_rmse: 0.050112\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 1.2159, Val Loss: 2.6544\n",
      "train_e_mae: 0.053761\n",
      "train_e_rmse: 0.064777\n",
      "train_f_mae: 0.020511\n",
      "train_f_rmse: 0.033661\n",
      "val_e_mae: 0.383262\n",
      "val_e_rmse: 0.383981\n",
      "val_f_mae: 0.030231\n",
      "val_f_rmse: 0.050070\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.9947, Val Loss: 2.6386\n",
      "train_e_mae: 0.059577\n",
      "train_e_rmse: 0.073040\n",
      "train_f_mae: 0.019985\n",
      "train_f_rmse: 0.032907\n",
      "val_e_mae: 0.368630\n",
      "val_e_rmse: 0.369289\n",
      "val_f_mae: 0.030135\n",
      "val_f_rmse: 0.050023\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.9583, Val Loss: 2.6234\n",
      "train_e_mae: 0.047119\n",
      "train_e_rmse: 0.057732\n",
      "train_f_mae: 0.020087\n",
      "train_f_rmse: 0.032759\n",
      "val_e_mae: 0.350513\n",
      "val_e_rmse: 0.351122\n",
      "val_f_mae: 0.030077\n",
      "val_f_rmse: 0.050002\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.9980, Val Loss: 2.6084\n",
      "train_e_mae: 0.045458\n",
      "train_e_rmse: 0.055294\n",
      "train_f_mae: 0.020553\n",
      "train_f_rmse: 0.033185\n",
      "val_e_mae: 0.332680\n",
      "val_e_rmse: 0.333245\n",
      "val_f_mae: 0.030012\n",
      "val_f_rmse: 0.049974\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.9873, Val Loss: 2.5869\n",
      "train_e_mae: 0.059943\n",
      "train_e_rmse: 0.071158\n",
      "train_f_mae: 0.019640\n",
      "train_f_rmse: 0.031709\n",
      "val_e_mae: 0.312678\n",
      "val_e_rmse: 0.313207\n",
      "val_f_mae: 0.029927\n",
      "val_f_rmse: 0.049888\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.9177, Val Loss: 2.5708\n",
      "train_e_mae: 0.043851\n",
      "train_e_rmse: 0.053753\n",
      "train_f_mae: 0.018734\n",
      "train_f_rmse: 0.030888\n",
      "val_e_mae: 0.294856\n",
      "val_e_rmse: 0.295348\n",
      "val_f_mae: 0.029869\n",
      "val_f_rmse: 0.049835\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.9856, Val Loss: 2.5561\n",
      "train_e_mae: 0.046883\n",
      "train_e_rmse: 0.056414\n",
      "train_f_mae: 0.018660\n",
      "train_f_rmse: 0.030547\n",
      "val_e_mae: 0.278532\n",
      "val_e_rmse: 0.278987\n",
      "val_f_mae: 0.029798\n",
      "val_f_rmse: 0.049782\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.8540, Val Loss: 2.5362\n",
      "train_e_mae: 0.036059\n",
      "train_e_rmse: 0.043984\n",
      "train_f_mae: 0.018050\n",
      "train_f_rmse: 0.029887\n",
      "val_e_mae: 0.260865\n",
      "val_e_rmse: 0.261287\n",
      "val_f_mae: 0.029697\n",
      "val_f_rmse: 0.049679\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.8471, Val Loss: 2.5172\n",
      "train_e_mae: 0.035896\n",
      "train_e_rmse: 0.043362\n",
      "train_f_mae: 0.017920\n",
      "train_f_rmse: 0.029559\n",
      "val_e_mae: 0.244264\n",
      "val_e_rmse: 0.244653\n",
      "val_f_mae: 0.029589\n",
      "val_f_rmse: 0.049572\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.8486, Val Loss: 2.4970\n",
      "train_e_mae: 0.031747\n",
      "train_e_rmse: 0.037298\n",
      "train_f_mae: 0.017541\n",
      "train_f_rmse: 0.029031\n",
      "val_e_mae: 0.228010\n",
      "val_e_rmse: 0.228366\n",
      "val_f_mae: 0.029468\n",
      "val_f_rmse: 0.049445\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.7926, Val Loss: 2.4728\n",
      "train_e_mae: 0.026360\n",
      "train_e_rmse: 0.031474\n",
      "train_f_mae: 0.017067\n",
      "train_f_rmse: 0.028452\n",
      "val_e_mae: 0.211203\n",
      "val_e_rmse: 0.211530\n",
      "val_f_mae: 0.029328\n",
      "val_f_rmse: 0.049275\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.7734, Val Loss: 2.4483\n",
      "train_e_mae: 0.020994\n",
      "train_e_rmse: 0.025293\n",
      "train_f_mae: 0.017024\n",
      "train_f_rmse: 0.028129\n",
      "val_e_mae: 0.195354\n",
      "val_e_rmse: 0.195651\n",
      "val_f_mae: 0.029180\n",
      "val_f_rmse: 0.049092\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.7664, Val Loss: 2.4226\n",
      "train_e_mae: 0.021994\n",
      "train_e_rmse: 0.025724\n",
      "train_f_mae: 0.016672\n",
      "train_f_rmse: 0.027572\n",
      "val_e_mae: 0.180387\n",
      "val_e_rmse: 0.180655\n",
      "val_f_mae: 0.029022\n",
      "val_f_rmse: 0.048887\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.7227, Val Loss: 2.3918\n",
      "train_e_mae: 0.019784\n",
      "train_e_rmse: 0.023115\n",
      "train_f_mae: 0.016520\n",
      "train_f_rmse: 0.027069\n",
      "val_e_mae: 0.164787\n",
      "val_e_rmse: 0.165029\n",
      "val_f_mae: 0.028829\n",
      "val_f_rmse: 0.048627\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.6816, Val Loss: 2.3601\n",
      "train_e_mae: 0.017918\n",
      "train_e_rmse: 0.021859\n",
      "train_f_mae: 0.016128\n",
      "train_f_rmse: 0.026435\n",
      "val_e_mae: 0.150255\n",
      "val_e_rmse: 0.150471\n",
      "val_f_mae: 0.028620\n",
      "val_f_rmse: 0.048347\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.6650, Val Loss: 2.3258\n",
      "train_e_mae: 0.017157\n",
      "train_e_rmse: 0.021080\n",
      "train_f_mae: 0.016024\n",
      "train_f_rmse: 0.025939\n",
      "val_e_mae: 0.136355\n",
      "val_e_rmse: 0.136546\n",
      "val_f_mae: 0.028385\n",
      "val_f_rmse: 0.048033\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.6277, Val Loss: 2.2864\n",
      "train_e_mae: 0.016831\n",
      "train_e_rmse: 0.020838\n",
      "train_f_mae: 0.015769\n",
      "train_f_rmse: 0.025375\n",
      "val_e_mae: 0.122264\n",
      "val_e_rmse: 0.122433\n",
      "val_f_mae: 0.028118\n",
      "val_f_rmse: 0.047659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 7.6791, Val Loss: 14.8258\n",
      "train_e_mae: 0.209372\n",
      "train_e_rmse: 0.286084\n",
      "train_f_mae: 0.041803\n",
      "train_f_rmse: 0.060674\n",
      "val_e_mae: 0.977354\n",
      "val_e_rmse: 0.977414\n",
      "val_f_mae: 0.089849\n",
      "val_f_rmse: 0.117773\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 2.3167, Val Loss: 6.7212\n",
      "train_e_mae: 0.431606\n",
      "train_e_rmse: 0.496485\n",
      "train_f_mae: 0.043278\n",
      "train_f_rmse: 0.062792\n",
      "val_e_mae: 0.609120\n",
      "val_e_rmse: 0.609239\n",
      "val_f_mae: 0.059362\n",
      "val_f_rmse: 0.079687\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 2.5012, Val Loss: 5.7196\n",
      "train_e_mae: 0.275783\n",
      "train_e_rmse: 0.318648\n",
      "train_f_mae: 0.026857\n",
      "train_f_rmse: 0.040207\n",
      "val_e_mae: 0.557923\n",
      "val_e_rmse: 0.558061\n",
      "val_f_mae: 0.054207\n",
      "val_f_rmse: 0.073540\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 1.6935, Val Loss: 4.9690\n",
      "train_e_mae: 0.212404\n",
      "train_e_rmse: 0.278648\n",
      "train_f_mae: 0.029853\n",
      "train_f_rmse: 0.044473\n",
      "val_e_mae: 0.517804\n",
      "val_e_rmse: 0.517961\n",
      "val_f_mae: 0.050150\n",
      "val_f_rmse: 0.068562\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 1.2019, Val Loss: 4.3665\n",
      "train_e_mae: 0.141889\n",
      "train_e_rmse: 0.174066\n",
      "train_f_mae: 0.026343\n",
      "train_f_rmse: 0.038013\n",
      "val_e_mae: 0.485113\n",
      "val_e_rmse: 0.485287\n",
      "val_f_mae: 0.046501\n",
      "val_f_rmse: 0.064273\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 1.1813, Val Loss: 3.8804\n",
      "train_e_mae: 0.126044\n",
      "train_e_rmse: 0.154662\n",
      "train_f_mae: 0.024205\n",
      "train_f_rmse: 0.037084\n",
      "val_e_mae: 0.451213\n",
      "val_e_rmse: 0.451400\n",
      "val_f_mae: 0.043381\n",
      "val_f_rmse: 0.060636\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.9570, Val Loss: 3.4707\n",
      "train_e_mae: 0.107556\n",
      "train_e_rmse: 0.131467\n",
      "train_f_mae: 0.023537\n",
      "train_f_rmse: 0.034717\n",
      "val_e_mae: 0.420155\n",
      "val_e_rmse: 0.420351\n",
      "val_f_mae: 0.040509\n",
      "val_f_rmse: 0.057393\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 1.7746, Val Loss: 3.1568\n",
      "train_e_mae: 0.103810\n",
      "train_e_rmse: 0.126630\n",
      "train_f_mae: 0.021135\n",
      "train_f_rmse: 0.031686\n",
      "val_e_mae: 0.392745\n",
      "val_e_rmse: 0.392945\n",
      "val_f_mae: 0.038098\n",
      "val_f_rmse: 0.054794\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.9883, Val Loss: 2.8812\n",
      "train_e_mae: 0.161477\n",
      "train_e_rmse: 0.191332\n",
      "train_f_mae: 0.024704\n",
      "train_f_rmse: 0.035854\n",
      "val_e_mae: 0.363130\n",
      "val_e_rmse: 0.363330\n",
      "val_f_mae: 0.035925\n",
      "val_f_rmse: 0.052432\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.7450, Val Loss: 2.6498\n",
      "train_e_mae: 0.078472\n",
      "train_e_rmse: 0.097769\n",
      "train_f_mae: 0.018561\n",
      "train_f_rmse: 0.028153\n",
      "val_e_mae: 0.338117\n",
      "val_e_rmse: 0.338316\n",
      "val_f_mae: 0.033914\n",
      "val_f_rmse: 0.050352\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.8362, Val Loss: 2.4487\n",
      "train_e_mae: 0.068408\n",
      "train_e_rmse: 0.083142\n",
      "train_f_mae: 0.018193\n",
      "train_f_rmse: 0.027068\n",
      "val_e_mae: 0.309297\n",
      "val_e_rmse: 0.309492\n",
      "val_f_mae: 0.032230\n",
      "val_f_rmse: 0.048507\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.5454, Val Loss: 2.2916\n",
      "train_e_mae: 0.083255\n",
      "train_e_rmse: 0.100053\n",
      "train_f_mae: 0.019249\n",
      "train_f_rmse: 0.028346\n",
      "val_e_mae: 0.286955\n",
      "val_e_rmse: 0.287145\n",
      "val_f_mae: 0.030874\n",
      "val_f_rmse: 0.047001\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.5121, Val Loss: 2.1464\n",
      "train_e_mae: 0.069012\n",
      "train_e_rmse: 0.081615\n",
      "train_f_mae: 0.017256\n",
      "train_f_rmse: 0.025760\n",
      "val_e_mae: 0.264259\n",
      "val_e_rmse: 0.264442\n",
      "val_f_mae: 0.029670\n",
      "val_f_rmse: 0.045568\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.5889, Val Loss: 2.0075\n",
      "train_e_mae: 0.077676\n",
      "train_e_rmse: 0.092510\n",
      "train_f_mae: 0.017773\n",
      "train_f_rmse: 0.025877\n",
      "val_e_mae: 0.239558\n",
      "val_e_rmse: 0.239737\n",
      "val_f_mae: 0.028558\n",
      "val_f_rmse: 0.044159\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.4465, Val Loss: 1.8966\n",
      "train_e_mae: 0.052316\n",
      "train_e_rmse: 0.063559\n",
      "train_f_mae: 0.014639\n",
      "train_f_rmse: 0.022292\n",
      "val_e_mae: 0.219802\n",
      "val_e_rmse: 0.219975\n",
      "val_f_mae: 0.027679\n",
      "val_f_rmse: 0.042991\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.6617, Val Loss: 1.7974\n",
      "train_e_mae: 0.048730\n",
      "train_e_rmse: 0.055678\n",
      "train_f_mae: 0.014539\n",
      "train_f_rmse: 0.022023\n",
      "val_e_mae: 0.200638\n",
      "val_e_rmse: 0.200802\n",
      "val_f_mae: 0.026936\n",
      "val_f_rmse: 0.041917\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.6624, Val Loss: 1.7018\n",
      "train_e_mae: 0.057390\n",
      "train_e_rmse: 0.069150\n",
      "train_f_mae: 0.016005\n",
      "train_f_rmse: 0.023658\n",
      "val_e_mae: 0.182741\n",
      "val_e_rmse: 0.182897\n",
      "val_f_mae: 0.026162\n",
      "val_f_rmse: 0.040845\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.3812, Val Loss: 1.6124\n",
      "train_e_mae: 0.046168\n",
      "train_e_rmse: 0.053942\n",
      "train_f_mae: 0.014419\n",
      "train_f_rmse: 0.021237\n",
      "val_e_mae: 0.164269\n",
      "val_e_rmse: 0.164419\n",
      "val_f_mae: 0.025524\n",
      "val_f_rmse: 0.039817\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.4235, Val Loss: 1.5353\n",
      "train_e_mae: 0.023968\n",
      "train_e_rmse: 0.028942\n",
      "train_f_mae: 0.014051\n",
      "train_f_rmse: 0.020906\n",
      "val_e_mae: 0.149737\n",
      "val_e_rmse: 0.149880\n",
      "val_f_mae: 0.024953\n",
      "val_f_rmse: 0.038895\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.3526, Val Loss: 1.4622\n",
      "train_e_mae: 0.023553\n",
      "train_e_rmse: 0.028901\n",
      "train_f_mae: 0.012894\n",
      "train_f_rmse: 0.019357\n",
      "val_e_mae: 0.136745\n",
      "val_e_rmse: 0.136881\n",
      "val_f_mae: 0.024376\n",
      "val_f_rmse: 0.037994\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.3149, Val Loss: 1.3925\n",
      "train_e_mae: 0.016378\n",
      "train_e_rmse: 0.019473\n",
      "train_f_mae: 0.011808\n",
      "train_f_rmse: 0.018071\n",
      "val_e_mae: 0.123731\n",
      "val_e_rmse: 0.123861\n",
      "val_f_mae: 0.023831\n",
      "val_f_rmse: 0.037110\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.2901, Val Loss: 1.3245\n",
      "train_e_mae: 0.013311\n",
      "train_e_rmse: 0.015258\n",
      "train_f_mae: 0.011366\n",
      "train_f_rmse: 0.017357\n",
      "val_e_mae: 0.111094\n",
      "val_e_rmse: 0.111216\n",
      "val_f_mae: 0.023299\n",
      "val_f_rmse: 0.036223\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.2659, Val Loss: 1.2582\n",
      "train_e_mae: 0.011878\n",
      "train_e_rmse: 0.013510\n",
      "train_f_mae: 0.010937\n",
      "train_f_rmse: 0.016618\n",
      "val_e_mae: 0.098843\n",
      "val_e_rmse: 0.098958\n",
      "val_f_mae: 0.022803\n",
      "val_f_rmse: 0.035333\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.2453, Val Loss: 1.1936\n",
      "train_e_mae: 0.011398\n",
      "train_e_rmse: 0.012801\n",
      "train_f_mae: 0.010524\n",
      "train_f_rmse: 0.015934\n",
      "val_e_mae: 0.087202\n",
      "val_e_rmse: 0.087311\n",
      "val_f_mae: 0.022311\n",
      "val_f_rmse: 0.034437\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.2283, Val Loss: 1.1302\n",
      "train_e_mae: 0.011194\n",
      "train_e_rmse: 0.012573\n",
      "train_f_mae: 0.010198\n",
      "train_f_rmse: 0.015347\n",
      "val_e_mae: 0.076344\n",
      "val_e_rmse: 0.076447\n",
      "val_f_mae: 0.021797\n",
      "val_f_rmse: 0.033532\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.2143, Val Loss: 1.0684\n",
      "train_e_mae: 0.011080\n",
      "train_e_rmse: 0.012503\n",
      "train_f_mae: 0.009916\n",
      "train_f_rmse: 0.014841\n",
      "val_e_mae: 0.066335\n",
      "val_e_rmse: 0.066434\n",
      "val_f_mae: 0.021275\n",
      "val_f_rmse: 0.032619\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.2028, Val Loss: 1.0082\n",
      "train_e_mae: 0.011100\n",
      "train_e_rmse: 0.012518\n",
      "train_f_mae: 0.009661\n",
      "train_f_rmse: 0.014411\n",
      "val_e_mae: 0.057291\n",
      "val_e_rmse: 0.057385\n",
      "val_f_mae: 0.020751\n",
      "val_f_rmse: 0.031700\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.1934, Val Loss: 0.9497\n",
      "train_e_mae: 0.011179\n",
      "train_e_rmse: 0.012585\n",
      "train_f_mae: 0.009433\n",
      "train_f_rmse: 0.014049\n",
      "val_e_mae: 0.049231\n",
      "val_e_rmse: 0.049323\n",
      "val_f_mae: 0.020253\n",
      "val_f_rmse: 0.030778\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.1857, Val Loss: 0.8933\n",
      "train_e_mae: 0.011279\n",
      "train_e_rmse: 0.012669\n",
      "train_f_mae: 0.009237\n",
      "train_f_rmse: 0.013746\n",
      "val_e_mae: 0.042170\n",
      "val_e_rmse: 0.042260\n",
      "val_f_mae: 0.019766\n",
      "val_f_rmse: 0.029857\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.1794, Val Loss: 0.8390\n",
      "train_e_mae: 0.011381\n",
      "train_e_rmse: 0.012756\n",
      "train_f_mae: 0.009070\n",
      "train_f_rmse: 0.013492\n",
      "val_e_mae: 0.036075\n",
      "val_e_rmse: 0.036163\n",
      "val_f_mae: 0.019273\n",
      "val_f_rmse: 0.028943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 1.7321, Val Loss: 1.3267\n",
      "train_e_mae: 0.163883\n",
      "train_e_rmse: 0.196638\n",
      "train_f_mae: 0.034267\n",
      "train_f_rmse: 0.051796\n",
      "val_e_mae: 0.391911\n",
      "val_e_rmse: 0.392007\n",
      "val_f_mae: 0.022039\n",
      "val_f_rmse: 0.034249\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.7861, Val Loss: 1.2274\n",
      "train_e_mae: 0.168211\n",
      "train_e_rmse: 0.201492\n",
      "train_f_mae: 0.022127\n",
      "train_f_rmse: 0.031479\n",
      "val_e_mae: 0.066826\n",
      "val_e_rmse: 0.067661\n",
      "val_f_mae: 0.023361\n",
      "val_f_rmse: 0.034969\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 2.3860, Val Loss: 1.0903\n",
      "train_e_mae: 0.214573\n",
      "train_e_rmse: 0.266116\n",
      "train_f_mae: 0.021432\n",
      "train_f_rmse: 0.033518\n",
      "val_e_mae: 0.059376\n",
      "val_e_rmse: 0.060280\n",
      "val_f_mae: 0.022139\n",
      "val_f_rmse: 0.032965\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.9876, Val Loss: 0.9817\n",
      "train_e_mae: 0.147567\n",
      "train_e_rmse: 0.180839\n",
      "train_f_mae: 0.019457\n",
      "train_f_rmse: 0.028973\n",
      "val_e_mae: 0.063604\n",
      "val_e_rmse: 0.064430\n",
      "val_f_mae: 0.021014\n",
      "val_f_rmse: 0.031266\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 1.0334, Val Loss: 0.8978\n",
      "train_e_mae: 0.101695\n",
      "train_e_rmse: 0.126625\n",
      "train_f_mae: 0.017586\n",
      "train_f_rmse: 0.025684\n",
      "val_e_mae: 0.069282\n",
      "val_e_rmse: 0.070026\n",
      "val_f_mae: 0.020094\n",
      "val_f_rmse: 0.029882\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.7348, Val Loss: 0.8144\n",
      "train_e_mae: 0.089447\n",
      "train_e_rmse: 0.110710\n",
      "train_f_mae: 0.015036\n",
      "train_f_rmse: 0.022468\n",
      "val_e_mae: 0.069471\n",
      "val_e_rmse: 0.070184\n",
      "val_f_mae: 0.019072\n",
      "val_f_rmse: 0.028451\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.2693, Val Loss: 0.7513\n",
      "train_e_mae: 0.068961\n",
      "train_e_rmse: 0.084695\n",
      "train_f_mae: 0.014553\n",
      "train_f_rmse: 0.021525\n",
      "val_e_mae: 0.073746\n",
      "val_e_rmse: 0.074388\n",
      "val_f_mae: 0.018275\n",
      "val_f_rmse: 0.027309\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.3331, Val Loss: 0.6930\n",
      "train_e_mae: 0.061375\n",
      "train_e_rmse: 0.074573\n",
      "train_f_mae: 0.014110\n",
      "train_f_rmse: 0.021936\n",
      "val_e_mae: 0.074421\n",
      "val_e_rmse: 0.075018\n",
      "val_f_mae: 0.017480\n",
      "val_f_rmse: 0.026218\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.2209, Val Loss: 0.6444\n",
      "train_e_mae: 0.054178\n",
      "train_e_rmse: 0.066256\n",
      "train_f_mae: 0.012648\n",
      "train_f_rmse: 0.018046\n",
      "val_e_mae: 0.074635\n",
      "val_e_rmse: 0.075191\n",
      "val_f_mae: 0.016841\n",
      "val_f_rmse: 0.025274\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.2860, Val Loss: 0.5989\n",
      "train_e_mae: 0.047579\n",
      "train_e_rmse: 0.057465\n",
      "train_f_mae: 0.011698\n",
      "train_f_rmse: 0.016651\n",
      "val_e_mae: 0.073935\n",
      "val_e_rmse: 0.074453\n",
      "val_f_mae: 0.016251\n",
      "val_f_rmse: 0.024358\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.2758, Val Loss: 0.5579\n",
      "train_e_mae: 0.039215\n",
      "train_e_rmse: 0.047017\n",
      "train_f_mae: 0.010791\n",
      "train_f_rmse: 0.015740\n",
      "val_e_mae: 0.072279\n",
      "val_e_rmse: 0.072764\n",
      "val_f_mae: 0.015631\n",
      "val_f_rmse: 0.023507\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.2339, Val Loss: 0.5229\n",
      "train_e_mae: 0.035552\n",
      "train_e_rmse: 0.043116\n",
      "train_f_mae: 0.010421\n",
      "train_f_rmse: 0.015191\n",
      "val_e_mae: 0.071726\n",
      "val_e_rmse: 0.072173\n",
      "val_f_mae: 0.015126\n",
      "val_f_rmse: 0.022752\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.2283, Val Loss: 0.4890\n",
      "train_e_mae: 0.032588\n",
      "train_e_rmse: 0.039071\n",
      "train_f_mae: 0.009987\n",
      "train_f_rmse: 0.014523\n",
      "val_e_mae: 0.069635\n",
      "val_e_rmse: 0.070055\n",
      "val_f_mae: 0.014558\n",
      "val_f_rmse: 0.022002\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.1984, Val Loss: 0.4603\n",
      "train_e_mae: 0.030556\n",
      "train_e_rmse: 0.036542\n",
      "train_f_mae: 0.009622\n",
      "train_f_rmse: 0.013959\n",
      "val_e_mae: 0.068322\n",
      "val_e_rmse: 0.068712\n",
      "val_f_mae: 0.014081\n",
      "val_f_rmse: 0.021345\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.1845, Val Loss: 0.4332\n",
      "train_e_mae: 0.028458\n",
      "train_e_rmse: 0.033861\n",
      "train_f_mae: 0.009269\n",
      "train_f_rmse: 0.013424\n",
      "val_e_mae: 0.066338\n",
      "val_e_rmse: 0.066703\n",
      "val_f_mae: 0.013591\n",
      "val_f_rmse: 0.020705\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.1644, Val Loss: 0.4095\n",
      "train_e_mae: 0.026678\n",
      "train_e_rmse: 0.031514\n",
      "train_f_mae: 0.008990\n",
      "train_f_rmse: 0.013032\n",
      "val_e_mae: 0.064925\n",
      "val_e_rmse: 0.065265\n",
      "val_f_mae: 0.013178\n",
      "val_f_rmse: 0.020130\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.1572, Val Loss: 0.3868\n",
      "train_e_mae: 0.024625\n",
      "train_e_rmse: 0.028497\n",
      "train_f_mae: 0.008736\n",
      "train_f_rmse: 0.012671\n",
      "val_e_mae: 0.063037\n",
      "val_e_rmse: 0.063355\n",
      "val_f_mae: 0.012744\n",
      "val_f_rmse: 0.019565\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.1679, Val Loss: 0.3676\n",
      "train_e_mae: 0.022854\n",
      "train_e_rmse: 0.025715\n",
      "train_f_mae: 0.008596\n",
      "train_f_rmse: 0.012427\n",
      "val_e_mae: 0.061428\n",
      "val_e_rmse: 0.061725\n",
      "val_f_mae: 0.012386\n",
      "val_f_rmse: 0.019072\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.1287, Val Loss: 0.3485\n",
      "train_e_mae: 0.021754\n",
      "train_e_rmse: 0.024518\n",
      "train_f_mae: 0.008261\n",
      "train_f_rmse: 0.012087\n",
      "val_e_mae: 0.059621\n",
      "val_e_rmse: 0.059899\n",
      "val_f_mae: 0.012000\n",
      "val_f_rmse: 0.018572\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.1236, Val Loss: 0.3314\n",
      "train_e_mae: 0.020304\n",
      "train_e_rmse: 0.022795\n",
      "train_f_mae: 0.007632\n",
      "train_f_rmse: 0.011397\n",
      "val_e_mae: 0.057810\n",
      "val_e_rmse: 0.058070\n",
      "val_f_mae: 0.011686\n",
      "val_f_rmse: 0.018111\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.1206, Val Loss: 0.3155\n",
      "train_e_mae: 0.018835\n",
      "train_e_rmse: 0.021267\n",
      "train_f_mae: 0.007369\n",
      "train_f_rmse: 0.011111\n",
      "val_e_mae: 0.055954\n",
      "val_e_rmse: 0.056197\n",
      "val_f_mae: 0.011388\n",
      "val_f_rmse: 0.017673\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.1182, Val Loss: 0.3010\n",
      "train_e_mae: 0.017802\n",
      "train_e_rmse: 0.020202\n",
      "train_f_mae: 0.007193\n",
      "train_f_rmse: 0.010909\n",
      "val_e_mae: 0.054131\n",
      "val_e_rmse: 0.054358\n",
      "val_f_mae: 0.011120\n",
      "val_f_rmse: 0.017263\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.1147, Val Loss: 0.2877\n",
      "train_e_mae: 0.017161\n",
      "train_e_rmse: 0.019549\n",
      "train_f_mae: 0.007079\n",
      "train_f_rmse: 0.010752\n",
      "val_e_mae: 0.052392\n",
      "val_e_rmse: 0.052604\n",
      "val_f_mae: 0.010867\n",
      "val_f_rmse: 0.016881\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.1124, Val Loss: 0.2757\n",
      "train_e_mae: 0.016806\n",
      "train_e_rmse: 0.019190\n",
      "train_f_mae: 0.007012\n",
      "train_f_rmse: 0.010633\n",
      "val_e_mae: 0.050786\n",
      "val_e_rmse: 0.050983\n",
      "val_f_mae: 0.010633\n",
      "val_f_rmse: 0.016526\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.1103, Val Loss: 0.2649\n",
      "train_e_mae: 0.016641\n",
      "train_e_rmse: 0.019018\n",
      "train_f_mae: 0.006949\n",
      "train_f_rmse: 0.010530\n",
      "val_e_mae: 0.049329\n",
      "val_e_rmse: 0.049513\n",
      "val_f_mae: 0.010427\n",
      "val_f_rmse: 0.016200\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.1085, Val Loss: 0.2551\n",
      "train_e_mae: 0.016590\n",
      "train_e_rmse: 0.018960\n",
      "train_f_mae: 0.006896\n",
      "train_f_rmse: 0.010439\n",
      "val_e_mae: 0.048028\n",
      "val_e_rmse: 0.048201\n",
      "val_f_mae: 0.010244\n",
      "val_f_rmse: 0.015899\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.1069, Val Loss: 0.2462\n",
      "train_e_mae: 0.016597\n",
      "train_e_rmse: 0.018960\n",
      "train_f_mae: 0.006848\n",
      "train_f_rmse: 0.010358\n",
      "val_e_mae: 0.046875\n",
      "val_e_rmse: 0.047037\n",
      "val_f_mae: 0.010083\n",
      "val_f_rmse: 0.015622\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.1055, Val Loss: 0.2382\n",
      "train_e_mae: 0.016631\n",
      "train_e_rmse: 0.018988\n",
      "train_f_mae: 0.006806\n",
      "train_f_rmse: 0.010285\n",
      "val_e_mae: 0.045855\n",
      "val_e_rmse: 0.046007\n",
      "val_f_mae: 0.009930\n",
      "val_f_rmse: 0.015365\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.1042, Val Loss: 0.2309\n",
      "train_e_mae: 0.016676\n",
      "train_e_rmse: 0.019027\n",
      "train_f_mae: 0.006767\n",
      "train_f_rmse: 0.010220\n",
      "val_e_mae: 0.044958\n",
      "val_e_rmse: 0.045101\n",
      "val_f_mae: 0.009788\n",
      "val_f_rmse: 0.015128\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.1031, Val Loss: 0.2242\n",
      "train_e_mae: 0.016728\n",
      "train_e_rmse: 0.019074\n",
      "train_f_mae: 0.006732\n",
      "train_f_rmse: 0.010161\n",
      "val_e_mae: 0.044163\n",
      "val_e_rmse: 0.044299\n",
      "val_f_mae: 0.009664\n",
      "val_f_rmse: 0.014907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 1.4841, Val Loss: 1.2499\n",
      "train_e_mae: 0.120786\n",
      "train_e_rmse: 0.148433\n",
      "train_f_mae: 0.027071\n",
      "train_f_rmse: 0.045593\n",
      "val_e_mae: 0.149349\n",
      "val_e_rmse: 0.149636\n",
      "val_f_mae: 0.024478\n",
      "val_f_rmse: 0.035036\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 1.6615, Val Loss: 1.0779\n",
      "train_e_mae: 0.281668\n",
      "train_e_rmse: 0.332923\n",
      "train_f_mae: 0.026362\n",
      "train_f_rmse: 0.040120\n",
      "val_e_mae: 0.527393\n",
      "val_e_rmse: 0.527517\n",
      "val_f_mae: 0.018705\n",
      "val_f_rmse: 0.028279\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 1.2525, Val Loss: 0.9946\n",
      "train_e_mae: 0.121010\n",
      "train_e_rmse: 0.146375\n",
      "train_f_mae: 0.020137\n",
      "train_f_rmse: 0.029313\n",
      "val_e_mae: 0.484354\n",
      "val_e_rmse: 0.484489\n",
      "val_f_mae: 0.018029\n",
      "val_f_rmse: 0.027565\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.4179, Val Loss: 0.8943\n",
      "train_e_mae: 0.103485\n",
      "train_e_rmse: 0.125193\n",
      "train_f_mae: 0.015843\n",
      "train_f_rmse: 0.022461\n",
      "val_e_mae: 0.449520\n",
      "val_e_rmse: 0.449662\n",
      "val_f_mae: 0.017069\n",
      "val_f_rmse: 0.026307\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.5356, Val Loss: 0.7969\n",
      "train_e_mae: 0.087537\n",
      "train_e_rmse: 0.107731\n",
      "train_f_mae: 0.014073\n",
      "train_f_rmse: 0.020202\n",
      "val_e_mae: 0.414512\n",
      "val_e_rmse: 0.414658\n",
      "val_f_mae: 0.016066\n",
      "val_f_rmse: 0.024999\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.6792, Val Loss: 0.7176\n",
      "train_e_mae: 0.058208\n",
      "train_e_rmse: 0.068844\n",
      "train_f_mae: 0.013745\n",
      "train_f_rmse: 0.021188\n",
      "val_e_mae: 0.383305\n",
      "val_e_rmse: 0.383454\n",
      "val_f_mae: 0.015210\n",
      "val_f_rmse: 0.023887\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.5923, Val Loss: 0.6493\n",
      "train_e_mae: 0.072803\n",
      "train_e_rmse: 0.088828\n",
      "train_f_mae: 0.016316\n",
      "train_f_rmse: 0.022879\n",
      "val_e_mae: 0.353986\n",
      "val_e_rmse: 0.354137\n",
      "val_f_mae: 0.014483\n",
      "val_f_rmse: 0.022888\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.2346, Val Loss: 0.5981\n",
      "train_e_mae: 0.072218\n",
      "train_e_rmse: 0.088189\n",
      "train_f_mae: 0.013830\n",
      "train_f_rmse: 0.019919\n",
      "val_e_mae: 0.329112\n",
      "val_e_rmse: 0.329264\n",
      "val_f_mae: 0.013954\n",
      "val_f_rmse: 0.022128\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.3066, Val Loss: 0.5474\n",
      "train_e_mae: 0.060064\n",
      "train_e_rmse: 0.073328\n",
      "train_f_mae: 0.013651\n",
      "train_f_rmse: 0.019099\n",
      "val_e_mae: 0.305331\n",
      "val_e_rmse: 0.305483\n",
      "val_f_mae: 0.013435\n",
      "val_f_rmse: 0.021309\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.2890, Val Loss: 0.5009\n",
      "train_e_mae: 0.086769\n",
      "train_e_rmse: 0.102781\n",
      "train_f_mae: 0.012633\n",
      "train_f_rmse: 0.017421\n",
      "val_e_mae: 0.281978\n",
      "val_e_rmse: 0.282131\n",
      "val_f_mae: 0.012991\n",
      "val_f_rmse: 0.020525\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.1151, Val Loss: 0.4619\n",
      "train_e_mae: 0.058228\n",
      "train_e_rmse: 0.073282\n",
      "train_f_mae: 0.009130\n",
      "train_f_rmse: 0.013180\n",
      "val_e_mae: 0.262925\n",
      "val_e_rmse: 0.263074\n",
      "val_f_mae: 0.012574\n",
      "val_f_rmse: 0.019816\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.2043, Val Loss: 0.4224\n",
      "train_e_mae: 0.033772\n",
      "train_e_rmse: 0.040808\n",
      "train_f_mae: 0.009706\n",
      "train_f_rmse: 0.013901\n",
      "val_e_mae: 0.243147\n",
      "val_e_rmse: 0.243291\n",
      "val_f_mae: 0.012158\n",
      "val_f_rmse: 0.019059\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.1098, Val Loss: 0.3867\n",
      "train_e_mae: 0.052743\n",
      "train_e_rmse: 0.064095\n",
      "train_f_mae: 0.008997\n",
      "train_f_rmse: 0.012889\n",
      "val_e_mae: 0.224091\n",
      "val_e_rmse: 0.224230\n",
      "val_f_mae: 0.011788\n",
      "val_f_rmse: 0.018341\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.1114, Val Loss: 0.3559\n",
      "train_e_mae: 0.030157\n",
      "train_e_rmse: 0.037131\n",
      "train_f_mae: 0.007864\n",
      "train_f_rmse: 0.011261\n",
      "val_e_mae: 0.207125\n",
      "val_e_rmse: 0.207258\n",
      "val_f_mae: 0.011468\n",
      "val_f_rmse: 0.017690\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.1275, Val Loss: 0.3280\n",
      "train_e_mae: 0.025514\n",
      "train_e_rmse: 0.031404\n",
      "train_f_mae: 0.007652\n",
      "train_f_rmse: 0.010954\n",
      "val_e_mae: 0.191481\n",
      "val_e_rmse: 0.191607\n",
      "val_f_mae: 0.011163\n",
      "val_f_rmse: 0.017067\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.1178, Val Loss: 0.3037\n",
      "train_e_mae: 0.019918\n",
      "train_e_rmse: 0.023831\n",
      "train_f_mae: 0.007321\n",
      "train_f_rmse: 0.010548\n",
      "val_e_mae: 0.177358\n",
      "val_e_rmse: 0.177477\n",
      "val_f_mae: 0.010887\n",
      "val_f_rmse: 0.016500\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0990, Val Loss: 0.2822\n",
      "train_e_mae: 0.021010\n",
      "train_e_rmse: 0.025773\n",
      "train_f_mae: 0.007548\n",
      "train_f_rmse: 0.010769\n",
      "val_e_mae: 0.164410\n",
      "val_e_rmse: 0.164523\n",
      "val_f_mae: 0.010645\n",
      "val_f_rmse: 0.015974\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.1079, Val Loss: 0.2628\n",
      "train_e_mae: 0.018922\n",
      "train_e_rmse: 0.022496\n",
      "train_f_mae: 0.007510\n",
      "train_f_rmse: 0.010614\n",
      "val_e_mae: 0.152263\n",
      "val_e_rmse: 0.152372\n",
      "val_f_mae: 0.010397\n",
      "val_f_rmse: 0.015479\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0996, Val Loss: 0.2460\n",
      "train_e_mae: 0.018166\n",
      "train_e_rmse: 0.021076\n",
      "train_f_mae: 0.006771\n",
      "train_f_rmse: 0.009820\n",
      "val_e_mae: 0.141203\n",
      "val_e_rmse: 0.141306\n",
      "val_f_mae: 0.010164\n",
      "val_f_rmse: 0.015036\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0899, Val Loss: 0.2313\n",
      "train_e_mae: 0.017538\n",
      "train_e_rmse: 0.020156\n",
      "train_f_mae: 0.006482\n",
      "train_f_rmse: 0.009521\n",
      "val_e_mae: 0.131235\n",
      "val_e_rmse: 0.131335\n",
      "val_f_mae: 0.009942\n",
      "val_f_rmse: 0.014629\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0855, Val Loss: 0.2182\n",
      "train_e_mae: 0.017041\n",
      "train_e_rmse: 0.019342\n",
      "train_f_mae: 0.006266\n",
      "train_f_rmse: 0.009290\n",
      "val_e_mae: 0.122036\n",
      "val_e_rmse: 0.122132\n",
      "val_f_mae: 0.009721\n",
      "val_f_rmse: 0.014256\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0835, Val Loss: 0.2067\n",
      "train_e_mae: 0.016678\n",
      "train_e_rmse: 0.018863\n",
      "train_f_mae: 0.006168\n",
      "train_f_rmse: 0.009172\n",
      "val_e_mae: 0.113656\n",
      "val_e_rmse: 0.113748\n",
      "val_f_mae: 0.009513\n",
      "val_f_rmse: 0.013920\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0824, Val Loss: 0.1967\n",
      "train_e_mae: 0.016468\n",
      "train_e_rmse: 0.018581\n",
      "train_f_mae: 0.006113\n",
      "train_f_rmse: 0.009089\n",
      "val_e_mae: 0.106039\n",
      "val_e_rmse: 0.106127\n",
      "val_f_mae: 0.009316\n",
      "val_f_rmse: 0.013616\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0812, Val Loss: 0.1879\n",
      "train_e_mae: 0.016370\n",
      "train_e_rmse: 0.018464\n",
      "train_f_mae: 0.006070\n",
      "train_f_rmse: 0.009022\n",
      "val_e_mae: 0.099123\n",
      "val_e_rmse: 0.099208\n",
      "val_f_mae: 0.009127\n",
      "val_f_rmse: 0.013343\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0803, Val Loss: 0.1802\n",
      "train_e_mae: 0.016317\n",
      "train_e_rmse: 0.018402\n",
      "train_f_mae: 0.006038\n",
      "train_f_rmse: 0.008967\n",
      "val_e_mae: 0.092867\n",
      "val_e_rmse: 0.092949\n",
      "val_f_mae: 0.008961\n",
      "val_f_rmse: 0.013098\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0795, Val Loss: 0.1735\n",
      "train_e_mae: 0.016318\n",
      "train_e_rmse: 0.018398\n",
      "train_f_mae: 0.006009\n",
      "train_f_rmse: 0.008918\n",
      "val_e_mae: 0.087201\n",
      "val_e_rmse: 0.087280\n",
      "val_f_mae: 0.008834\n",
      "val_f_rmse: 0.012880\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0788, Val Loss: 0.1677\n",
      "train_e_mae: 0.016329\n",
      "train_e_rmse: 0.018407\n",
      "train_f_mae: 0.005984\n",
      "train_f_rmse: 0.008875\n",
      "val_e_mae: 0.082072\n",
      "val_e_rmse: 0.082149\n",
      "val_f_mae: 0.008721\n",
      "val_f_rmse: 0.012686\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0781, Val Loss: 0.1626\n",
      "train_e_mae: 0.016352\n",
      "train_e_rmse: 0.018429\n",
      "train_f_mae: 0.005962\n",
      "train_f_rmse: 0.008836\n",
      "val_e_mae: 0.077438\n",
      "val_e_rmse: 0.077513\n",
      "val_f_mae: 0.008622\n",
      "val_f_rmse: 0.012513\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0775, Val Loss: 0.1581\n",
      "train_e_mae: 0.016379\n",
      "train_e_rmse: 0.018455\n",
      "train_f_mae: 0.005942\n",
      "train_f_rmse: 0.008801\n",
      "val_e_mae: 0.073248\n",
      "val_e_rmse: 0.073321\n",
      "val_f_mae: 0.008540\n",
      "val_f_rmse: 0.012359\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0770, Val Loss: 0.1542\n",
      "train_e_mae: 0.016407\n",
      "train_e_rmse: 0.018482\n",
      "train_f_mae: 0.005924\n",
      "train_f_rmse: 0.008770\n",
      "val_e_mae: 0.069458\n",
      "val_e_rmse: 0.069529\n",
      "val_f_mae: 0.008467\n",
      "val_f_rmse: 0.012222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 1.4829, Val Loss: 1.4736\n",
      "train_e_mae: 0.112728\n",
      "train_e_rmse: 0.144980\n",
      "train_f_mae: 0.022790\n",
      "train_f_rmse: 0.033895\n",
      "val_e_mae: 0.523751\n",
      "val_e_rmse: 0.523829\n",
      "val_f_mae: 0.024175\n",
      "val_f_rmse: 0.034630\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.4085, Val Loss: 0.8481\n",
      "train_e_mae: 0.150682\n",
      "train_e_rmse: 0.193622\n",
      "train_f_mae: 0.017597\n",
      "train_f_rmse: 0.024243\n",
      "val_e_mae: 0.256306\n",
      "val_e_rmse: 0.256430\n",
      "val_f_mae: 0.021729\n",
      "val_f_rmse: 0.027971\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.5012, Val Loss: 0.7358\n",
      "train_e_mae: 0.073618\n",
      "train_e_rmse: 0.091290\n",
      "train_f_mae: 0.016720\n",
      "train_f_rmse: 0.025037\n",
      "val_e_mae: 0.242696\n",
      "val_e_rmse: 0.242826\n",
      "val_f_mae: 0.020375\n",
      "val_f_rmse: 0.026015\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.4890, Val Loss: 0.6462\n",
      "train_e_mae: 0.081536\n",
      "train_e_rmse: 0.101404\n",
      "train_f_mae: 0.016628\n",
      "train_f_rmse: 0.024613\n",
      "val_e_mae: 0.229104\n",
      "val_e_rmse: 0.229243\n",
      "val_f_mae: 0.019061\n",
      "val_f_rmse: 0.024365\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.3420, Val Loss: 0.5574\n",
      "train_e_mae: 0.099411\n",
      "train_e_rmse: 0.121750\n",
      "train_f_mae: 0.012742\n",
      "train_f_rmse: 0.018513\n",
      "val_e_mae: 0.217598\n",
      "val_e_rmse: 0.217746\n",
      "val_f_mae: 0.017576\n",
      "val_f_rmse: 0.022582\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.2984, Val Loss: 0.4892\n",
      "train_e_mae: 0.067267\n",
      "train_e_rmse: 0.080180\n",
      "train_f_mae: 0.010967\n",
      "train_f_rmse: 0.016818\n",
      "val_e_mae: 0.206030\n",
      "val_e_rmse: 0.206185\n",
      "val_f_mae: 0.016452\n",
      "val_f_rmse: 0.021134\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.2108, Val Loss: 0.4243\n",
      "train_e_mae: 0.063958\n",
      "train_e_rmse: 0.078615\n",
      "train_f_mae: 0.010446\n",
      "train_f_rmse: 0.014709\n",
      "val_e_mae: 0.193582\n",
      "val_e_rmse: 0.193743\n",
      "val_f_mae: 0.015328\n",
      "val_f_rmse: 0.019666\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1370, Val Loss: 0.3771\n",
      "train_e_mae: 0.050604\n",
      "train_e_rmse: 0.061425\n",
      "train_f_mae: 0.009375\n",
      "train_f_rmse: 0.012973\n",
      "val_e_mae: 0.183486\n",
      "val_e_rmse: 0.183648\n",
      "val_f_mae: 0.014485\n",
      "val_f_rmse: 0.018530\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.3198, Val Loss: 0.3299\n",
      "train_e_mae: 0.041445\n",
      "train_e_rmse: 0.049862\n",
      "train_f_mae: 0.009185\n",
      "train_f_rmse: 0.012925\n",
      "val_e_mae: 0.171521\n",
      "val_e_rmse: 0.171683\n",
      "val_f_mae: 0.013599\n",
      "val_f_rmse: 0.017334\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.1346, Val Loss: 0.2987\n",
      "train_e_mae: 0.042032\n",
      "train_e_rmse: 0.052156\n",
      "train_f_mae: 0.009302\n",
      "train_f_rmse: 0.012985\n",
      "val_e_mae: 0.161257\n",
      "val_e_rmse: 0.161414\n",
      "val_f_mae: 0.012918\n",
      "val_f_rmse: 0.016511\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.1264, Val Loss: 0.2678\n",
      "train_e_mae: 0.027847\n",
      "train_e_rmse: 0.033967\n",
      "train_f_mae: 0.007788\n",
      "train_f_rmse: 0.010741\n",
      "val_e_mae: 0.150714\n",
      "val_e_rmse: 0.150866\n",
      "val_f_mae: 0.012227\n",
      "val_f_rmse: 0.015653\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.1220, Val Loss: 0.2412\n",
      "train_e_mae: 0.040172\n",
      "train_e_rmse: 0.048403\n",
      "train_f_mae: 0.008025\n",
      "train_f_rmse: 0.010878\n",
      "val_e_mae: 0.140601\n",
      "val_e_rmse: 0.140746\n",
      "val_f_mae: 0.011580\n",
      "val_f_rmse: 0.014879\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.1662, Val Loss: 0.2164\n",
      "train_e_mae: 0.070843\n",
      "train_e_rmse: 0.079668\n",
      "train_f_mae: 0.008037\n",
      "train_f_rmse: 0.011388\n",
      "val_e_mae: 0.128506\n",
      "val_e_rmse: 0.128647\n",
      "val_f_mae: 0.010905\n",
      "val_f_rmse: 0.014135\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.1181, Val Loss: 0.2009\n",
      "train_e_mae: 0.030430\n",
      "train_e_rmse: 0.037164\n",
      "train_f_mae: 0.007466\n",
      "train_f_rmse: 0.010802\n",
      "val_e_mae: 0.121429\n",
      "val_e_rmse: 0.121564\n",
      "val_f_mae: 0.010482\n",
      "val_f_rmse: 0.013644\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.1026, Val Loss: 0.1859\n",
      "train_e_mae: 0.024936\n",
      "train_e_rmse: 0.030139\n",
      "train_f_mae: 0.007131\n",
      "train_f_rmse: 0.010068\n",
      "val_e_mae: 0.113877\n",
      "val_e_rmse: 0.114008\n",
      "val_f_mae: 0.010004\n",
      "val_f_rmse: 0.013149\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0939, Val Loss: 0.1740\n",
      "train_e_mae: 0.025016\n",
      "train_e_rmse: 0.030018\n",
      "train_f_mae: 0.006767\n",
      "train_f_rmse: 0.009599\n",
      "val_e_mae: 0.107551\n",
      "val_e_rmse: 0.107677\n",
      "val_f_mae: 0.009619\n",
      "val_f_rmse: 0.012745\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0804, Val Loss: 0.1637\n",
      "train_e_mae: 0.024312\n",
      "train_e_rmse: 0.028666\n",
      "train_f_mae: 0.006584\n",
      "train_f_rmse: 0.009341\n",
      "val_e_mae: 0.101421\n",
      "val_e_rmse: 0.101544\n",
      "val_f_mae: 0.009245\n",
      "val_f_rmse: 0.012384\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0823, Val Loss: 0.1557\n",
      "train_e_mae: 0.023567\n",
      "train_e_rmse: 0.027624\n",
      "train_f_mae: 0.006398\n",
      "train_f_rmse: 0.009036\n",
      "val_e_mae: 0.096358\n",
      "val_e_rmse: 0.096477\n",
      "val_f_mae: 0.008961\n",
      "val_f_rmse: 0.012098\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0788, Val Loss: 0.1478\n",
      "train_e_mae: 0.022845\n",
      "train_e_rmse: 0.026411\n",
      "train_f_mae: 0.006157\n",
      "train_f_rmse: 0.008721\n",
      "val_e_mae: 0.091122\n",
      "val_e_rmse: 0.091239\n",
      "val_f_mae: 0.008691\n",
      "val_f_rmse: 0.011808\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0700, Val Loss: 0.1417\n",
      "train_e_mae: 0.021701\n",
      "train_e_rmse: 0.024775\n",
      "train_f_mae: 0.005916\n",
      "train_f_rmse: 0.008451\n",
      "val_e_mae: 0.086603\n",
      "val_e_rmse: 0.086716\n",
      "val_f_mae: 0.008498\n",
      "val_f_rmse: 0.011583\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0660, Val Loss: 0.1361\n",
      "train_e_mae: 0.020649\n",
      "train_e_rmse: 0.023187\n",
      "train_f_mae: 0.005679\n",
      "train_f_rmse: 0.008205\n",
      "val_e_mae: 0.082205\n",
      "val_e_rmse: 0.082315\n",
      "val_f_mae: 0.008316\n",
      "val_f_rmse: 0.011370\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0659, Val Loss: 0.1314\n",
      "train_e_mae: 0.020009\n",
      "train_e_rmse: 0.022334\n",
      "train_f_mae: 0.005592\n",
      "train_f_rmse: 0.008103\n",
      "val_e_mae: 0.078185\n",
      "val_e_rmse: 0.078292\n",
      "val_f_mae: 0.008158\n",
      "val_f_rmse: 0.011192\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0645, Val Loss: 0.1274\n",
      "train_e_mae: 0.019575\n",
      "train_e_rmse: 0.021733\n",
      "train_f_mae: 0.005536\n",
      "train_f_rmse: 0.008031\n",
      "val_e_mae: 0.074465\n",
      "val_e_rmse: 0.074570\n",
      "val_f_mae: 0.008015\n",
      "val_f_rmse: 0.011039\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0639, Val Loss: 0.1240\n",
      "train_e_mae: 0.019271\n",
      "train_e_rmse: 0.021379\n",
      "train_f_mae: 0.005506\n",
      "train_f_rmse: 0.007986\n",
      "val_e_mae: 0.070996\n",
      "val_e_rmse: 0.071098\n",
      "val_f_mae: 0.007883\n",
      "val_f_rmse: 0.010907\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0634, Val Loss: 0.1211\n",
      "train_e_mae: 0.019057\n",
      "train_e_rmse: 0.021140\n",
      "train_f_mae: 0.005483\n",
      "train_f_rmse: 0.007949\n",
      "val_e_mae: 0.067770\n",
      "val_e_rmse: 0.067869\n",
      "val_f_mae: 0.007771\n",
      "val_f_rmse: 0.010794\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0629, Val Loss: 0.1186\n",
      "train_e_mae: 0.018915\n",
      "train_e_rmse: 0.020982\n",
      "train_f_mae: 0.005464\n",
      "train_f_rmse: 0.007916\n",
      "val_e_mae: 0.064782\n",
      "val_e_rmse: 0.064879\n",
      "val_f_mae: 0.007671\n",
      "val_f_rmse: 0.010697\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0625, Val Loss: 0.1165\n",
      "train_e_mae: 0.018815\n",
      "train_e_rmse: 0.020876\n",
      "train_f_mae: 0.005446\n",
      "train_f_rmse: 0.007888\n",
      "val_e_mae: 0.062011\n",
      "val_e_rmse: 0.062105\n",
      "val_f_mae: 0.007593\n",
      "val_f_rmse: 0.010615\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0621, Val Loss: 0.1147\n",
      "train_e_mae: 0.018743\n",
      "train_e_rmse: 0.020798\n",
      "train_f_mae: 0.005431\n",
      "train_f_rmse: 0.007862\n",
      "val_e_mae: 0.059448\n",
      "val_e_rmse: 0.059540\n",
      "val_f_mae: 0.007524\n",
      "val_f_rmse: 0.010545\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0617, Val Loss: 0.1132\n",
      "train_e_mae: 0.018688\n",
      "train_e_rmse: 0.020741\n",
      "train_f_mae: 0.005418\n",
      "train_f_rmse: 0.007839\n",
      "val_e_mae: 0.057075\n",
      "val_e_rmse: 0.057166\n",
      "val_f_mae: 0.007474\n",
      "val_f_rmse: 0.010485\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0614, Val Loss: 0.1119\n",
      "train_e_mae: 0.018648\n",
      "train_e_rmse: 0.020698\n",
      "train_f_mae: 0.005406\n",
      "train_f_rmse: 0.007818\n",
      "val_e_mae: 0.054885\n",
      "val_e_rmse: 0.054974\n",
      "val_f_mae: 0.007429\n",
      "val_f_rmse: 0.010433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 0.8922, Val Loss: 0.7316\n",
      "train_e_mae: 0.112271\n",
      "train_e_rmse: 0.143623\n",
      "train_f_mae: 0.019964\n",
      "train_f_rmse: 0.031649\n",
      "val_e_mae: 0.017977\n",
      "val_e_rmse: 0.019142\n",
      "val_f_mae: 0.017195\n",
      "val_f_rmse: 0.027042\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.4667, Val Loss: 0.9315\n",
      "train_e_mae: 0.116426\n",
      "train_e_rmse: 0.137059\n",
      "train_f_mae: 0.015709\n",
      "train_f_rmse: 0.022765\n",
      "val_e_mae: 0.016152\n",
      "val_e_rmse: 0.017798\n",
      "val_f_mae: 0.017650\n",
      "val_f_rmse: 0.030516\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2451, Val Loss: 0.7774\n",
      "train_e_mae: 0.090025\n",
      "train_e_rmse: 0.110265\n",
      "train_f_mae: 0.011928\n",
      "train_f_rmse: 0.016854\n",
      "val_e_mae: 0.007085\n",
      "val_e_rmse: 0.008370\n",
      "val_f_mae: 0.016203\n",
      "val_f_rmse: 0.027881\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2073, Val Loss: 0.6631\n",
      "train_e_mae: 0.079702\n",
      "train_e_rmse: 0.097016\n",
      "train_f_mae: 0.010354\n",
      "train_f_rmse: 0.014421\n",
      "val_e_mae: 0.008623\n",
      "val_e_rmse: 0.010099\n",
      "val_f_mae: 0.015146\n",
      "val_f_rmse: 0.025750\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.3017, Val Loss: 0.5719\n",
      "train_e_mae: 0.079102\n",
      "train_e_rmse: 0.095954\n",
      "train_f_mae: 0.011150\n",
      "train_f_rmse: 0.015542\n",
      "val_e_mae: 0.012830\n",
      "val_e_rmse: 0.014717\n",
      "val_f_mae: 0.014228\n",
      "val_f_rmse: 0.023909\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.2485, Val Loss: 0.4989\n",
      "train_e_mae: 0.052790\n",
      "train_e_rmse: 0.064148\n",
      "train_f_mae: 0.009755\n",
      "train_f_rmse: 0.013532\n",
      "val_e_mae: 0.016953\n",
      "val_e_rmse: 0.018287\n",
      "val_f_mae: 0.013476\n",
      "val_f_rmse: 0.022329\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.0903, Val Loss: 0.4336\n",
      "train_e_mae: 0.057867\n",
      "train_e_rmse: 0.068743\n",
      "train_f_mae: 0.009617\n",
      "train_f_rmse: 0.013914\n",
      "val_e_mae: 0.020084\n",
      "val_e_rmse: 0.021117\n",
      "val_f_mae: 0.012721\n",
      "val_f_rmse: 0.020813\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1163, Val Loss: 0.3824\n",
      "train_e_mae: 0.037049\n",
      "train_e_rmse: 0.045352\n",
      "train_f_mae: 0.010506\n",
      "train_f_rmse: 0.014442\n",
      "val_e_mae: 0.023183\n",
      "val_e_rmse: 0.024007\n",
      "val_f_mae: 0.012115\n",
      "val_f_rmse: 0.019540\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.1029, Val Loss: 0.3367\n",
      "train_e_mae: 0.044600\n",
      "train_e_rmse: 0.054807\n",
      "train_f_mae: 0.009404\n",
      "train_f_rmse: 0.012700\n",
      "val_e_mae: 0.027810\n",
      "val_e_rmse: 0.028450\n",
      "val_f_mae: 0.011567\n",
      "val_f_rmse: 0.018326\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.2629, Val Loss: 0.2939\n",
      "train_e_mae: 0.034808\n",
      "train_e_rmse: 0.042196\n",
      "train_f_mae: 0.010470\n",
      "train_f_rmse: 0.014363\n",
      "val_e_mae: 0.030835\n",
      "val_e_rmse: 0.031372\n",
      "val_f_mae: 0.011021\n",
      "val_f_rmse: 0.017116\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.3150, Val Loss: 0.2666\n",
      "train_e_mae: 0.044588\n",
      "train_e_rmse: 0.054577\n",
      "train_f_mae: 0.009751\n",
      "train_f_rmse: 0.013457\n",
      "val_e_mae: 0.031904\n",
      "val_e_rmse: 0.032393\n",
      "val_f_mae: 0.010660\n",
      "val_f_rmse: 0.016297\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.1152, Val Loss: 0.2404\n",
      "train_e_mae: 0.037108\n",
      "train_e_rmse: 0.045333\n",
      "train_f_mae: 0.008404\n",
      "train_f_rmse: 0.011619\n",
      "val_e_mae: 0.034820\n",
      "val_e_rmse: 0.035240\n",
      "val_f_mae: 0.010276\n",
      "val_f_rmse: 0.015464\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0963, Val Loss: 0.2157\n",
      "train_e_mae: 0.030067\n",
      "train_e_rmse: 0.036064\n",
      "train_f_mae: 0.007438\n",
      "train_f_rmse: 0.010087\n",
      "val_e_mae: 0.036373\n",
      "val_e_rmse: 0.036748\n",
      "val_f_mae: 0.009893\n",
      "val_f_rmse: 0.014640\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.0746, Val Loss: 0.1968\n",
      "train_e_mae: 0.027889\n",
      "train_e_rmse: 0.032783\n",
      "train_f_mae: 0.006833\n",
      "train_f_rmse: 0.009748\n",
      "val_e_mae: 0.037827\n",
      "val_e_rmse: 0.038163\n",
      "val_f_mae: 0.009580\n",
      "val_f_rmse: 0.013977\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0883, Val Loss: 0.1808\n",
      "train_e_mae: 0.027291\n",
      "train_e_rmse: 0.032443\n",
      "train_f_mae: 0.006561\n",
      "train_f_rmse: 0.009022\n",
      "val_e_mae: 0.038824\n",
      "val_e_rmse: 0.039128\n",
      "val_f_mae: 0.009300\n",
      "val_f_rmse: 0.013390\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0635, Val Loss: 0.1681\n",
      "train_e_mae: 0.025518\n",
      "train_e_rmse: 0.029931\n",
      "train_f_mae: 0.006263\n",
      "train_f_rmse: 0.008763\n",
      "val_e_mae: 0.039791\n",
      "val_e_rmse: 0.040068\n",
      "val_f_mae: 0.009058\n",
      "val_f_rmse: 0.012902\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0624, Val Loss: 0.1570\n",
      "train_e_mae: 0.023717\n",
      "train_e_rmse: 0.027021\n",
      "train_f_mae: 0.005699\n",
      "train_f_rmse: 0.008014\n",
      "val_e_mae: 0.040322\n",
      "val_e_rmse: 0.040575\n",
      "val_f_mae: 0.008832\n",
      "val_f_rmse: 0.012465\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0587, Val Loss: 0.1476\n",
      "train_e_mae: 0.022131\n",
      "train_e_rmse: 0.024744\n",
      "train_f_mae: 0.005434\n",
      "train_f_rmse: 0.007708\n",
      "val_e_mae: 0.040683\n",
      "val_e_rmse: 0.040915\n",
      "val_f_mae: 0.008629\n",
      "val_f_rmse: 0.012078\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0575, Val Loss: 0.1398\n",
      "train_e_mae: 0.021059\n",
      "train_e_rmse: 0.023394\n",
      "train_f_mae: 0.005302\n",
      "train_f_rmse: 0.007566\n",
      "val_e_mae: 0.040874\n",
      "val_e_rmse: 0.041088\n",
      "val_f_mae: 0.008458\n",
      "val_f_rmse: 0.011751\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0563, Val Loss: 0.1330\n",
      "train_e_mae: 0.020355\n",
      "train_e_rmse: 0.022522\n",
      "train_f_mae: 0.005244\n",
      "train_f_rmse: 0.007489\n",
      "val_e_mae: 0.040851\n",
      "val_e_rmse: 0.041049\n",
      "val_f_mae: 0.008303\n",
      "val_f_rmse: 0.011459\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0556, Val Loss: 0.1273\n",
      "train_e_mae: 0.019918\n",
      "train_e_rmse: 0.022018\n",
      "train_f_mae: 0.005210\n",
      "train_f_rmse: 0.007441\n",
      "val_e_mae: 0.040779\n",
      "val_e_rmse: 0.040963\n",
      "val_f_mae: 0.008163\n",
      "val_f_rmse: 0.011208\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0550, Val Loss: 0.1224\n",
      "train_e_mae: 0.019637\n",
      "train_e_rmse: 0.021706\n",
      "train_f_mae: 0.005186\n",
      "train_f_rmse: 0.007403\n",
      "val_e_mae: 0.040618\n",
      "val_e_rmse: 0.040789\n",
      "val_f_mae: 0.008037\n",
      "val_f_rmse: 0.010989\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0546, Val Loss: 0.1183\n",
      "train_e_mae: 0.019452\n",
      "train_e_rmse: 0.021502\n",
      "train_f_mae: 0.005167\n",
      "train_f_rmse: 0.007371\n",
      "val_e_mae: 0.040419\n",
      "val_e_rmse: 0.040579\n",
      "val_f_mae: 0.007923\n",
      "val_f_rmse: 0.010798\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0542, Val Loss: 0.1147\n",
      "train_e_mae: 0.019332\n",
      "train_e_rmse: 0.021372\n",
      "train_f_mae: 0.005151\n",
      "train_f_rmse: 0.007342\n",
      "val_e_mae: 0.040198\n",
      "val_e_rmse: 0.040348\n",
      "val_f_mae: 0.007821\n",
      "val_f_rmse: 0.010633\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0538, Val Loss: 0.1117\n",
      "train_e_mae: 0.019249\n",
      "train_e_rmse: 0.021283\n",
      "train_f_mae: 0.005136\n",
      "train_f_rmse: 0.007317\n",
      "val_e_mae: 0.039967\n",
      "val_e_rmse: 0.040108\n",
      "val_f_mae: 0.007729\n",
      "val_f_rmse: 0.010490\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0535, Val Loss: 0.1090\n",
      "train_e_mae: 0.019185\n",
      "train_e_rmse: 0.021214\n",
      "train_f_mae: 0.005123\n",
      "train_f_rmse: 0.007295\n",
      "val_e_mae: 0.039726\n",
      "val_e_rmse: 0.039860\n",
      "val_f_mae: 0.007647\n",
      "val_f_rmse: 0.010366\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0532, Val Loss: 0.1068\n",
      "train_e_mae: 0.019135\n",
      "train_e_rmse: 0.021161\n",
      "train_f_mae: 0.005111\n",
      "train_f_rmse: 0.007275\n",
      "val_e_mae: 0.039484\n",
      "val_e_rmse: 0.039611\n",
      "val_f_mae: 0.007574\n",
      "val_f_rmse: 0.010259\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0530, Val Loss: 0.1049\n",
      "train_e_mae: 0.019096\n",
      "train_e_rmse: 0.021119\n",
      "train_f_mae: 0.005100\n",
      "train_f_rmse: 0.007257\n",
      "val_e_mae: 0.039247\n",
      "val_e_rmse: 0.039367\n",
      "val_f_mae: 0.007507\n",
      "val_f_rmse: 0.010165\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0528, Val Loss: 0.1032\n",
      "train_e_mae: 0.019064\n",
      "train_e_rmse: 0.021086\n",
      "train_f_mae: 0.005091\n",
      "train_f_rmse: 0.007241\n",
      "val_e_mae: 0.039014\n",
      "val_e_rmse: 0.039130\n",
      "val_f_mae: 0.007448\n",
      "val_f_rmse: 0.010083\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0526, Val Loss: 0.1018\n",
      "train_e_mae: 0.019037\n",
      "train_e_rmse: 0.021057\n",
      "train_f_mae: 0.005082\n",
      "train_f_rmse: 0.007226\n",
      "val_e_mae: 0.038792\n",
      "val_e_rmse: 0.038902\n",
      "val_f_mae: 0.007398\n",
      "val_f_rmse: 0.010012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 0.3868, Val Loss: 1.3141\n",
      "train_e_mae: 0.138889\n",
      "train_e_rmse: 0.177802\n",
      "train_f_mae: 0.019085\n",
      "train_f_rmse: 0.031174\n",
      "val_e_mae: 0.107338\n",
      "val_e_rmse: 0.107463\n",
      "val_f_mae: 0.022100\n",
      "val_f_rmse: 0.036091\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.4879, Val Loss: 0.9242\n",
      "train_e_mae: 0.089071\n",
      "train_e_rmse: 0.111796\n",
      "train_f_mae: 0.016806\n",
      "train_f_rmse: 0.024315\n",
      "val_e_mae: 0.100791\n",
      "val_e_rmse: 0.100889\n",
      "val_f_mae: 0.019314\n",
      "val_f_rmse: 0.030232\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.4542, Val Loss: 0.7533\n",
      "train_e_mae: 0.157124\n",
      "train_e_rmse: 0.186683\n",
      "train_f_mae: 0.014923\n",
      "train_f_rmse: 0.021588\n",
      "val_e_mae: 0.100735\n",
      "val_e_rmse: 0.100850\n",
      "val_f_mae: 0.017370\n",
      "val_f_rmse: 0.027261\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.3335, Val Loss: 0.6136\n",
      "train_e_mae: 0.082929\n",
      "train_e_rmse: 0.100811\n",
      "train_f_mae: 0.011808\n",
      "train_f_rmse: 0.016677\n",
      "val_e_mae: 0.101073\n",
      "val_e_rmse: 0.101202\n",
      "val_f_mae: 0.015613\n",
      "val_f_rmse: 0.024563\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.1887, Val Loss: 0.5150\n",
      "train_e_mae: 0.065699\n",
      "train_e_rmse: 0.080054\n",
      "train_f_mae: 0.009297\n",
      "train_f_rmse: 0.013176\n",
      "val_e_mae: 0.099972\n",
      "val_e_rmse: 0.100110\n",
      "val_f_mae: 0.014364\n",
      "val_f_rmse: 0.022471\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.1264, Val Loss: 0.4346\n",
      "train_e_mae: 0.058831\n",
      "train_e_rmse: 0.070361\n",
      "train_f_mae: 0.009213\n",
      "train_f_rmse: 0.012975\n",
      "val_e_mae: 0.097432\n",
      "val_e_rmse: 0.097574\n",
      "val_f_mae: 0.013277\n",
      "val_f_rmse: 0.020618\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.1193, Val Loss: 0.3743\n",
      "train_e_mae: 0.054901\n",
      "train_e_rmse: 0.066682\n",
      "train_f_mae: 0.008066\n",
      "train_f_rmse: 0.011300\n",
      "val_e_mae: 0.093029\n",
      "val_e_rmse: 0.093174\n",
      "val_f_mae: 0.012424\n",
      "val_f_rmse: 0.019122\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1866, Val Loss: 0.3260\n",
      "train_e_mae: 0.034982\n",
      "train_e_rmse: 0.041018\n",
      "train_f_mae: 0.008844\n",
      "train_f_rmse: 0.012653\n",
      "val_e_mae: 0.089300\n",
      "val_e_rmse: 0.089445\n",
      "val_f_mae: 0.011723\n",
      "val_f_rmse: 0.017834\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.1490, Val Loss: 0.2855\n",
      "train_e_mae: 0.037339\n",
      "train_e_rmse: 0.045561\n",
      "train_f_mae: 0.008184\n",
      "train_f_rmse: 0.011856\n",
      "val_e_mae: 0.085243\n",
      "val_e_rmse: 0.085391\n",
      "val_f_mae: 0.011092\n",
      "val_f_rmse: 0.016679\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.1325, Val Loss: 0.2521\n",
      "train_e_mae: 0.036432\n",
      "train_e_rmse: 0.044312\n",
      "train_f_mae: 0.008200\n",
      "train_f_rmse: 0.010989\n",
      "val_e_mae: 0.083107\n",
      "val_e_rmse: 0.083253\n",
      "val_f_mae: 0.010550\n",
      "val_f_rmse: 0.015658\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.0772, Val Loss: 0.2248\n",
      "train_e_mae: 0.034222\n",
      "train_e_rmse: 0.041242\n",
      "train_f_mae: 0.007415\n",
      "train_f_rmse: 0.010152\n",
      "val_e_mae: 0.080116\n",
      "val_e_rmse: 0.080262\n",
      "val_f_mae: 0.010100\n",
      "val_f_rmse: 0.014779\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.0872, Val Loss: 0.2025\n",
      "train_e_mae: 0.033493\n",
      "train_e_rmse: 0.040723\n",
      "train_f_mae: 0.007500\n",
      "train_f_rmse: 0.010264\n",
      "val_e_mae: 0.077836\n",
      "val_e_rmse: 0.077980\n",
      "val_f_mae: 0.009736\n",
      "val_f_rmse: 0.014015\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0715, Val Loss: 0.1838\n",
      "train_e_mae: 0.029491\n",
      "train_e_rmse: 0.034123\n",
      "train_f_mae: 0.006685\n",
      "train_f_rmse: 0.008934\n",
      "val_e_mae: 0.075491\n",
      "val_e_rmse: 0.075633\n",
      "val_f_mae: 0.009424\n",
      "val_f_rmse: 0.013346\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.1200, Val Loss: 0.1685\n",
      "train_e_mae: 0.027744\n",
      "train_e_rmse: 0.032268\n",
      "train_f_mae: 0.006257\n",
      "train_f_rmse: 0.008811\n",
      "val_e_mae: 0.072589\n",
      "val_e_rmse: 0.072729\n",
      "val_f_mae: 0.009141\n",
      "val_f_rmse: 0.012774\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0764, Val Loss: 0.1557\n",
      "train_e_mae: 0.026790\n",
      "train_e_rmse: 0.030688\n",
      "train_f_mae: 0.006276\n",
      "train_f_rmse: 0.008509\n",
      "val_e_mae: 0.070584\n",
      "val_e_rmse: 0.070721\n",
      "val_f_mae: 0.008892\n",
      "val_f_rmse: 0.012278\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0548, Val Loss: 0.1453\n",
      "train_e_mae: 0.024770\n",
      "train_e_rmse: 0.027931\n",
      "train_f_mae: 0.005678\n",
      "train_f_rmse: 0.007785\n",
      "val_e_mae: 0.068204\n",
      "val_e_rmse: 0.068337\n",
      "val_f_mae: 0.008668\n",
      "val_f_rmse: 0.011859\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0527, Val Loss: 0.1366\n",
      "train_e_mae: 0.023569\n",
      "train_e_rmse: 0.026383\n",
      "train_f_mae: 0.005241\n",
      "train_f_rmse: 0.007299\n",
      "val_e_mae: 0.065876\n",
      "val_e_rmse: 0.066007\n",
      "val_f_mae: 0.008465\n",
      "val_f_rmse: 0.011498\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0508, Val Loss: 0.1291\n",
      "train_e_mae: 0.022468\n",
      "train_e_rmse: 0.024862\n",
      "train_f_mae: 0.005041\n",
      "train_f_rmse: 0.007099\n",
      "val_e_mae: 0.063666\n",
      "val_e_rmse: 0.063793\n",
      "val_f_mae: 0.008278\n",
      "val_f_rmse: 0.011180\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0495, Val Loss: 0.1227\n",
      "train_e_mae: 0.021721\n",
      "train_e_rmse: 0.023988\n",
      "train_f_mae: 0.004962\n",
      "train_f_rmse: 0.007010\n",
      "val_e_mae: 0.061539\n",
      "val_e_rmse: 0.061662\n",
      "val_f_mae: 0.008108\n",
      "val_f_rmse: 0.010906\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0489, Val Loss: 0.1174\n",
      "train_e_mae: 0.021214\n",
      "train_e_rmse: 0.023404\n",
      "train_f_mae: 0.004927\n",
      "train_f_rmse: 0.006963\n",
      "val_e_mae: 0.059550\n",
      "val_e_rmse: 0.059669\n",
      "val_f_mae: 0.007958\n",
      "val_f_rmse: 0.010668\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0484, Val Loss: 0.1128\n",
      "train_e_mae: 0.020906\n",
      "train_e_rmse: 0.023060\n",
      "train_f_mae: 0.004904\n",
      "train_f_rmse: 0.006929\n",
      "val_e_mae: 0.057685\n",
      "val_e_rmse: 0.057800\n",
      "val_f_mae: 0.007822\n",
      "val_f_rmse: 0.010461\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0480, Val Loss: 0.1088\n",
      "train_e_mae: 0.020703\n",
      "train_e_rmse: 0.022837\n",
      "train_f_mae: 0.004887\n",
      "train_f_rmse: 0.006903\n",
      "val_e_mae: 0.055975\n",
      "val_e_rmse: 0.056086\n",
      "val_f_mae: 0.007697\n",
      "val_f_rmse: 0.010280\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0477, Val Loss: 0.1054\n",
      "train_e_mae: 0.020571\n",
      "train_e_rmse: 0.022691\n",
      "train_f_mae: 0.004873\n",
      "train_f_rmse: 0.006880\n",
      "val_e_mae: 0.054406\n",
      "val_e_rmse: 0.054515\n",
      "val_f_mae: 0.007585\n",
      "val_f_rmse: 0.010122\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0474, Val Loss: 0.1025\n",
      "train_e_mae: 0.020471\n",
      "train_e_rmse: 0.022582\n",
      "train_f_mae: 0.004860\n",
      "train_f_rmse: 0.006860\n",
      "val_e_mae: 0.052973\n",
      "val_e_rmse: 0.053079\n",
      "val_f_mae: 0.007484\n",
      "val_f_rmse: 0.009984\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0472, Val Loss: 0.1000\n",
      "train_e_mae: 0.020394\n",
      "train_e_rmse: 0.022498\n",
      "train_f_mae: 0.004850\n",
      "train_f_rmse: 0.006841\n",
      "val_e_mae: 0.051666\n",
      "val_e_rmse: 0.051769\n",
      "val_f_mae: 0.007391\n",
      "val_f_rmse: 0.009863\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0470, Val Loss: 0.0978\n",
      "train_e_mae: 0.020333\n",
      "train_e_rmse: 0.022432\n",
      "train_f_mae: 0.004839\n",
      "train_f_rmse: 0.006825\n",
      "val_e_mae: 0.050473\n",
      "val_e_rmse: 0.050573\n",
      "val_f_mae: 0.007309\n",
      "val_f_rmse: 0.009757\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0468, Val Loss: 0.0958\n",
      "train_e_mae: 0.020282\n",
      "train_e_rmse: 0.022377\n",
      "train_f_mae: 0.004830\n",
      "train_f_rmse: 0.006811\n",
      "val_e_mae: 0.049384\n",
      "val_e_rmse: 0.049482\n",
      "val_f_mae: 0.007233\n",
      "val_f_rmse: 0.009664\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0466, Val Loss: 0.0942\n",
      "train_e_mae: 0.020241\n",
      "train_e_rmse: 0.022332\n",
      "train_f_mae: 0.004822\n",
      "train_f_rmse: 0.006797\n",
      "val_e_mae: 0.048391\n",
      "val_e_rmse: 0.048487\n",
      "val_f_mae: 0.007164\n",
      "val_f_rmse: 0.009583\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0465, Val Loss: 0.0927\n",
      "train_e_mae: 0.020205\n",
      "train_e_rmse: 0.022293\n",
      "train_f_mae: 0.004815\n",
      "train_f_rmse: 0.006785\n",
      "val_e_mae: 0.047488\n",
      "val_e_rmse: 0.047582\n",
      "val_f_mae: 0.007105\n",
      "val_f_rmse: 0.009511\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0463, Val Loss: 0.0915\n",
      "train_e_mae: 0.020175\n",
      "train_e_rmse: 0.022260\n",
      "train_f_mae: 0.004808\n",
      "train_f_rmse: 0.006775\n",
      "val_e_mae: 0.046663\n",
      "val_e_rmse: 0.046755\n",
      "val_f_mae: 0.007056\n",
      "val_f_rmse: 0.009448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 1.7510, Val Loss: 1.5543\n",
      "train_e_mae: 0.158826\n",
      "train_e_rmse: 0.202739\n",
      "train_f_mae: 0.023394\n",
      "train_f_rmse: 0.036476\n",
      "val_e_mae: 0.443279\n",
      "val_e_rmse: 0.443331\n",
      "val_f_mae: 0.029904\n",
      "val_f_rmse: 0.036848\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.4973, Val Loss: 0.7976\n",
      "train_e_mae: 0.231365\n",
      "train_e_rmse: 0.268657\n",
      "train_f_mae: 0.019120\n",
      "train_f_rmse: 0.027079\n",
      "val_e_mae: 0.120499\n",
      "val_e_rmse: 0.120819\n",
      "val_f_mae: 0.018552\n",
      "val_f_rmse: 0.027981\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2078, Val Loss: 0.6800\n",
      "train_e_mae: 0.096199\n",
      "train_e_rmse: 0.126134\n",
      "train_f_mae: 0.011796\n",
      "train_f_rmse: 0.016166\n",
      "val_e_mae: 0.120973\n",
      "val_e_rmse: 0.121274\n",
      "val_f_mae: 0.017145\n",
      "val_f_rmse: 0.025794\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2373, Val Loss: 0.5817\n",
      "train_e_mae: 0.044586\n",
      "train_e_rmse: 0.055104\n",
      "train_f_mae: 0.011217\n",
      "train_f_rmse: 0.015622\n",
      "val_e_mae: 0.119591\n",
      "val_e_rmse: 0.119870\n",
      "val_f_mae: 0.015916\n",
      "val_f_rmse: 0.023818\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.3538, Val Loss: 0.5083\n",
      "train_e_mae: 0.097131\n",
      "train_e_rmse: 0.118413\n",
      "train_f_mae: 0.012742\n",
      "train_f_rmse: 0.019099\n",
      "val_e_mae: 0.118685\n",
      "val_e_rmse: 0.118944\n",
      "val_f_mae: 0.014882\n",
      "val_f_rmse: 0.022230\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.1462, Val Loss: 0.4414\n",
      "train_e_mae: 0.053016\n",
      "train_e_rmse: 0.065080\n",
      "train_f_mae: 0.009717\n",
      "train_f_rmse: 0.013421\n",
      "val_e_mae: 0.116592\n",
      "val_e_rmse: 0.116835\n",
      "val_f_mae: 0.013928\n",
      "val_f_rmse: 0.020683\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.1332, Val Loss: 0.3810\n",
      "train_e_mae: 0.053150\n",
      "train_e_rmse: 0.067284\n",
      "train_f_mae: 0.009278\n",
      "train_f_rmse: 0.013933\n",
      "val_e_mae: 0.113264\n",
      "val_e_rmse: 0.113493\n",
      "val_f_mae: 0.013007\n",
      "val_f_rmse: 0.019186\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1610, Val Loss: 0.3365\n",
      "train_e_mae: 0.061577\n",
      "train_e_rmse: 0.073981\n",
      "train_f_mae: 0.010269\n",
      "train_f_rmse: 0.014597\n",
      "val_e_mae: 0.109356\n",
      "val_e_rmse: 0.109576\n",
      "val_f_mae: 0.012270\n",
      "val_f_rmse: 0.018014\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.2746, Val Loss: 0.2974\n",
      "train_e_mae: 0.042192\n",
      "train_e_rmse: 0.052362\n",
      "train_f_mae: 0.008102\n",
      "train_f_rmse: 0.011772\n",
      "val_e_mae: 0.105673\n",
      "val_e_rmse: 0.105880\n",
      "val_f_mae: 0.011604\n",
      "val_f_rmse: 0.016918\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.0976, Val Loss: 0.2606\n",
      "train_e_mae: 0.037392\n",
      "train_e_rmse: 0.045877\n",
      "train_f_mae: 0.007984\n",
      "train_f_rmse: 0.010997\n",
      "val_e_mae: 0.100851\n",
      "val_e_rmse: 0.101049\n",
      "val_f_mae: 0.010938\n",
      "val_f_rmse: 0.015824\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.0883, Val Loss: 0.2314\n",
      "train_e_mae: 0.040418\n",
      "train_e_rmse: 0.048917\n",
      "train_f_mae: 0.006470\n",
      "train_f_rmse: 0.008669\n",
      "val_e_mae: 0.096442\n",
      "val_e_rmse: 0.096629\n",
      "val_f_mae: 0.010376\n",
      "val_f_rmse: 0.014903\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.0668, Val Loss: 0.2056\n",
      "train_e_mae: 0.023372\n",
      "train_e_rmse: 0.026789\n",
      "train_f_mae: 0.006419\n",
      "train_f_rmse: 0.008475\n",
      "val_e_mae: 0.091947\n",
      "val_e_rmse: 0.092123\n",
      "val_f_mae: 0.009831\n",
      "val_f_rmse: 0.014041\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0626, Val Loss: 0.1843\n",
      "train_e_mae: 0.022624\n",
      "train_e_rmse: 0.026831\n",
      "train_f_mae: 0.005803\n",
      "train_f_rmse: 0.007770\n",
      "val_e_mae: 0.087656\n",
      "val_e_rmse: 0.087822\n",
      "val_f_mae: 0.009345\n",
      "val_f_rmse: 0.013288\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.0508, Val Loss: 0.1658\n",
      "train_e_mae: 0.020063\n",
      "train_e_rmse: 0.022403\n",
      "train_f_mae: 0.005197\n",
      "train_f_rmse: 0.007155\n",
      "val_e_mae: 0.083392\n",
      "val_e_rmse: 0.083548\n",
      "val_f_mae: 0.008886\n",
      "val_f_rmse: 0.012601\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0464, Val Loss: 0.1503\n",
      "train_e_mae: 0.019204\n",
      "train_e_rmse: 0.021396\n",
      "train_f_mae: 0.004934\n",
      "train_f_rmse: 0.006894\n",
      "val_e_mae: 0.079408\n",
      "val_e_rmse: 0.079554\n",
      "val_f_mae: 0.008486\n",
      "val_f_rmse: 0.011997\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0460, Val Loss: 0.1372\n",
      "train_e_mae: 0.018778\n",
      "train_e_rmse: 0.020746\n",
      "train_f_mae: 0.004847\n",
      "train_f_rmse: 0.006781\n",
      "val_e_mae: 0.075685\n",
      "val_e_rmse: 0.075823\n",
      "val_f_mae: 0.008128\n",
      "val_f_rmse: 0.011464\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0449, Val Loss: 0.1262\n",
      "train_e_mae: 0.018639\n",
      "train_e_rmse: 0.020528\n",
      "train_f_mae: 0.004789\n",
      "train_f_rmse: 0.006702\n",
      "val_e_mae: 0.072222\n",
      "val_e_rmse: 0.072352\n",
      "val_f_mae: 0.007823\n",
      "val_f_rmse: 0.010997\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0444, Val Loss: 0.1169\n",
      "train_e_mae: 0.018591\n",
      "train_e_rmse: 0.020453\n",
      "train_f_mae: 0.004754\n",
      "train_f_rmse: 0.006653\n",
      "val_e_mae: 0.069029\n",
      "val_e_rmse: 0.069152\n",
      "val_f_mae: 0.007554\n",
      "val_f_rmse: 0.010591\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0439, Val Loss: 0.1093\n",
      "train_e_mae: 0.018581\n",
      "train_e_rmse: 0.020443\n",
      "train_f_mae: 0.004726\n",
      "train_f_rmse: 0.006613\n",
      "val_e_mae: 0.066121\n",
      "val_e_rmse: 0.066237\n",
      "val_f_mae: 0.007317\n",
      "val_f_rmse: 0.010240\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0435, Val Loss: 0.1028\n",
      "train_e_mae: 0.018589\n",
      "train_e_rmse: 0.020449\n",
      "train_f_mae: 0.004705\n",
      "train_f_rmse: 0.006580\n",
      "val_e_mae: 0.063450\n",
      "val_e_rmse: 0.063561\n",
      "val_f_mae: 0.007105\n",
      "val_f_rmse: 0.009940\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0432, Val Loss: 0.0975\n",
      "train_e_mae: 0.018596\n",
      "train_e_rmse: 0.020460\n",
      "train_f_mae: 0.004687\n",
      "train_f_rmse: 0.006551\n",
      "val_e_mae: 0.061002\n",
      "val_e_rmse: 0.061107\n",
      "val_f_mae: 0.006915\n",
      "val_f_rmse: 0.009683\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0429, Val Loss: 0.0931\n",
      "train_e_mae: 0.018600\n",
      "train_e_rmse: 0.020466\n",
      "train_f_mae: 0.004672\n",
      "train_f_rmse: 0.006527\n",
      "val_e_mae: 0.058761\n",
      "val_e_rmse: 0.058862\n",
      "val_f_mae: 0.006758\n",
      "val_f_rmse: 0.009466\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0426, Val Loss: 0.0894\n",
      "train_e_mae: 0.018603\n",
      "train_e_rmse: 0.020471\n",
      "train_f_mae: 0.004659\n",
      "train_f_rmse: 0.006505\n",
      "val_e_mae: 0.056710\n",
      "val_e_rmse: 0.056807\n",
      "val_f_mae: 0.006628\n",
      "val_f_rmse: 0.009283\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0424, Val Loss: 0.0864\n",
      "train_e_mae: 0.018605\n",
      "train_e_rmse: 0.020475\n",
      "train_f_mae: 0.004648\n",
      "train_f_rmse: 0.006485\n",
      "val_e_mae: 0.054832\n",
      "val_e_rmse: 0.054925\n",
      "val_f_mae: 0.006516\n",
      "val_f_rmse: 0.009130\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0422, Val Loss: 0.0839\n",
      "train_e_mae: 0.018608\n",
      "train_e_rmse: 0.020480\n",
      "train_f_mae: 0.004637\n",
      "train_f_rmse: 0.006468\n",
      "val_e_mae: 0.053116\n",
      "val_e_rmse: 0.053206\n",
      "val_f_mae: 0.006427\n",
      "val_f_rmse: 0.009002\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0420, Val Loss: 0.0818\n",
      "train_e_mae: 0.018611\n",
      "train_e_rmse: 0.020485\n",
      "train_f_mae: 0.004628\n",
      "train_f_rmse: 0.006452\n",
      "val_e_mae: 0.051548\n",
      "val_e_rmse: 0.051635\n",
      "val_f_mae: 0.006355\n",
      "val_f_rmse: 0.008896\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0418, Val Loss: 0.0801\n",
      "train_e_mae: 0.018615\n",
      "train_e_rmse: 0.020490\n",
      "train_f_mae: 0.004619\n",
      "train_f_rmse: 0.006438\n",
      "val_e_mae: 0.050113\n",
      "val_e_rmse: 0.050197\n",
      "val_f_mae: 0.006297\n",
      "val_f_rmse: 0.008808\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0416, Val Loss: 0.0787\n",
      "train_e_mae: 0.018619\n",
      "train_e_rmse: 0.020495\n",
      "train_f_mae: 0.004612\n",
      "train_f_rmse: 0.006425\n",
      "val_e_mae: 0.048805\n",
      "val_e_rmse: 0.048886\n",
      "val_f_mae: 0.006261\n",
      "val_f_rmse: 0.008737\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0415, Val Loss: 0.0776\n",
      "train_e_mae: 0.018623\n",
      "train_e_rmse: 0.020501\n",
      "train_f_mae: 0.004605\n",
      "train_f_rmse: 0.006413\n",
      "val_e_mae: 0.047609\n",
      "val_e_rmse: 0.047688\n",
      "val_f_mae: 0.006251\n",
      "val_f_rmse: 0.008678\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0414, Val Loss: 0.0766\n",
      "train_e_mae: 0.018627\n",
      "train_e_rmse: 0.020506\n",
      "train_f_mae: 0.004598\n",
      "train_f_rmse: 0.006403\n",
      "val_e_mae: 0.046515\n",
      "val_e_rmse: 0.046592\n",
      "val_f_mae: 0.006247\n",
      "val_f_rmse: 0.008630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 0.8659, Val Loss: 0.9055\n",
      "train_e_mae: 0.177150\n",
      "train_e_rmse: 0.218031\n",
      "train_f_mae: 0.016386\n",
      "train_f_rmse: 0.024437\n",
      "val_e_mae: 0.399612\n",
      "val_e_rmse: 0.399680\n",
      "val_f_mae: 0.019539\n",
      "val_f_rmse: 0.027308\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.4672, Val Loss: 0.4660\n",
      "train_e_mae: 0.136650\n",
      "train_e_rmse: 0.165052\n",
      "train_f_mae: 0.014538\n",
      "train_f_rmse: 0.020046\n",
      "val_e_mae: 0.173671\n",
      "val_e_rmse: 0.173814\n",
      "val_f_mae: 0.015514\n",
      "val_f_rmse: 0.020876\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.6091, Val Loss: 0.3893\n",
      "train_e_mae: 0.110071\n",
      "train_e_rmse: 0.125401\n",
      "train_f_mae: 0.015122\n",
      "train_f_rmse: 0.022773\n",
      "val_e_mae: 0.163312\n",
      "val_e_rmse: 0.163461\n",
      "val_f_mae: 0.014255\n",
      "val_f_rmse: 0.019042\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2413, Val Loss: 0.3376\n",
      "train_e_mae: 0.099593\n",
      "train_e_rmse: 0.119834\n",
      "train_f_mae: 0.010664\n",
      "train_f_rmse: 0.015597\n",
      "val_e_mae: 0.158528\n",
      "val_e_rmse: 0.158683\n",
      "val_f_mae: 0.013310\n",
      "val_f_rmse: 0.017674\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.2600, Val Loss: 0.2881\n",
      "train_e_mae: 0.065927\n",
      "train_e_rmse: 0.080798\n",
      "train_f_mae: 0.009610\n",
      "train_f_rmse: 0.014969\n",
      "val_e_mae: 0.151260\n",
      "val_e_rmse: 0.151421\n",
      "val_f_mae: 0.012338\n",
      "val_f_rmse: 0.016284\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.1217, Val Loss: 0.2468\n",
      "train_e_mae: 0.056793\n",
      "train_e_rmse: 0.068948\n",
      "train_f_mae: 0.008008\n",
      "train_f_rmse: 0.011843\n",
      "val_e_mae: 0.145602\n",
      "val_e_rmse: 0.145766\n",
      "val_f_mae: 0.011439\n",
      "val_f_rmse: 0.015017\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.1057, Val Loss: 0.2139\n",
      "train_e_mae: 0.043889\n",
      "train_e_rmse: 0.050588\n",
      "train_f_mae: 0.007593\n",
      "train_f_rmse: 0.010643\n",
      "val_e_mae: 0.139903\n",
      "val_e_rmse: 0.140067\n",
      "val_f_mae: 0.010657\n",
      "val_f_rmse: 0.013937\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.0817, Val Loss: 0.1870\n",
      "train_e_mae: 0.036984\n",
      "train_e_rmse: 0.041686\n",
      "train_f_mae: 0.007329\n",
      "train_f_rmse: 0.009905\n",
      "val_e_mae: 0.133252\n",
      "val_e_rmse: 0.133415\n",
      "val_f_mae: 0.009962\n",
      "val_f_rmse: 0.013007\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.0872, Val Loss: 0.1641\n",
      "train_e_mae: 0.032345\n",
      "train_e_rmse: 0.036776\n",
      "train_f_mae: 0.006936\n",
      "train_f_rmse: 0.009358\n",
      "val_e_mae: 0.126538\n",
      "val_e_rmse: 0.126698\n",
      "val_f_mae: 0.009318\n",
      "val_f_rmse: 0.012168\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.0536, Val Loss: 0.1454\n",
      "train_e_mae: 0.028520\n",
      "train_e_rmse: 0.032277\n",
      "train_f_mae: 0.005452\n",
      "train_f_rmse: 0.007366\n",
      "val_e_mae: 0.119921\n",
      "val_e_rmse: 0.120075\n",
      "val_f_mae: 0.008751\n",
      "val_f_rmse: 0.011446\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.0440, Val Loss: 0.1297\n",
      "train_e_mae: 0.024975\n",
      "train_e_rmse: 0.028510\n",
      "train_f_mae: 0.004941\n",
      "train_f_rmse: 0.006761\n",
      "val_e_mae: 0.113323\n",
      "val_e_rmse: 0.113471\n",
      "val_f_mae: 0.008235\n",
      "val_f_rmse: 0.010808\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.0417, Val Loss: 0.1165\n",
      "train_e_mae: 0.022027\n",
      "train_e_rmse: 0.024368\n",
      "train_f_mae: 0.004687\n",
      "train_f_rmse: 0.006464\n",
      "val_e_mae: 0.106866\n",
      "val_e_rmse: 0.107007\n",
      "val_f_mae: 0.007772\n",
      "val_f_rmse: 0.010248\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0402, Val Loss: 0.1056\n",
      "train_e_mae: 0.020877\n",
      "train_e_rmse: 0.023077\n",
      "train_f_mae: 0.004575\n",
      "train_f_rmse: 0.006337\n",
      "val_e_mae: 0.100806\n",
      "val_e_rmse: 0.100941\n",
      "val_f_mae: 0.007375\n",
      "val_f_rmse: 0.009768\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.0394, Val Loss: 0.0968\n",
      "train_e_mae: 0.020369\n",
      "train_e_rmse: 0.022468\n",
      "train_f_mae: 0.004515\n",
      "train_f_rmse: 0.006263\n",
      "val_e_mae: 0.095255\n",
      "val_e_rmse: 0.095384\n",
      "val_f_mae: 0.007028\n",
      "val_f_rmse: 0.009365\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0389, Val Loss: 0.0896\n",
      "train_e_mae: 0.020158\n",
      "train_e_rmse: 0.022177\n",
      "train_f_mae: 0.004481\n",
      "train_f_rmse: 0.006213\n",
      "val_e_mae: 0.090140\n",
      "val_e_rmse: 0.090263\n",
      "val_f_mae: 0.006722\n",
      "val_f_rmse: 0.009024\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0384, Val Loss: 0.0837\n",
      "train_e_mae: 0.020052\n",
      "train_e_rmse: 0.022054\n",
      "train_f_mae: 0.004458\n",
      "train_f_rmse: 0.006175\n",
      "val_e_mae: 0.085450\n",
      "val_e_rmse: 0.085567\n",
      "val_f_mae: 0.006459\n",
      "val_f_rmse: 0.008741\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0381, Val Loss: 0.0790\n",
      "train_e_mae: 0.019972\n",
      "train_e_rmse: 0.021968\n",
      "train_f_mae: 0.004441\n",
      "train_f_rmse: 0.006143\n",
      "val_e_mae: 0.081159\n",
      "val_e_rmse: 0.081272\n",
      "val_f_mae: 0.006237\n",
      "val_f_rmse: 0.008508\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0377, Val Loss: 0.0752\n",
      "train_e_mae: 0.019908\n",
      "train_e_rmse: 0.021898\n",
      "train_f_mae: 0.004424\n",
      "train_f_rmse: 0.006115\n",
      "val_e_mae: 0.077224\n",
      "val_e_rmse: 0.077332\n",
      "val_f_mae: 0.006063\n",
      "val_f_rmse: 0.008319\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0374, Val Loss: 0.0721\n",
      "train_e_mae: 0.019855\n",
      "train_e_rmse: 0.021841\n",
      "train_f_mae: 0.004410\n",
      "train_f_rmse: 0.006090\n",
      "val_e_mae: 0.073615\n",
      "val_e_rmse: 0.073719\n",
      "val_f_mae: 0.005920\n",
      "val_f_rmse: 0.008166\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0372, Val Loss: 0.0697\n",
      "train_e_mae: 0.019815\n",
      "train_e_rmse: 0.021798\n",
      "train_f_mae: 0.004398\n",
      "train_f_rmse: 0.006068\n",
      "val_e_mae: 0.070308\n",
      "val_e_rmse: 0.070409\n",
      "val_f_mae: 0.005813\n",
      "val_f_rmse: 0.008046\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0369, Val Loss: 0.0678\n",
      "train_e_mae: 0.019784\n",
      "train_e_rmse: 0.021765\n",
      "train_f_mae: 0.004387\n",
      "train_f_rmse: 0.006048\n",
      "val_e_mae: 0.067273\n",
      "val_e_rmse: 0.067370\n",
      "val_f_mae: 0.005741\n",
      "val_f_rmse: 0.007951\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0367, Val Loss: 0.0662\n",
      "train_e_mae: 0.019759\n",
      "train_e_rmse: 0.021740\n",
      "train_f_mae: 0.004377\n",
      "train_f_rmse: 0.006030\n",
      "val_e_mae: 0.064494\n",
      "val_e_rmse: 0.064588\n",
      "val_f_mae: 0.005705\n",
      "val_f_rmse: 0.007879\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0365, Val Loss: 0.0651\n",
      "train_e_mae: 0.019740\n",
      "train_e_rmse: 0.021719\n",
      "train_f_mae: 0.004368\n",
      "train_f_rmse: 0.006013\n",
      "val_e_mae: 0.061951\n",
      "val_e_rmse: 0.062042\n",
      "val_f_mae: 0.005688\n",
      "val_f_rmse: 0.007825\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0364, Val Loss: 0.0642\n",
      "train_e_mae: 0.019724\n",
      "train_e_rmse: 0.021703\n",
      "train_f_mae: 0.004360\n",
      "train_f_rmse: 0.005998\n",
      "val_e_mae: 0.059621\n",
      "val_e_rmse: 0.059710\n",
      "val_f_mae: 0.005688\n",
      "val_f_rmse: 0.007785\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0362, Val Loss: 0.0635\n",
      "train_e_mae: 0.019711\n",
      "train_e_rmse: 0.021689\n",
      "train_f_mae: 0.004353\n",
      "train_f_rmse: 0.005984\n",
      "val_e_mae: 0.057493\n",
      "val_e_rmse: 0.057580\n",
      "val_f_mae: 0.005690\n",
      "val_f_rmse: 0.007757\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0361, Val Loss: 0.0630\n",
      "train_e_mae: 0.019699\n",
      "train_e_rmse: 0.021677\n",
      "train_f_mae: 0.004346\n",
      "train_f_rmse: 0.005972\n",
      "val_e_mae: 0.055550\n",
      "val_e_rmse: 0.055634\n",
      "val_f_mae: 0.005699\n",
      "val_f_rmse: 0.007739\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0359, Val Loss: 0.0626\n",
      "train_e_mae: 0.019689\n",
      "train_e_rmse: 0.021667\n",
      "train_f_mae: 0.004340\n",
      "train_f_rmse: 0.005961\n",
      "val_e_mae: 0.053776\n",
      "val_e_rmse: 0.053859\n",
      "val_f_mae: 0.005708\n",
      "val_f_rmse: 0.007728\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0358, Val Loss: 0.0624\n",
      "train_e_mae: 0.019680\n",
      "train_e_rmse: 0.021657\n",
      "train_f_mae: 0.004334\n",
      "train_f_rmse: 0.005950\n",
      "val_e_mae: 0.052160\n",
      "val_e_rmse: 0.052241\n",
      "val_f_mae: 0.005722\n",
      "val_f_rmse: 0.007723\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0357, Val Loss: 0.0622\n",
      "train_e_mae: 0.019672\n",
      "train_e_rmse: 0.021649\n",
      "train_f_mae: 0.004329\n",
      "train_f_rmse: 0.005941\n",
      "val_e_mae: 0.050687\n",
      "val_e_rmse: 0.050767\n",
      "val_f_mae: 0.005737\n",
      "val_f_rmse: 0.007722\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0356, Val Loss: 0.0621\n",
      "train_e_mae: 0.019665\n",
      "train_e_rmse: 0.021642\n",
      "train_f_mae: 0.004324\n",
      "train_f_rmse: 0.005932\n",
      "val_e_mae: 0.049348\n",
      "val_e_rmse: 0.049426\n",
      "val_f_mae: 0.005756\n",
      "val_f_rmse: 0.007725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 0.9696, Val Loss: 0.6712\n",
      "train_e_mae: 0.175596\n",
      "train_e_rmse: 0.218578\n",
      "train_f_mae: 0.015478\n",
      "train_f_rmse: 0.025579\n",
      "val_e_mae: 0.373147\n",
      "val_e_rmse: 0.373224\n",
      "val_f_mae: 0.016990\n",
      "val_f_rmse: 0.023063\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.3472, Val Loss: 0.7986\n",
      "train_e_mae: 0.148573\n",
      "train_e_rmse: 0.178335\n",
      "train_f_mae: 0.013433\n",
      "train_f_rmse: 0.020645\n",
      "val_e_mae: 0.216636\n",
      "val_e_rmse: 0.216833\n",
      "val_f_mae: 0.016009\n",
      "val_f_rmse: 0.027416\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.1250, Val Loss: 0.6765\n",
      "train_e_mae: 0.137995\n",
      "train_e_rmse: 0.161518\n",
      "train_f_mae: 0.011357\n",
      "train_f_rmse: 0.015557\n",
      "val_e_mae: 0.207586\n",
      "val_e_rmse: 0.207787\n",
      "val_f_mae: 0.014892\n",
      "val_f_rmse: 0.025165\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.1583, Val Loss: 0.5780\n",
      "train_e_mae: 0.065981\n",
      "train_e_rmse: 0.080865\n",
      "train_f_mae: 0.011406\n",
      "train_f_rmse: 0.015520\n",
      "val_e_mae: 0.195901\n",
      "val_e_rmse: 0.196103\n",
      "val_f_mae: 0.013932\n",
      "val_f_rmse: 0.023229\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.1654, Val Loss: 0.5004\n",
      "train_e_mae: 0.042368\n",
      "train_e_rmse: 0.051706\n",
      "train_f_mae: 0.009001\n",
      "train_f_rmse: 0.012143\n",
      "val_e_mae: 0.184141\n",
      "val_e_rmse: 0.184337\n",
      "val_f_mae: 0.013072\n",
      "val_f_rmse: 0.021598\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.1499, Val Loss: 0.4341\n",
      "train_e_mae: 0.045134\n",
      "train_e_rmse: 0.054972\n",
      "train_f_mae: 0.008077\n",
      "train_f_rmse: 0.010834\n",
      "val_e_mae: 0.173381\n",
      "val_e_rmse: 0.173569\n",
      "val_f_mae: 0.012307\n",
      "val_f_rmse: 0.020098\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.0919, Val Loss: 0.3817\n",
      "train_e_mae: 0.034473\n",
      "train_e_rmse: 0.042964\n",
      "train_f_mae: 0.007724\n",
      "train_f_rmse: 0.011524\n",
      "val_e_mae: 0.162461\n",
      "val_e_rmse: 0.162641\n",
      "val_f_mae: 0.011684\n",
      "val_f_rmse: 0.018848\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1023, Val Loss: 0.3338\n",
      "train_e_mae: 0.033463\n",
      "train_e_rmse: 0.039515\n",
      "train_f_mae: 0.007015\n",
      "train_f_rmse: 0.010621\n",
      "val_e_mae: 0.152716\n",
      "val_e_rmse: 0.152889\n",
      "val_f_mae: 0.011097\n",
      "val_f_rmse: 0.017620\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.0771, Val Loss: 0.2911\n",
      "train_e_mae: 0.038855\n",
      "train_e_rmse: 0.047479\n",
      "train_f_mae: 0.006784\n",
      "train_f_rmse: 0.009185\n",
      "val_e_mae: 0.144398\n",
      "val_e_rmse: 0.144567\n",
      "val_f_mae: 0.010543\n",
      "val_f_rmse: 0.016438\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.0585, Val Loss: 0.2539\n",
      "train_e_mae: 0.032832\n",
      "train_e_rmse: 0.038106\n",
      "train_f_mae: 0.006386\n",
      "train_f_rmse: 0.008319\n",
      "val_e_mae: 0.136214\n",
      "val_e_rmse: 0.136378\n",
      "val_f_mae: 0.010034\n",
      "val_f_rmse: 0.015341\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.0492, Val Loss: 0.2232\n",
      "train_e_mae: 0.031608\n",
      "train_e_rmse: 0.038054\n",
      "train_f_mae: 0.006028\n",
      "train_f_rmse: 0.007859\n",
      "val_e_mae: 0.128660\n",
      "val_e_rmse: 0.128819\n",
      "val_f_mae: 0.009569\n",
      "val_f_rmse: 0.014374\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.0488, Val Loss: 0.1962\n",
      "train_e_mae: 0.027231\n",
      "train_e_rmse: 0.031225\n",
      "train_f_mae: 0.005559\n",
      "train_f_rmse: 0.007357\n",
      "val_e_mae: 0.121229\n",
      "val_e_rmse: 0.121382\n",
      "val_f_mae: 0.009137\n",
      "val_f_rmse: 0.013473\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0369, Val Loss: 0.1743\n",
      "train_e_mae: 0.024043\n",
      "train_e_rmse: 0.026843\n",
      "train_f_mae: 0.004675\n",
      "train_f_rmse: 0.006352\n",
      "val_e_mae: 0.114287\n",
      "val_e_rmse: 0.114433\n",
      "val_f_mae: 0.008750\n",
      "val_f_rmse: 0.012698\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.0364, Val Loss: 0.1561\n",
      "train_e_mae: 0.022284\n",
      "train_e_rmse: 0.024596\n",
      "train_f_mae: 0.004446\n",
      "train_f_rmse: 0.006089\n",
      "val_e_mae: 0.107740\n",
      "val_e_rmse: 0.107879\n",
      "val_f_mae: 0.008404\n",
      "val_f_rmse: 0.012019\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0357, Val Loss: 0.1405\n",
      "train_e_mae: 0.021163\n",
      "train_e_rmse: 0.023363\n",
      "train_f_mae: 0.004322\n",
      "train_f_rmse: 0.005948\n",
      "val_e_mae: 0.101544\n",
      "val_e_rmse: 0.101678\n",
      "val_f_mae: 0.008085\n",
      "val_f_rmse: 0.011408\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0349, Val Loss: 0.1275\n",
      "train_e_mae: 0.020577\n",
      "train_e_rmse: 0.022662\n",
      "train_f_mae: 0.004273\n",
      "train_f_rmse: 0.005878\n",
      "val_e_mae: 0.095829\n",
      "val_e_rmse: 0.095957\n",
      "val_f_mae: 0.007799\n",
      "val_f_rmse: 0.010876\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0344, Val Loss: 0.1165\n",
      "train_e_mae: 0.020259\n",
      "train_e_rmse: 0.022257\n",
      "train_f_mae: 0.004245\n",
      "train_f_rmse: 0.005839\n",
      "val_e_mae: 0.090559\n",
      "val_e_rmse: 0.090680\n",
      "val_f_mae: 0.007544\n",
      "val_f_rmse: 0.010408\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0341, Val Loss: 0.1074\n",
      "train_e_mae: 0.020112\n",
      "train_e_rmse: 0.022088\n",
      "train_f_mae: 0.004227\n",
      "train_f_rmse: 0.005810\n",
      "val_e_mae: 0.085730\n",
      "val_e_rmse: 0.085846\n",
      "val_f_mae: 0.007326\n",
      "val_f_rmse: 0.010002\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0339, Val Loss: 0.0997\n",
      "train_e_mae: 0.020022\n",
      "train_e_rmse: 0.021988\n",
      "train_f_mae: 0.004215\n",
      "train_f_rmse: 0.005787\n",
      "val_e_mae: 0.081315\n",
      "val_e_rmse: 0.081427\n",
      "val_f_mae: 0.007140\n",
      "val_f_rmse: 0.009649\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0336, Val Loss: 0.0933\n",
      "train_e_mae: 0.019958\n",
      "train_e_rmse: 0.021918\n",
      "train_f_mae: 0.004204\n",
      "train_f_rmse: 0.005766\n",
      "val_e_mae: 0.077277\n",
      "val_e_rmse: 0.077384\n",
      "val_f_mae: 0.006990\n",
      "val_f_rmse: 0.009344\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0334, Val Loss: 0.0879\n",
      "train_e_mae: 0.019900\n",
      "train_e_rmse: 0.021855\n",
      "train_f_mae: 0.004195\n",
      "train_f_rmse: 0.005749\n",
      "val_e_mae: 0.073588\n",
      "val_e_rmse: 0.073691\n",
      "val_f_mae: 0.006858\n",
      "val_f_rmse: 0.009081\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0333, Val Loss: 0.0834\n",
      "train_e_mae: 0.019852\n",
      "train_e_rmse: 0.021804\n",
      "train_f_mae: 0.004186\n",
      "train_f_rmse: 0.005733\n",
      "val_e_mae: 0.070217\n",
      "val_e_rmse: 0.070317\n",
      "val_f_mae: 0.006741\n",
      "val_f_rmse: 0.008857\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0331, Val Loss: 0.0796\n",
      "train_e_mae: 0.019812\n",
      "train_e_rmse: 0.021760\n",
      "train_f_mae: 0.004179\n",
      "train_f_rmse: 0.005719\n",
      "val_e_mae: 0.067142\n",
      "val_e_rmse: 0.067238\n",
      "val_f_mae: 0.006636\n",
      "val_f_rmse: 0.008665\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0330, Val Loss: 0.0764\n",
      "train_e_mae: 0.019779\n",
      "train_e_rmse: 0.021724\n",
      "train_f_mae: 0.004172\n",
      "train_f_rmse: 0.005706\n",
      "val_e_mae: 0.064338\n",
      "val_e_rmse: 0.064431\n",
      "val_f_mae: 0.006539\n",
      "val_f_rmse: 0.008502\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0328, Val Loss: 0.0738\n",
      "train_e_mae: 0.019750\n",
      "train_e_rmse: 0.021694\n",
      "train_f_mae: 0.004165\n",
      "train_f_rmse: 0.005694\n",
      "val_e_mae: 0.061780\n",
      "val_e_rmse: 0.061871\n",
      "val_f_mae: 0.006453\n",
      "val_f_rmse: 0.008363\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0327, Val Loss: 0.0715\n",
      "train_e_mae: 0.019727\n",
      "train_e_rmse: 0.021668\n",
      "train_f_mae: 0.004160\n",
      "train_f_rmse: 0.005684\n",
      "val_e_mae: 0.059453\n",
      "val_e_rmse: 0.059540\n",
      "val_f_mae: 0.006377\n",
      "val_f_rmse: 0.008244\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0326, Val Loss: 0.0696\n",
      "train_e_mae: 0.019707\n",
      "train_e_rmse: 0.021647\n",
      "train_f_mae: 0.004155\n",
      "train_f_rmse: 0.005674\n",
      "val_e_mae: 0.057334\n",
      "val_e_rmse: 0.057419\n",
      "val_f_mae: 0.006308\n",
      "val_f_rmse: 0.008144\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0325, Val Loss: 0.0680\n",
      "train_e_mae: 0.019690\n",
      "train_e_rmse: 0.021629\n",
      "train_f_mae: 0.004150\n",
      "train_f_rmse: 0.005666\n",
      "val_e_mae: 0.055407\n",
      "val_e_rmse: 0.055491\n",
      "val_f_mae: 0.006246\n",
      "val_f_rmse: 0.008059\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0324, Val Loss: 0.0667\n",
      "train_e_mae: 0.019675\n",
      "train_e_rmse: 0.021613\n",
      "train_f_mae: 0.004146\n",
      "train_f_rmse: 0.005658\n",
      "val_e_mae: 0.053653\n",
      "val_e_rmse: 0.053734\n",
      "val_f_mae: 0.006190\n",
      "val_f_rmse: 0.007986\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0324, Val Loss: 0.0655\n",
      "train_e_mae: 0.019662\n",
      "train_e_rmse: 0.021599\n",
      "train_f_mae: 0.004142\n",
      "train_f_rmse: 0.005651\n",
      "val_e_mae: 0.052059\n",
      "val_e_rmse: 0.052139\n",
      "val_f_mae: 0.006143\n",
      "val_f_rmse: 0.007924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.01 #####\n",
      "Epoch 10, Train Loss: 0.3514, Val Loss: 0.4921\n",
      "train_e_mae: 0.111127\n",
      "train_e_rmse: 0.139344\n",
      "train_f_mae: 0.014297\n",
      "train_f_rmse: 0.022266\n",
      "val_e_mae: 0.063637\n",
      "val_e_rmse: 0.063999\n",
      "val_f_mae: 0.014996\n",
      "val_f_rmse: 0.022091\n",
      "##### Step: 19 Learning rate: 0.009000000000000001 #####\n",
      "Epoch 20, Train Loss: 0.1836, Val Loss: 0.4915\n",
      "train_e_mae: 0.118975\n",
      "train_e_rmse: 0.132404\n",
      "train_f_mae: 0.010994\n",
      "train_f_rmse: 0.015865\n",
      "val_e_mae: 0.026018\n",
      "val_e_rmse: 0.026638\n",
      "val_f_mae: 0.015351\n",
      "val_f_rmse: 0.022154\n",
      "##### Step: 29 Learning rate: 0.008100000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2072, Val Loss: 0.4188\n",
      "train_e_mae: 0.065789\n",
      "train_e_rmse: 0.079406\n",
      "train_f_mae: 0.012688\n",
      "train_f_rmse: 0.018126\n",
      "val_e_mae: 0.016480\n",
      "val_e_rmse: 0.017434\n",
      "val_f_mae: 0.014212\n",
      "val_f_rmse: 0.020456\n",
      "##### Step: 39 Learning rate: 0.007290000000000001 #####\n",
      "Epoch 40, Train Loss: 0.1366, Val Loss: 0.3577\n",
      "train_e_mae: 0.059335\n",
      "train_e_rmse: 0.073157\n",
      "train_f_mae: 0.009408\n",
      "train_f_rmse: 0.013095\n",
      "val_e_mae: 0.009111\n",
      "val_e_rmse: 0.010733\n",
      "val_f_mae: 0.013196\n",
      "val_f_rmse: 0.018911\n",
      "##### Step: 49 Learning rate: 0.006561000000000002 #####\n",
      "Epoch 50, Train Loss: 0.1306, Val Loss: 0.3019\n",
      "train_e_mae: 0.054513\n",
      "train_e_rmse: 0.067361\n",
      "train_f_mae: 0.009654\n",
      "train_f_rmse: 0.013588\n",
      "val_e_mae: 0.004852\n",
      "val_e_rmse: 0.005598\n",
      "val_f_mae: 0.012152\n",
      "val_f_rmse: 0.017375\n",
      "##### Step: 59 Learning rate: 0.005904900000000002 #####\n",
      "Epoch 60, Train Loss: 0.0907, Val Loss: 0.2596\n",
      "train_e_mae: 0.082586\n",
      "train_e_rmse: 0.096613\n",
      "train_f_mae: 0.007818\n",
      "train_f_rmse: 0.010759\n",
      "val_e_mae: 0.006575\n",
      "val_e_rmse: 0.007940\n",
      "val_f_mae: 0.011321\n",
      "val_f_rmse: 0.016109\n",
      "##### Step: 69 Learning rate: 0.005314410000000002 #####\n",
      "Epoch 70, Train Loss: 0.0958, Val Loss: 0.2246\n",
      "train_e_mae: 0.058789\n",
      "train_e_rmse: 0.069471\n",
      "train_f_mae: 0.008064\n",
      "train_f_rmse: 0.011450\n",
      "val_e_mae: 0.010545\n",
      "val_e_rmse: 0.011764\n",
      "val_f_mae: 0.010557\n",
      "val_f_rmse: 0.014983\n",
      "##### Step: 79 Learning rate: 0.004782969000000002 #####\n",
      "Epoch 80, Train Loss: 0.1409, Val Loss: 0.1988\n",
      "train_e_mae: 0.034719\n",
      "train_e_rmse: 0.042396\n",
      "train_f_mae: 0.006979\n",
      "train_f_rmse: 0.010045\n",
      "val_e_mae: 0.014591\n",
      "val_e_rmse: 0.015425\n",
      "val_f_mae: 0.009955\n",
      "val_f_rmse: 0.014090\n",
      "##### Step: 89 Learning rate: 0.004304672100000002 #####\n",
      "Epoch 90, Train Loss: 0.1016, Val Loss: 0.1704\n",
      "train_e_mae: 0.036069\n",
      "train_e_rmse: 0.044287\n",
      "train_f_mae: 0.006369\n",
      "train_f_rmse: 0.009011\n",
      "val_e_mae: 0.017452\n",
      "val_e_rmse: 0.018104\n",
      "val_f_mae: 0.009268\n",
      "val_f_rmse: 0.013043\n",
      "##### Step: 99 Learning rate: 0.003874204890000002 #####\n",
      "Epoch 100, Train Loss: 0.0769, Val Loss: 0.1517\n",
      "train_e_mae: 0.030576\n",
      "train_e_rmse: 0.036906\n",
      "train_f_mae: 0.006054\n",
      "train_f_rmse: 0.008299\n",
      "val_e_mae: 0.021193\n",
      "val_e_rmse: 0.021703\n",
      "val_f_mae: 0.008781\n",
      "val_f_rmse: 0.012297\n",
      "##### Step: 109 Learning rate: 0.003486784401000002 #####\n",
      "Epoch 110, Train Loss: 0.0685, Val Loss: 0.1324\n",
      "train_e_mae: 0.029017\n",
      "train_e_rmse: 0.034178\n",
      "train_f_mae: 0.005760\n",
      "train_f_rmse: 0.007742\n",
      "val_e_mae: 0.023695\n",
      "val_e_rmse: 0.024125\n",
      "val_f_mae: 0.008248\n",
      "val_f_rmse: 0.011481\n",
      "##### Step: 119 Learning rate: 0.003138105960900002 #####\n",
      "Epoch 120, Train Loss: 0.0578, Val Loss: 0.1193\n",
      "train_e_mae: 0.028375\n",
      "train_e_rmse: 0.033536\n",
      "train_f_mae: 0.005478\n",
      "train_f_rmse: 0.007291\n",
      "val_e_mae: 0.026601\n",
      "val_e_rmse: 0.026963\n",
      "val_f_mae: 0.007869\n",
      "val_f_rmse: 0.010890\n",
      "##### Step: 129 Learning rate: 0.0028242953648100018 #####\n",
      "Epoch 130, Train Loss: 0.0445, Val Loss: 0.1064\n",
      "train_e_mae: 0.026337\n",
      "train_e_rmse: 0.030532\n",
      "train_f_mae: 0.005041\n",
      "train_f_rmse: 0.006716\n",
      "val_e_mae: 0.028534\n",
      "val_e_rmse: 0.028852\n",
      "val_f_mae: 0.007468\n",
      "val_f_rmse: 0.010274\n",
      "##### Step: 139 Learning rate: 0.0025418658283290017 #####\n",
      "Epoch 140, Train Loss: 0.0355, Val Loss: 0.0966\n",
      "train_e_mae: 0.023918\n",
      "train_e_rmse: 0.026870\n",
      "train_f_mae: 0.004404\n",
      "train_f_rmse: 0.005957\n",
      "val_e_mae: 0.030283\n",
      "val_e_rmse: 0.030563\n",
      "val_f_mae: 0.007150\n",
      "val_f_rmse: 0.009783\n",
      "##### Step: 149 Learning rate: 0.0022876792454961017 #####\n",
      "Epoch 150, Train Loss: 0.0322, Val Loss: 0.0881\n",
      "train_e_mae: 0.022201\n",
      "train_e_rmse: 0.024666\n",
      "train_f_mae: 0.004177\n",
      "train_f_rmse: 0.005714\n",
      "val_e_mae: 0.031507\n",
      "val_e_rmse: 0.031757\n",
      "val_f_mae: 0.006853\n",
      "val_f_rmse: 0.009335\n",
      "##### Step: 159 Learning rate: 0.0020589113209464917 #####\n",
      "Epoch 160, Train Loss: 0.0320, Val Loss: 0.0814\n",
      "train_e_mae: 0.021134\n",
      "train_e_rmse: 0.023311\n",
      "train_f_mae: 0.004101\n",
      "train_f_rmse: 0.005626\n",
      "val_e_mae: 0.032488\n",
      "val_e_rmse: 0.032712\n",
      "val_f_mae: 0.006595\n",
      "val_f_rmse: 0.008960\n",
      "##### Step: 169 Learning rate: 0.0018530201888518425 #####\n",
      "Epoch 170, Train Loss: 0.0313, Val Loss: 0.0758\n",
      "train_e_mae: 0.020495\n",
      "train_e_rmse: 0.022529\n",
      "train_f_mae: 0.004067\n",
      "train_f_rmse: 0.005572\n",
      "val_e_mae: 0.033279\n",
      "val_e_rmse: 0.033482\n",
      "val_f_mae: 0.006371\n",
      "val_f_rmse: 0.008643\n",
      "##### Step: 179 Learning rate: 0.0016677181699666583 #####\n",
      "Epoch 180, Train Loss: 0.0311, Val Loss: 0.0713\n",
      "train_e_mae: 0.020142\n",
      "train_e_rmse: 0.022107\n",
      "train_f_mae: 0.004051\n",
      "train_f_rmse: 0.005544\n",
      "val_e_mae: 0.033906\n",
      "val_e_rmse: 0.034091\n",
      "val_f_mae: 0.006186\n",
      "val_f_rmse: 0.008374\n",
      "##### Step: 189 Learning rate: 0.0015009463529699924 #####\n",
      "Epoch 190, Train Loss: 0.0309, Val Loss: 0.0676\n",
      "train_e_mae: 0.019918\n",
      "train_e_rmse: 0.021846\n",
      "train_f_mae: 0.004039\n",
      "train_f_rmse: 0.005521\n",
      "val_e_mae: 0.034429\n",
      "val_e_rmse: 0.034598\n",
      "val_f_mae: 0.006030\n",
      "val_f_rmse: 0.008149\n",
      "##### Step: 199 Learning rate: 0.0013508517176729932 #####\n",
      "Epoch 200, Train Loss: 0.0307, Val Loss: 0.0646\n",
      "train_e_mae: 0.019783\n",
      "train_e_rmse: 0.021694\n",
      "train_f_mae: 0.004030\n",
      "train_f_rmse: 0.005503\n",
      "val_e_mae: 0.034863\n",
      "val_e_rmse: 0.035019\n",
      "val_f_mae: 0.005902\n",
      "val_f_rmse: 0.007962\n",
      "##### Step: 209 Learning rate: 0.001215766545905694 #####\n",
      "Epoch 210, Train Loss: 0.0305, Val Loss: 0.0622\n",
      "train_e_mae: 0.019695\n",
      "train_e_rmse: 0.021596\n",
      "train_f_mae: 0.004023\n",
      "train_f_rmse: 0.005488\n",
      "val_e_mae: 0.035227\n",
      "val_e_rmse: 0.035372\n",
      "val_f_mae: 0.005798\n",
      "val_f_rmse: 0.007807\n",
      "##### Step: 219 Learning rate: 0.0010941898913151245 #####\n",
      "Epoch 220, Train Loss: 0.0304, Val Loss: 0.0603\n",
      "train_e_mae: 0.019629\n",
      "train_e_rmse: 0.021523\n",
      "train_f_mae: 0.004016\n",
      "train_f_rmse: 0.005474\n",
      "val_e_mae: 0.035535\n",
      "val_e_rmse: 0.035669\n",
      "val_f_mae: 0.005719\n",
      "val_f_rmse: 0.007680\n",
      "##### Step: 229 Learning rate: 0.0009847709021836122 #####\n",
      "Epoch 230, Train Loss: 0.0302, Val Loss: 0.0587\n",
      "train_e_mae: 0.019576\n",
      "train_e_rmse: 0.021465\n",
      "train_f_mae: 0.004010\n",
      "train_f_rmse: 0.005461\n",
      "val_e_mae: 0.035794\n",
      "val_e_rmse: 0.035919\n",
      "val_f_mae: 0.005654\n",
      "val_f_rmse: 0.007576\n",
      "##### Step: 239 Learning rate: 0.0008862938119652509 #####\n",
      "Epoch 240, Train Loss: 0.0301, Val Loss: 0.0574\n",
      "train_e_mae: 0.019533\n",
      "train_e_rmse: 0.021417\n",
      "train_f_mae: 0.004004\n",
      "train_f_rmse: 0.005450\n",
      "val_e_mae: 0.036008\n",
      "val_e_rmse: 0.036126\n",
      "val_f_mae: 0.005608\n",
      "val_f_rmse: 0.007492\n",
      "##### Step: 249 Learning rate: 0.0007976644307687258 #####\n",
      "Epoch 250, Train Loss: 0.0300, Val Loss: 0.0564\n",
      "train_e_mae: 0.019498\n",
      "train_e_rmse: 0.021380\n",
      "train_f_mae: 0.003999\n",
      "train_f_rmse: 0.005440\n",
      "val_e_mae: 0.036183\n",
      "val_e_rmse: 0.036295\n",
      "val_f_mae: 0.005583\n",
      "val_f_rmse: 0.007425\n",
      "##### Step: 259 Learning rate: 0.0007178979876918532 #####\n",
      "Epoch 260, Train Loss: 0.0299, Val Loss: 0.0557\n",
      "train_e_mae: 0.019469\n",
      "train_e_rmse: 0.021348\n",
      "train_f_mae: 0.003994\n",
      "train_f_rmse: 0.005431\n",
      "val_e_mae: 0.036328\n",
      "val_e_rmse: 0.036434\n",
      "val_f_mae: 0.005569\n",
      "val_f_rmse: 0.007370\n",
      "##### Step: 269 Learning rate: 0.0006461081889226679 #####\n",
      "Epoch 270, Train Loss: 0.0298, Val Loss: 0.0550\n",
      "train_e_mae: 0.019444\n",
      "train_e_rmse: 0.021321\n",
      "train_f_mae: 0.003990\n",
      "train_f_rmse: 0.005422\n",
      "val_e_mae: 0.036443\n",
      "val_e_rmse: 0.036544\n",
      "val_f_mae: 0.005557\n",
      "val_f_rmse: 0.007327\n",
      "##### Step: 279 Learning rate: 0.0005814973700304011 #####\n",
      "Epoch 280, Train Loss: 0.0297, Val Loss: 0.0545\n",
      "train_e_mae: 0.019423\n",
      "train_e_rmse: 0.021298\n",
      "train_f_mae: 0.003986\n",
      "train_f_rmse: 0.005415\n",
      "val_e_mae: 0.036534\n",
      "val_e_rmse: 0.036630\n",
      "val_f_mae: 0.005548\n",
      "val_f_rmse: 0.007293\n",
      "##### Step: 289 Learning rate: 0.0005233476330273611 #####\n",
      "Epoch 290, Train Loss: 0.0297, Val Loss: 0.0541\n",
      "train_e_mae: 0.019405\n",
      "train_e_rmse: 0.021279\n",
      "train_f_mae: 0.003982\n",
      "train_f_rmse: 0.005408\n",
      "val_e_mae: 0.036603\n",
      "val_e_rmse: 0.036695\n",
      "val_f_mae: 0.005545\n",
      "val_f_rmse: 0.007266\n",
      "##### Step: 299 Learning rate: 0.000471012869724625 #####\n",
      "Epoch 300, Train Loss: 0.0296, Val Loss: 0.0538\n",
      "train_e_mae: 0.019390\n",
      "train_e_rmse: 0.021262\n",
      "train_f_mae: 0.003979\n",
      "train_f_rmse: 0.005402\n",
      "val_e_mae: 0.036655\n",
      "val_e_rmse: 0.036743\n",
      "val_f_mae: 0.005541\n",
      "val_f_rmse: 0.007245\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-2, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 10, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=300, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edd317e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 3.1119, Val Loss: 1.7751\n",
      "train_e_mae: 0.032843\n",
      "train_e_rmse: 0.039328\n",
      "train_f_mae: 0.006265\n",
      "train_f_rmse: 0.009363\n",
      "val_e_mae: 0.038237\n",
      "val_e_rmse: 0.038357\n",
      "val_f_mae: 0.013117\n",
      "val_f_rmse: 0.017432\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 2.9113, Val Loss: 2.6816\n",
      "train_e_mae: 0.024278\n",
      "train_e_rmse: 0.029022\n",
      "train_f_mae: 0.008515\n",
      "train_f_rmse: 0.012462\n",
      "val_e_mae: 0.049064\n",
      "val_e_rmse: 0.049162\n",
      "val_f_mae: 0.012475\n",
      "val_f_rmse: 0.016268\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 2.1656, Val Loss: 1.9758\n",
      "train_e_mae: 0.042574\n",
      "train_e_rmse: 0.047996\n",
      "train_f_mae: 0.007934\n",
      "train_f_rmse: 0.011259\n",
      "val_e_mae: 0.041984\n",
      "val_e_rmse: 0.042068\n",
      "val_f_mae: 0.011105\n",
      "val_f_rmse: 0.014355\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 1.5631, Val Loss: 1.3404\n",
      "train_e_mae: 0.034409\n",
      "train_e_rmse: 0.039702\n",
      "train_f_mae: 0.007673\n",
      "train_f_rmse: 0.010880\n",
      "val_e_mae: 0.034125\n",
      "val_e_rmse: 0.034204\n",
      "val_f_mae: 0.010122\n",
      "val_f_rmse: 0.013056\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 2.6554, Val Loss: 0.9804\n",
      "train_e_mae: 0.036749\n",
      "train_e_rmse: 0.040835\n",
      "train_f_mae: 0.005216\n",
      "train_f_rmse: 0.006864\n",
      "val_e_mae: 0.028714\n",
      "val_e_rmse: 0.028793\n",
      "val_f_mae: 0.009552\n",
      "val_f_rmse: 0.012303\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.4128, Val Loss: 0.8143\n",
      "train_e_mae: 0.030341\n",
      "train_e_rmse: 0.033825\n",
      "train_f_mae: 0.005053\n",
      "train_f_rmse: 0.006771\n",
      "val_e_mae: 0.025936\n",
      "val_e_rmse: 0.026014\n",
      "val_f_mae: 0.009132\n",
      "val_f_rmse: 0.011727\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 1.1248, Val Loss: 0.6792\n",
      "train_e_mae: 0.030120\n",
      "train_e_rmse: 0.032338\n",
      "train_f_mae: 0.004630\n",
      "train_f_rmse: 0.006226\n",
      "val_e_mae: 0.023468\n",
      "val_e_rmse: 0.023544\n",
      "val_f_mae: 0.008724\n",
      "val_f_rmse: 0.011176\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.8427, Val Loss: 0.5097\n",
      "train_e_mae: 0.027077\n",
      "train_e_rmse: 0.029944\n",
      "train_f_mae: 0.004426\n",
      "train_f_rmse: 0.005966\n",
      "val_e_mae: 0.019834\n",
      "val_e_rmse: 0.019911\n",
      "val_f_mae: 0.008317\n",
      "val_f_rmse: 0.010641\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 1.6018, Val Loss: 0.3753\n",
      "train_e_mae: 0.029151\n",
      "train_e_rmse: 0.032075\n",
      "train_f_mae: 0.004403\n",
      "train_f_rmse: 0.005933\n",
      "val_e_mae: 0.016407\n",
      "val_e_rmse: 0.016487\n",
      "val_f_mae: 0.007964\n",
      "val_f_rmse: 0.010174\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.3642, Val Loss: 0.3131\n",
      "train_e_mae: 0.024297\n",
      "train_e_rmse: 0.026942\n",
      "train_f_mae: 0.004370\n",
      "train_f_rmse: 0.005914\n",
      "val_e_mae: 0.014651\n",
      "val_e_rmse: 0.014730\n",
      "val_f_mae: 0.007689\n",
      "val_f_rmse: 0.009802\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.9401, Val Loss: 0.2732\n",
      "train_e_mae: 0.024926\n",
      "train_e_rmse: 0.026861\n",
      "train_f_mae: 0.004360\n",
      "train_f_rmse: 0.005924\n",
      "val_e_mae: 0.013456\n",
      "val_e_rmse: 0.013533\n",
      "val_f_mae: 0.007455\n",
      "val_f_rmse: 0.009489\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.4285, Val Loss: 0.2185\n",
      "train_e_mae: 0.022003\n",
      "train_e_rmse: 0.024269\n",
      "train_f_mae: 0.004325\n",
      "train_f_rmse: 0.005875\n",
      "val_e_mae: 0.011493\n",
      "val_e_rmse: 0.011573\n",
      "val_f_mae: 0.007229\n",
      "val_f_rmse: 0.009198\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.8612, Val Loss: 0.1738\n",
      "train_e_mae: 0.022957\n",
      "train_e_rmse: 0.024925\n",
      "train_f_mae: 0.004320\n",
      "train_f_rmse: 0.005860\n",
      "val_e_mae: 0.009599\n",
      "val_e_rmse: 0.009685\n",
      "val_f_mae: 0.007020\n",
      "val_f_rmse: 0.008943\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.3704, Val Loss: 0.1566\n",
      "train_e_mae: 0.019707\n",
      "train_e_rmse: 0.021813\n",
      "train_f_mae: 0.004295\n",
      "train_f_rmse: 0.005843\n",
      "val_e_mae: 0.008871\n",
      "val_e_rmse: 0.008955\n",
      "val_f_mae: 0.006849\n",
      "val_f_rmse: 0.008738\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.8215, Val Loss: 0.1472\n",
      "train_e_mae: 0.020960\n",
      "train_e_rmse: 0.023024\n",
      "train_f_mae: 0.004302\n",
      "train_f_rmse: 0.005866\n",
      "val_e_mae: 0.008510\n",
      "val_e_rmse: 0.008591\n",
      "val_f_mae: 0.006703\n",
      "val_f_rmse: 0.008567\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2048, Val Loss: 0.1285\n",
      "train_e_mae: 0.017916\n",
      "train_e_rmse: 0.020236\n",
      "train_f_mae: 0.004279\n",
      "train_f_rmse: 0.005833\n",
      "val_e_mae: 0.007515\n",
      "val_e_rmse: 0.007599\n",
      "val_f_mae: 0.006565\n",
      "val_f_rmse: 0.008412\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.4328, Val Loss: 0.1125\n",
      "train_e_mae: 0.017923\n",
      "train_e_rmse: 0.019699\n",
      "train_f_mae: 0.004273\n",
      "train_f_rmse: 0.005818\n",
      "val_e_mae: 0.006543\n",
      "val_e_rmse: 0.006632\n",
      "val_f_mae: 0.006443\n",
      "val_f_rmse: 0.008277\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.3848, Val Loss: 0.1095\n",
      "train_e_mae: 0.016135\n",
      "train_e_rmse: 0.018336\n",
      "train_f_mae: 0.004263\n",
      "train_f_rmse: 0.005816\n",
      "val_e_mae: 0.006453\n",
      "val_e_rmse: 0.006538\n",
      "val_f_mae: 0.006343\n",
      "val_f_rmse: 0.008173\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.7554, Val Loss: 0.1103\n",
      "train_e_mae: 0.018121\n",
      "train_e_rmse: 0.020605\n",
      "train_f_mae: 0.004279\n",
      "train_f_rmse: 0.005844\n",
      "val_e_mae: 0.006622\n",
      "val_e_rmse: 0.006700\n",
      "val_f_mae: 0.006255\n",
      "val_f_rmse: 0.008089\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.0819, Val Loss: 0.1052\n",
      "train_e_mae: 0.015223\n",
      "train_e_rmse: 0.018118\n",
      "train_f_mae: 0.004262\n",
      "train_f_rmse: 0.005817\n",
      "val_e_mae: 0.006332\n",
      "val_e_rmse: 0.006409\n",
      "val_f_mae: 0.006171\n",
      "val_f_rmse: 0.008010\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.0624, Val Loss: 0.0958\n",
      "train_e_mae: 0.006794\n",
      "train_e_rmse: 0.008454\n",
      "train_f_mae: 0.004212\n",
      "train_f_rmse: 0.005744\n",
      "val_e_mae: 0.005640\n",
      "val_e_rmse: 0.005724\n",
      "val_f_mae: 0.006095\n",
      "val_f_rmse: 0.007941\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.5287, Val Loss: 0.0905\n",
      "train_e_mae: 0.014460\n",
      "train_e_rmse: 0.016513\n",
      "train_f_mae: 0.004253\n",
      "train_f_rmse: 0.005789\n",
      "val_e_mae: 0.005230\n",
      "val_e_rmse: 0.005317\n",
      "val_f_mae: 0.006039\n",
      "val_f_rmse: 0.007885\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.1318, Val Loss: 0.0839\n",
      "train_e_mae: 0.007207\n",
      "train_e_rmse: 0.008728\n",
      "train_f_mae: 0.004217\n",
      "train_f_rmse: 0.005747\n",
      "val_e_mae: 0.004645\n",
      "val_e_rmse: 0.004740\n",
      "val_f_mae: 0.006000\n",
      "val_f_rmse: 0.007838\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.0685, Val Loss: 0.0797\n",
      "train_e_mae: 0.013180\n",
      "train_e_rmse: 0.015342\n",
      "train_f_mae: 0.004244\n",
      "train_f_rmse: 0.005783\n",
      "val_e_mae: 0.004239\n",
      "val_e_rmse: 0.004341\n",
      "val_f_mae: 0.005977\n",
      "val_f_rmse: 0.007800\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.1953, Val Loss: 0.0744\n",
      "train_e_mae: 0.006718\n",
      "train_e_rmse: 0.008221\n",
      "train_f_mae: 0.004215\n",
      "train_f_rmse: 0.005746\n",
      "val_e_mae: 0.003640\n",
      "val_e_rmse: 0.003755\n",
      "val_f_mae: 0.005955\n",
      "val_f_rmse: 0.007767\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.0859, Val Loss: 0.0743\n",
      "train_e_mae: 0.011796\n",
      "train_e_rmse: 0.013886\n",
      "train_f_mae: 0.004235\n",
      "train_f_rmse: 0.005777\n",
      "val_e_mae: 0.003665\n",
      "val_e_rmse: 0.003779\n",
      "val_f_mae: 0.005940\n",
      "val_f_rmse: 0.007746\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.1630, Val Loss: 0.0707\n",
      "train_e_mae: 0.005744\n",
      "train_e_rmse: 0.006992\n",
      "train_f_mae: 0.004212\n",
      "train_f_rmse: 0.005746\n",
      "val_e_mae: 0.003196\n",
      "val_e_rmse: 0.003325\n",
      "val_f_mae: 0.005924\n",
      "val_f_rmse: 0.007723\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.0617, Val Loss: 0.0719\n",
      "train_e_mae: 0.010788\n",
      "train_e_rmse: 0.012972\n",
      "train_f_mae: 0.004231\n",
      "train_f_rmse: 0.005775\n",
      "val_e_mae: 0.003411\n",
      "val_e_rmse: 0.003532\n",
      "val_f_mae: 0.005914\n",
      "val_f_rmse: 0.007712\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.4376, Val Loss: 0.0732\n",
      "train_e_mae: 0.010342\n",
      "train_e_rmse: 0.012370\n",
      "train_f_mae: 0.004226\n",
      "train_f_rmse: 0.005772\n",
      "val_e_mae: 0.003610\n",
      "val_e_rmse: 0.003725\n",
      "val_f_mae: 0.005907\n",
      "val_f_rmse: 0.007702\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2108, Val Loss: 0.0675\n",
      "train_e_mae: 0.013835\n",
      "train_e_rmse: 0.015843\n",
      "train_f_mae: 0.004264\n",
      "train_f_rmse: 0.005794\n",
      "val_e_mae: 0.002775\n",
      "val_e_rmse: 0.002922\n",
      "val_f_mae: 0.005894\n",
      "val_f_rmse: 0.007681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.1027, Val Loss: 0.0652\n",
      "train_e_mae: 0.006184\n",
      "train_e_rmse: 0.007410\n",
      "train_f_mae: 0.004204\n",
      "train_f_rmse: 0.005747\n",
      "val_e_mae: 0.002347\n",
      "val_e_rmse: 0.002519\n",
      "val_f_mae: 0.005887\n",
      "val_f_rmse: 0.007670\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.0751, Val Loss: 0.0647\n",
      "train_e_mae: 0.004865\n",
      "train_e_rmse: 0.006007\n",
      "train_f_mae: 0.004205\n",
      "train_f_rmse: 0.005749\n",
      "val_e_mae: 0.002254\n",
      "val_e_rmse: 0.002435\n",
      "val_f_mae: 0.005885\n",
      "val_f_rmse: 0.007663\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.0658, Val Loss: 0.0633\n",
      "train_e_mae: 0.005529\n",
      "train_e_rmse: 0.006841\n",
      "train_f_mae: 0.004211\n",
      "train_f_rmse: 0.005752\n",
      "val_e_mae: 0.001969\n",
      "val_e_rmse: 0.002175\n",
      "val_f_mae: 0.005882\n",
      "val_f_rmse: 0.007655\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.0647, Val Loss: 0.0622\n",
      "train_e_mae: 0.005314\n",
      "train_e_rmse: 0.006538\n",
      "train_f_mae: 0.004205\n",
      "train_f_rmse: 0.005752\n",
      "val_e_mae: 0.001692\n",
      "val_e_rmse: 0.001931\n",
      "val_f_mae: 0.005880\n",
      "val_f_rmse: 0.007650\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.0619, Val Loss: 0.0620\n",
      "train_e_mae: 0.004932\n",
      "train_e_rmse: 0.005870\n",
      "train_f_mae: 0.004193\n",
      "train_f_rmse: 0.005750\n",
      "val_e_mae: 0.001642\n",
      "val_e_rmse: 0.001890\n",
      "val_f_mae: 0.005878\n",
      "val_f_rmse: 0.007645\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.0927, Val Loss: 0.0624\n",
      "train_e_mae: 0.005123\n",
      "train_e_rmse: 0.006079\n",
      "train_f_mae: 0.004203\n",
      "train_f_rmse: 0.005747\n",
      "val_e_mae: 0.001766\n",
      "val_e_rmse: 0.002002\n",
      "val_f_mae: 0.005878\n",
      "val_f_rmse: 0.007642\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.0494, Val Loss: 0.0625\n",
      "train_e_mae: 0.004523\n",
      "train_e_rmse: 0.005437\n",
      "train_f_mae: 0.004193\n",
      "train_f_rmse: 0.005746\n",
      "val_e_mae: 0.001812\n",
      "val_e_rmse: 0.002046\n",
      "val_f_mae: 0.005878\n",
      "val_f_rmse: 0.007639\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.0494, Val Loss: 0.0631\n",
      "train_e_mae: 0.005228\n",
      "train_e_rmse: 0.006130\n",
      "train_f_mae: 0.004197\n",
      "train_f_rmse: 0.005749\n",
      "val_e_mae: 0.001968\n",
      "val_e_rmse: 0.002189\n",
      "val_f_mae: 0.005879\n",
      "val_f_rmse: 0.007638\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.0490, Val Loss: 0.0634\n",
      "train_e_mae: 0.004185\n",
      "train_e_rmse: 0.005110\n",
      "train_f_mae: 0.004192\n",
      "train_f_rmse: 0.005744\n",
      "val_e_mae: 0.002050\n",
      "val_e_rmse: 0.002266\n",
      "val_f_mae: 0.005879\n",
      "val_f_rmse: 0.007636\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.0489, Val Loss: 0.0632\n",
      "train_e_mae: 0.004143\n",
      "train_e_rmse: 0.005086\n",
      "train_f_mae: 0.004189\n",
      "train_f_rmse: 0.005741\n",
      "val_e_mae: 0.001997\n",
      "val_e_rmse: 0.002222\n",
      "val_f_mae: 0.005878\n",
      "val_f_rmse: 0.007632\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.6081, Val Loss: 0.5953\n",
      "train_e_mae: 0.017117\n",
      "train_e_rmse: 0.018690\n",
      "train_f_mae: 0.004252\n",
      "train_f_rmse: 0.005829\n",
      "val_e_mae: 0.023061\n",
      "val_e_rmse: 0.023087\n",
      "val_f_mae: 0.006167\n",
      "val_f_rmse: 0.007893\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.3689, Val Loss: 0.3911\n",
      "train_e_mae: 0.017811\n",
      "train_e_rmse: 0.018281\n",
      "train_f_mae: 0.004266\n",
      "train_f_rmse: 0.005840\n",
      "val_e_mae: 0.018256\n",
      "val_e_rmse: 0.018282\n",
      "val_f_mae: 0.005731\n",
      "val_f_rmse: 0.007541\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.1675, Val Loss: 0.3248\n",
      "train_e_mae: 0.013454\n",
      "train_e_rmse: 0.014197\n",
      "train_f_mae: 0.004241\n",
      "train_f_rmse: 0.005810\n",
      "val_e_mae: 0.016342\n",
      "val_e_rmse: 0.016371\n",
      "val_f_mae: 0.005730\n",
      "val_f_rmse: 0.007533\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2695, Val Loss: 0.2842\n",
      "train_e_mae: 0.012736\n",
      "train_e_rmse: 0.013418\n",
      "train_f_mae: 0.004231\n",
      "train_f_rmse: 0.005805\n",
      "val_e_mae: 0.015054\n",
      "val_e_rmse: 0.015087\n",
      "val_f_mae: 0.005724\n",
      "val_f_rmse: 0.007524\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.1423, Val Loss: 0.2374\n",
      "train_e_mae: 0.011265\n",
      "train_e_rmse: 0.012003\n",
      "train_f_mae: 0.004225\n",
      "train_f_rmse: 0.005794\n",
      "val_e_mae: 0.013413\n",
      "val_e_rmse: 0.013450\n",
      "val_f_mae: 0.005720\n",
      "val_f_rmse: 0.007515\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2068, Val Loss: 0.2082\n",
      "train_e_mae: 0.011166\n",
      "train_e_rmse: 0.011778\n",
      "train_f_mae: 0.004221\n",
      "train_f_rmse: 0.005790\n",
      "val_e_mae: 0.012281\n",
      "val_e_rmse: 0.012322\n",
      "val_f_mae: 0.005714\n",
      "val_f_rmse: 0.007505\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.1314, Val Loss: 0.1746\n",
      "train_e_mae: 0.010034\n",
      "train_e_rmse: 0.010680\n",
      "train_f_mae: 0.004214\n",
      "train_f_rmse: 0.005784\n",
      "val_e_mae: 0.010834\n",
      "val_e_rmse: 0.010882\n",
      "val_f_mae: 0.005710\n",
      "val_f_rmse: 0.007497\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.1627, Val Loss: 0.1540\n",
      "train_e_mae: 0.010002\n",
      "train_e_rmse: 0.010564\n",
      "train_f_mae: 0.004209\n",
      "train_f_rmse: 0.005782\n",
      "val_e_mae: 0.009841\n",
      "val_e_rmse: 0.009895\n",
      "val_f_mae: 0.005703\n",
      "val_f_rmse: 0.007487\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.1232, Val Loss: 0.1304\n",
      "train_e_mae: 0.009007\n",
      "train_e_rmse: 0.009602\n",
      "train_f_mae: 0.004201\n",
      "train_f_rmse: 0.005775\n",
      "val_e_mae: 0.008564\n",
      "val_e_rmse: 0.008627\n",
      "val_f_mae: 0.005699\n",
      "val_f_rmse: 0.007479\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.1282, Val Loss: 0.1160\n",
      "train_e_mae: 0.008986\n",
      "train_e_rmse: 0.009534\n",
      "train_f_mae: 0.004196\n",
      "train_f_rmse: 0.005773\n",
      "val_e_mae: 0.007690\n",
      "val_e_rmse: 0.007762\n",
      "val_f_mae: 0.005692\n",
      "val_f_rmse: 0.007470\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.1167, Val Loss: 0.0998\n",
      "train_e_mae: 0.008097\n",
      "train_e_rmse: 0.008679\n",
      "train_f_mae: 0.004189\n",
      "train_f_rmse: 0.005767\n",
      "val_e_mae: 0.006557\n",
      "val_e_rmse: 0.006643\n",
      "val_f_mae: 0.005688\n",
      "val_f_rmse: 0.007462\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.1020, Val Loss: 0.0902\n",
      "train_e_mae: 0.008079\n",
      "train_e_rmse: 0.008654\n",
      "train_f_mae: 0.004185\n",
      "train_f_rmse: 0.005765\n",
      "val_e_mae: 0.005784\n",
      "val_e_rmse: 0.005883\n",
      "val_f_mae: 0.005681\n",
      "val_f_rmse: 0.007453\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.1112, Val Loss: 0.0794\n",
      "train_e_mae: 0.007281\n",
      "train_e_rmse: 0.007897\n",
      "train_f_mae: 0.004178\n",
      "train_f_rmse: 0.005759\n",
      "val_e_mae: 0.004777\n",
      "val_e_rmse: 0.004898\n",
      "val_f_mae: 0.005676\n",
      "val_f_rmse: 0.007446\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.0822, Val Loss: 0.0733\n",
      "train_e_mae: 0.007267\n",
      "train_e_rmse: 0.007909\n",
      "train_f_mae: 0.004174\n",
      "train_f_rmse: 0.005757\n",
      "val_e_mae: 0.004092\n",
      "val_e_rmse: 0.004236\n",
      "val_f_mae: 0.005670\n",
      "val_f_rmse: 0.007437\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.1066, Val Loss: 0.0666\n",
      "train_e_mae: 0.006550\n",
      "train_e_rmse: 0.007244\n",
      "train_f_mae: 0.004168\n",
      "train_f_rmse: 0.005752\n",
      "val_e_mae: 0.003195\n",
      "val_e_rmse: 0.003380\n",
      "val_f_mae: 0.005666\n",
      "val_f_rmse: 0.007430\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.0674, Val Loss: 0.0630\n",
      "train_e_mae: 0.006538\n",
      "train_e_rmse: 0.007291\n",
      "train_f_mae: 0.004163\n",
      "train_f_rmse: 0.005750\n",
      "val_e_mae: 0.002582\n",
      "val_e_rmse: 0.002812\n",
      "val_f_mae: 0.005661\n",
      "val_f_rmse: 0.007422\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.1031, Val Loss: 0.0594\n",
      "train_e_mae: 0.005894\n",
      "train_e_rmse: 0.006716\n",
      "train_f_mae: 0.004158\n",
      "train_f_rmse: 0.005745\n",
      "val_e_mae: 0.001779\n",
      "val_e_rmse: 0.002104\n",
      "val_f_mae: 0.005657\n",
      "val_f_rmse: 0.007415\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.0563, Val Loss: 0.0577\n",
      "train_e_mae: 0.005889\n",
      "train_e_rmse: 0.006796\n",
      "train_f_mae: 0.004154\n",
      "train_f_rmse: 0.005743\n",
      "val_e_mae: 0.001436\n",
      "val_e_rmse: 0.001672\n",
      "val_f_mae: 0.005652\n",
      "val_f_rmse: 0.007407\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.1008, Val Loss: 0.0563\n",
      "train_e_mae: 0.005356\n",
      "train_e_rmse: 0.006313\n",
      "train_f_mae: 0.004149\n",
      "train_f_rmse: 0.005738\n",
      "val_e_mae: 0.001202\n",
      "val_e_rmse: 0.001249\n",
      "val_f_mae: 0.005649\n",
      "val_f_rmse: 0.007401\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.0482, Val Loss: 0.0560\n",
      "train_e_mae: 0.005421\n",
      "train_e_rmse: 0.006433\n",
      "train_f_mae: 0.004145\n",
      "train_f_rmse: 0.005736\n",
      "val_e_mae: 0.001042\n",
      "val_e_rmse: 0.001152\n",
      "val_f_mae: 0.005643\n",
      "val_f_rmse: 0.007393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.1006, Val Loss: 0.0564\n",
      "train_e_mae: 0.005073\n",
      "train_e_rmse: 0.006053\n",
      "train_f_mae: 0.004140\n",
      "train_f_rmse: 0.005731\n",
      "val_e_mae: 0.001017\n",
      "val_e_rmse: 0.001340\n",
      "val_f_mae: 0.005640\n",
      "val_f_rmse: 0.007387\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.0423, Val Loss: 0.0571\n",
      "train_e_mae: 0.005185\n",
      "train_e_rmse: 0.006236\n",
      "train_f_mae: 0.004136\n",
      "train_f_rmse: 0.005729\n",
      "val_e_mae: 0.001184\n",
      "val_e_rmse: 0.001635\n",
      "val_f_mae: 0.005634\n",
      "val_f_rmse: 0.007379\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.1048, Val Loss: 0.0589\n",
      "train_e_mae: 0.004965\n",
      "train_e_rmse: 0.006012\n",
      "train_f_mae: 0.004130\n",
      "train_f_rmse: 0.005724\n",
      "val_e_mae: 0.001780\n",
      "val_e_rmse: 0.002136\n",
      "val_f_mae: 0.005630\n",
      "val_f_rmse: 0.007373\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.0500, Val Loss: 0.0598\n",
      "train_e_mae: 0.005026\n",
      "train_e_rmse: 0.006041\n",
      "train_f_mae: 0.004127\n",
      "train_f_rmse: 0.005715\n",
      "val_e_mae: 0.002035\n",
      "val_e_rmse: 0.002357\n",
      "val_f_mae: 0.005622\n",
      "val_f_rmse: 0.007363\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.0382, Val Loss: 0.0612\n",
      "train_e_mae: 0.004285\n",
      "train_e_rmse: 0.005162\n",
      "train_f_mae: 0.004114\n",
      "train_f_rmse: 0.005703\n",
      "val_e_mae: 0.002379\n",
      "val_e_rmse: 0.002665\n",
      "val_f_mae: 0.005614\n",
      "val_f_rmse: 0.007354\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.0384, Val Loss: 0.0619\n",
      "train_e_mae: 0.004156\n",
      "train_e_rmse: 0.004951\n",
      "train_f_mae: 0.004110\n",
      "train_f_rmse: 0.005696\n",
      "val_e_mae: 0.002551\n",
      "val_e_rmse: 0.002823\n",
      "val_f_mae: 0.005604\n",
      "val_f_rmse: 0.007342\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2161, Val Loss: 0.0602\n",
      "train_e_mae: 0.006657\n",
      "train_e_rmse: 0.008565\n",
      "train_f_mae: 0.004132\n",
      "train_f_rmse: 0.005702\n",
      "val_e_mae: 0.002252\n",
      "val_e_rmse: 0.002559\n",
      "val_f_mae: 0.005590\n",
      "val_f_rmse: 0.007328\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.0524, Val Loss: 0.0632\n",
      "train_e_mae: 0.008419\n",
      "train_e_rmse: 0.010208\n",
      "train_f_mae: 0.004121\n",
      "train_f_rmse: 0.005709\n",
      "val_e_mae: 0.002842\n",
      "val_e_rmse: 0.003095\n",
      "val_f_mae: 0.005586\n",
      "val_f_rmse: 0.007323\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.1601, Val Loss: 0.0641\n",
      "train_e_mae: 0.006717\n",
      "train_e_rmse: 0.008112\n",
      "train_f_mae: 0.004121\n",
      "train_f_rmse: 0.005695\n",
      "val_e_mae: 0.003028\n",
      "val_e_rmse: 0.003270\n",
      "val_f_mae: 0.005577\n",
      "val_f_rmse: 0.007310\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.0370, Val Loss: 0.0657\n",
      "train_e_mae: 0.006836\n",
      "train_e_rmse: 0.009037\n",
      "train_f_mae: 0.004106\n",
      "train_f_rmse: 0.005698\n",
      "val_e_mae: 0.003297\n",
      "val_e_rmse: 0.003525\n",
      "val_f_mae: 0.005570\n",
      "val_f_rmse: 0.007298\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.0833, Val Loss: 0.0638\n",
      "train_e_mae: 0.004512\n",
      "train_e_rmse: 0.005349\n",
      "train_f_mae: 0.004391\n",
      "train_f_rmse: 0.006103\n",
      "val_e_mae: 0.003042\n",
      "val_e_rmse: 0.003280\n",
      "val_f_mae: 0.005558\n",
      "val_f_rmse: 0.007285\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.0425, Val Loss: 0.0663\n",
      "train_e_mae: 0.003403\n",
      "train_e_rmse: 0.004267\n",
      "train_f_mae: 0.004344\n",
      "train_f_rmse: 0.006054\n",
      "val_e_mae: 0.003426\n",
      "val_e_rmse: 0.003649\n",
      "val_f_mae: 0.005558\n",
      "val_f_rmse: 0.007279\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.0749, Val Loss: 0.0675\n",
      "train_e_mae: 0.004247\n",
      "train_e_rmse: 0.005003\n",
      "train_f_mae: 0.005154\n",
      "train_f_rmse: 0.007474\n",
      "val_e_mae: 0.003657\n",
      "val_e_rmse: 0.003875\n",
      "val_f_mae: 0.005525\n",
      "val_f_rmse: 0.007242\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.0579, Val Loss: 0.0659\n",
      "train_e_mae: 0.004664\n",
      "train_e_rmse: 0.005501\n",
      "train_f_mae: 0.004465\n",
      "train_f_rmse: 0.006108\n",
      "val_e_mae: 0.003481\n",
      "val_e_rmse: 0.003707\n",
      "val_f_mae: 0.005510\n",
      "val_f_rmse: 0.007219\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.0430, Val Loss: 0.0670\n",
      "train_e_mae: 0.004122\n",
      "train_e_rmse: 0.004790\n",
      "train_f_mae: 0.004276\n",
      "train_f_rmse: 0.005863\n",
      "val_e_mae: 0.003613\n",
      "val_e_rmse: 0.003837\n",
      "val_f_mae: 0.005526\n",
      "val_f_rmse: 0.007227\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.0509, Val Loss: 0.0676\n",
      "train_e_mae: 0.004249\n",
      "train_e_rmse: 0.004884\n",
      "train_f_mae: 0.004084\n",
      "train_f_rmse: 0.005701\n",
      "val_e_mae: 0.003702\n",
      "val_e_rmse: 0.003923\n",
      "val_f_mae: 0.005524\n",
      "val_f_rmse: 0.007223\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.0602, Val Loss: 0.0678\n",
      "train_e_mae: 0.003872\n",
      "train_e_rmse: 0.004548\n",
      "train_f_mae: 0.004095\n",
      "train_f_rmse: 0.005671\n",
      "val_e_mae: 0.003747\n",
      "val_e_rmse: 0.003966\n",
      "val_f_mae: 0.005519\n",
      "val_f_rmse: 0.007215\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.0427, Val Loss: 0.0680\n",
      "train_e_mae: 0.003636\n",
      "train_e_rmse: 0.004261\n",
      "train_f_mae: 0.004093\n",
      "train_f_rmse: 0.005669\n",
      "val_e_mae: 0.003782\n",
      "val_e_rmse: 0.004002\n",
      "val_f_mae: 0.005520\n",
      "val_f_rmse: 0.007213\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.0437, Val Loss: 0.0685\n",
      "train_e_mae: 0.003442\n",
      "train_e_rmse: 0.004081\n",
      "train_f_mae: 0.004087\n",
      "train_f_rmse: 0.005665\n",
      "val_e_mae: 0.003851\n",
      "val_e_rmse: 0.004069\n",
      "val_f_mae: 0.005520\n",
      "val_f_rmse: 0.007210\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.0620, Val Loss: 0.0698\n",
      "train_e_mae: 0.003528\n",
      "train_e_rmse: 0.004169\n",
      "train_f_mae: 0.004081\n",
      "train_f_rmse: 0.005664\n",
      "val_e_mae: 0.004019\n",
      "val_e_rmse: 0.004230\n",
      "val_f_mae: 0.005519\n",
      "val_f_rmse: 0.007207\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.5122, Val Loss: 0.2972\n",
      "train_e_mae: 0.016782\n",
      "train_e_rmse: 0.018017\n",
      "train_f_mae: 0.004162\n",
      "train_f_rmse: 0.005733\n",
      "val_e_mae: 0.015645\n",
      "val_e_rmse: 0.015697\n",
      "val_f_mae: 0.005429\n",
      "val_f_rmse: 0.007128\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.5232, Val Loss: 0.7159\n",
      "train_e_mae: 0.017620\n",
      "train_e_rmse: 0.018273\n",
      "train_f_mae: 0.004160\n",
      "train_f_rmse: 0.005757\n",
      "val_e_mae: 0.025684\n",
      "val_e_rmse: 0.025724\n",
      "val_f_mae: 0.005630\n",
      "val_f_rmse: 0.007364\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.0562, Val Loss: 0.6429\n",
      "train_e_mae: 0.013271\n",
      "train_e_rmse: 0.015760\n",
      "train_f_mae: 0.004130\n",
      "train_f_rmse: 0.005736\n",
      "val_e_mae: 0.024230\n",
      "val_e_rmse: 0.024271\n",
      "val_f_mae: 0.005605\n",
      "val_f_rmse: 0.007335\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.5778, Val Loss: 0.6112\n",
      "train_e_mae: 0.012649\n",
      "train_e_rmse: 0.016123\n",
      "train_f_mae: 0.004122\n",
      "train_f_rmse: 0.005738\n",
      "val_e_mae: 0.023572\n",
      "val_e_rmse: 0.023615\n",
      "val_f_mae: 0.005589\n",
      "val_f_rmse: 0.007318\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2178, Val Loss: 0.5203\n",
      "train_e_mae: 0.011304\n",
      "train_e_rmse: 0.013124\n",
      "train_f_mae: 0.004140\n",
      "train_f_rmse: 0.005729\n",
      "val_e_mae: 0.021565\n",
      "val_e_rmse: 0.021611\n",
      "val_f_mae: 0.005573\n",
      "val_f_rmse: 0.007297\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.0429, Val Loss: 0.4715\n",
      "train_e_mae: 0.010981\n",
      "train_e_rmse: 0.012847\n",
      "train_f_mae: 0.004104\n",
      "train_f_rmse: 0.005718\n",
      "val_e_mae: 0.020409\n",
      "val_e_rmse: 0.020458\n",
      "val_f_mae: 0.005556\n",
      "val_f_rmse: 0.007276\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.0638, Val Loss: 0.4029\n",
      "train_e_mae: 0.008833\n",
      "train_e_rmse: 0.011820\n",
      "train_f_mae: 0.004250\n",
      "train_f_rmse: 0.005842\n",
      "val_e_mae: 0.018667\n",
      "val_e_rmse: 0.018719\n",
      "val_f_mae: 0.005532\n",
      "val_f_rmse: 0.007248\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.0943, Val Loss: 0.3628\n",
      "train_e_mae: 0.010229\n",
      "train_e_rmse: 0.010720\n",
      "train_f_mae: 0.004182\n",
      "train_f_rmse: 0.005818\n",
      "val_e_mae: 0.017564\n",
      "val_e_rmse: 0.017621\n",
      "val_f_mae: 0.005521\n",
      "val_f_rmse: 0.007232\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2239, Val Loss: 0.3343\n",
      "train_e_mae: 0.009767\n",
      "train_e_rmse: 0.010840\n",
      "train_f_mae: 0.004115\n",
      "train_f_rmse: 0.005719\n",
      "val_e_mae: 0.016738\n",
      "val_e_rmse: 0.016797\n",
      "val_f_mae: 0.005513\n",
      "val_f_rmse: 0.007218\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.0416, Val Loss: 0.3146\n",
      "train_e_mae: 0.007832\n",
      "train_e_rmse: 0.009706\n",
      "train_f_mae: 0.004088\n",
      "train_f_rmse: 0.005682\n",
      "val_e_mae: 0.016146\n",
      "val_e_rmse: 0.016208\n",
      "val_f_mae: 0.005504\n",
      "val_f_rmse: 0.007205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2312, Val Loss: 0.2995\n",
      "train_e_mae: 0.009648\n",
      "train_e_rmse: 0.011685\n",
      "train_f_mae: 0.004094\n",
      "train_f_rmse: 0.005704\n",
      "val_e_mae: 0.015676\n",
      "val_e_rmse: 0.015740\n",
      "val_f_mae: 0.005496\n",
      "val_f_rmse: 0.007194\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 1.5266, Val Loss: 0.2218\n",
      "train_e_mae: 0.017915\n",
      "train_e_rmse: 0.023707\n",
      "train_f_mae: 0.006065\n",
      "train_f_rmse: 0.011163\n",
      "val_e_mae: 0.012967\n",
      "val_e_rmse: 0.013032\n",
      "val_f_mae: 0.005537\n",
      "val_f_rmse: 0.007211\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 1.4795, Val Loss: 0.1341\n",
      "train_e_mae: 0.033815\n",
      "train_e_rmse: 0.040570\n",
      "train_f_mae: 0.009947\n",
      "train_f_rmse: 0.019915\n",
      "val_e_mae: 0.008645\n",
      "val_e_rmse: 0.008709\n",
      "val_f_mae: 0.005952\n",
      "val_f_rmse: 0.007630\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.6821, Val Loss: 0.1589\n",
      "train_e_mae: 0.028798\n",
      "train_e_rmse: 0.034573\n",
      "train_f_mae: 0.005740\n",
      "train_f_rmse: 0.008623\n",
      "val_e_mae: 0.010108\n",
      "val_e_rmse: 0.010171\n",
      "val_f_mae: 0.005814\n",
      "val_f_rmse: 0.007449\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2738, Val Loss: 0.1450\n",
      "train_e_mae: 0.011981\n",
      "train_e_rmse: 0.013914\n",
      "train_f_mae: 0.005336\n",
      "train_f_rmse: 0.008155\n",
      "val_e_mae: 0.009418\n",
      "val_e_rmse: 0.009489\n",
      "val_f_mae: 0.005785\n",
      "val_f_rmse: 0.007414\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.0424, Val Loss: 0.1300\n",
      "train_e_mae: 0.011471\n",
      "train_e_rmse: 0.013723\n",
      "train_f_mae: 0.005206\n",
      "train_f_rmse: 0.007695\n",
      "val_e_mae: 0.008536\n",
      "val_e_rmse: 0.008612\n",
      "val_f_mae: 0.005831\n",
      "val_f_rmse: 0.007473\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.0558, Val Loss: 0.1249\n",
      "train_e_mae: 0.003454\n",
      "train_e_rmse: 0.004117\n",
      "train_f_mae: 0.004190\n",
      "train_f_rmse: 0.005855\n",
      "val_e_mae: 0.008272\n",
      "val_e_rmse: 0.008354\n",
      "val_f_mae: 0.005792\n",
      "val_f_rmse: 0.007424\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.0854, Val Loss: 0.1194\n",
      "train_e_mae: 0.009020\n",
      "train_e_rmse: 0.010513\n",
      "train_f_mae: 0.004231\n",
      "train_f_rmse: 0.005908\n",
      "val_e_mae: 0.008004\n",
      "val_e_rmse: 0.008095\n",
      "val_f_mae: 0.005718\n",
      "val_f_rmse: 0.007342\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2563, Val Loss: 0.1215\n",
      "train_e_mae: 0.009679\n",
      "train_e_rmse: 0.010553\n",
      "train_f_mae: 0.004144\n",
      "train_f_rmse: 0.005787\n",
      "val_e_mae: 0.008168\n",
      "val_e_rmse: 0.008260\n",
      "val_f_mae: 0.005675\n",
      "val_f_rmse: 0.007299\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.0408, Val Loss: 0.1169\n",
      "train_e_mae: 0.008277\n",
      "train_e_rmse: 0.009312\n",
      "train_f_mae: 0.004146\n",
      "train_f_rmse: 0.005719\n",
      "val_e_mae: 0.007903\n",
      "val_e_rmse: 0.008000\n",
      "val_f_mae: 0.005650\n",
      "val_f_rmse: 0.007273\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.0357, Val Loss: 0.1137\n",
      "train_e_mae: 0.006165\n",
      "train_e_rmse: 0.008331\n",
      "train_f_mae: 0.004130\n",
      "train_f_rmse: 0.005754\n",
      "val_e_mae: 0.007725\n",
      "val_e_rmse: 0.007828\n",
      "val_f_mae: 0.005615\n",
      "val_f_rmse: 0.007239\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2324, Val Loss: 0.1093\n",
      "train_e_mae: 0.006138\n",
      "train_e_rmse: 0.007631\n",
      "train_f_mae: 0.004853\n",
      "train_f_rmse: 0.007151\n",
      "val_e_mae: 0.007431\n",
      "val_e_rmse: 0.007538\n",
      "val_f_mae: 0.005622\n",
      "val_f_rmse: 0.007244\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.0409, Val Loss: 0.1002\n",
      "train_e_mae: 0.007039\n",
      "train_e_rmse: 0.008701\n",
      "train_f_mae: 0.005619\n",
      "train_f_rmse: 0.008757\n",
      "val_e_mae: 0.006654\n",
      "val_e_rmse: 0.006770\n",
      "val_f_mae: 0.005742\n",
      "val_f_rmse: 0.007373\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.3631, Val Loss: 0.1004\n",
      "train_e_mae: 0.015478\n",
      "train_e_rmse: 0.017321\n",
      "train_f_mae: 0.004968\n",
      "train_f_rmse: 0.007334\n",
      "val_e_mae: 0.006842\n",
      "val_e_rmse: 0.006964\n",
      "val_f_mae: 0.005574\n",
      "val_f_rmse: 0.007202\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.3270, Val Loss: 0.1044\n",
      "train_e_mae: 0.016468\n",
      "train_e_rmse: 0.018478\n",
      "train_f_mae: 0.004853\n",
      "train_f_rmse: 0.006927\n",
      "val_e_mae: 0.007215\n",
      "val_e_rmse: 0.007335\n",
      "val_f_mae: 0.005465\n",
      "val_f_rmse: 0.007114\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.3285, Val Loss: 0.1040\n",
      "train_e_mae: 0.016510\n",
      "train_e_rmse: 0.018254\n",
      "train_f_mae: 0.004515\n",
      "train_f_rmse: 0.006227\n",
      "val_e_mae: 0.007150\n",
      "val_e_rmse: 0.007270\n",
      "val_f_mae: 0.005520\n",
      "val_f_rmse: 0.007154\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.1008, Val Loss: 0.1082\n",
      "train_e_mae: 0.013778\n",
      "train_e_rmse: 0.016339\n",
      "train_f_mae: 0.004251\n",
      "train_f_rmse: 0.005868\n",
      "val_e_mae: 0.007413\n",
      "val_e_rmse: 0.007532\n",
      "val_f_mae: 0.005536\n",
      "val_f_rmse: 0.007175\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.0357, Val Loss: 0.1066\n",
      "train_e_mae: 0.005367\n",
      "train_e_rmse: 0.006582\n",
      "train_f_mae: 0.004054\n",
      "train_f_rmse: 0.005680\n",
      "val_e_mae: 0.007326\n",
      "val_e_rmse: 0.007448\n",
      "val_f_mae: 0.005503\n",
      "val_f_rmse: 0.007150\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.1284, Val Loss: 0.1058\n",
      "train_e_mae: 0.004490\n",
      "train_e_rmse: 0.005518\n",
      "train_f_mae: 0.004055\n",
      "train_f_rmse: 0.005648\n",
      "val_e_mae: 0.007280\n",
      "val_e_rmse: 0.007406\n",
      "val_f_mae: 0.005486\n",
      "val_f_rmse: 0.007141\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.0397, Val Loss: 0.1008\n",
      "train_e_mae: 0.003459\n",
      "train_e_rmse: 0.004239\n",
      "train_f_mae: 0.004112\n",
      "train_f_rmse: 0.005687\n",
      "val_e_mae: 0.006942\n",
      "val_e_rmse: 0.007074\n",
      "val_f_mae: 0.005471\n",
      "val_f_rmse: 0.007125\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.1826, Val Loss: 0.1047\n",
      "train_e_mae: 0.009035\n",
      "train_e_rmse: 0.010005\n",
      "train_f_mae: 0.004339\n",
      "train_f_rmse: 0.006133\n",
      "val_e_mae: 0.007198\n",
      "val_e_rmse: 0.007331\n",
      "val_f_mae: 0.005458\n",
      "val_f_rmse: 0.007140\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.0741, Val Loss: 0.0948\n",
      "train_e_mae: 0.005796\n",
      "train_e_rmse: 0.006936\n",
      "train_f_mae: 0.004498\n",
      "train_f_rmse: 0.006255\n",
      "val_e_mae: 0.006464\n",
      "val_e_rmse: 0.006608\n",
      "val_f_mae: 0.005484\n",
      "val_f_rmse: 0.007150\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.0403, Val Loss: 0.1002\n",
      "train_e_mae: 0.004734\n",
      "train_e_rmse: 0.005750\n",
      "train_f_mae: 0.004486\n",
      "train_f_rmse: 0.006369\n",
      "val_e_mae: 0.006911\n",
      "val_e_rmse: 0.007054\n",
      "val_f_mae: 0.005425\n",
      "val_f_rmse: 0.007105\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.1032, Val Loss: 0.0991\n",
      "train_e_mae: 0.009245\n",
      "train_e_rmse: 0.010491\n",
      "train_f_mae: 0.004146\n",
      "train_f_rmse: 0.005715\n",
      "val_e_mae: 0.006826\n",
      "val_e_rmse: 0.006969\n",
      "val_f_mae: 0.005427\n",
      "val_f_rmse: 0.007107\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.0472, Val Loss: 0.0968\n",
      "train_e_mae: 0.008617\n",
      "train_e_rmse: 0.009916\n",
      "train_f_mae: 0.004073\n",
      "train_f_rmse: 0.005704\n",
      "val_e_mae: 0.006662\n",
      "val_e_rmse: 0.006810\n",
      "val_f_mae: 0.005420\n",
      "val_f_rmse: 0.007101\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2251, Val Loss: 0.0971\n",
      "train_e_mae: 0.005801\n",
      "train_e_rmse: 0.007116\n",
      "train_f_mae: 0.004090\n",
      "train_f_rmse: 0.005655\n",
      "val_e_mae: 0.006688\n",
      "val_e_rmse: 0.006836\n",
      "val_f_mae: 0.005418\n",
      "val_f_rmse: 0.007096\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.1289, Val Loss: 0.0942\n",
      "train_e_mae: 0.008904\n",
      "train_e_rmse: 0.009789\n",
      "train_f_mae: 0.004067\n",
      "train_f_rmse: 0.005648\n",
      "val_e_mae: 0.006477\n",
      "val_e_rmse: 0.006631\n",
      "val_f_mae: 0.005411\n",
      "val_f_rmse: 0.007086\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.0562, Val Loss: 0.0895\n",
      "train_e_mae: 0.006526\n",
      "train_e_rmse: 0.007460\n",
      "train_f_mae: 0.004101\n",
      "train_f_rmse: 0.005676\n",
      "val_e_mae: 0.006103\n",
      "val_e_rmse: 0.006265\n",
      "val_f_mae: 0.005411\n",
      "val_f_rmse: 0.007086\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.0460, Val Loss: 0.0916\n",
      "train_e_mae: 0.003565\n",
      "train_e_rmse: 0.004282\n",
      "train_f_mae: 0.004276\n",
      "train_f_rmse: 0.005983\n",
      "val_e_mae: 0.006290\n",
      "val_e_rmse: 0.006452\n",
      "val_f_mae: 0.005396\n",
      "val_f_rmse: 0.007069\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.1749, Val Loss: 0.0928\n",
      "train_e_mae: 0.005944\n",
      "train_e_rmse: 0.007065\n",
      "train_f_mae: 0.004139\n",
      "train_f_rmse: 0.005729\n",
      "val_e_mae: 0.006385\n",
      "val_e_rmse: 0.006544\n",
      "val_f_mae: 0.005396\n",
      "val_f_rmse: 0.007067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.2706, Val Loss: 0.1398\n",
      "train_e_mae: 0.016586\n",
      "train_e_rmse: 0.017330\n",
      "train_f_mae: 0.004148\n",
      "train_f_rmse: 0.005752\n",
      "val_e_mae: 0.009462\n",
      "val_e_rmse: 0.009550\n",
      "val_f_mae: 0.005295\n",
      "val_f_rmse: 0.006969\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.8041, Val Loss: 1.0422\n",
      "train_e_mae: 0.017389\n",
      "train_e_rmse: 0.020197\n",
      "train_f_mae: 0.004134\n",
      "train_f_rmse: 0.005745\n",
      "val_e_mae: 0.031428\n",
      "val_e_rmse: 0.031462\n",
      "val_f_mae: 0.005509\n",
      "val_f_rmse: 0.007238\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2620, Val Loss: 0.8915\n",
      "train_e_mae: 0.013129\n",
      "train_e_rmse: 0.015220\n",
      "train_f_mae: 0.004136\n",
      "train_f_rmse: 0.005776\n",
      "val_e_mae: 0.028939\n",
      "val_e_rmse: 0.028976\n",
      "val_f_mae: 0.005484\n",
      "val_f_rmse: 0.007207\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.0435, Val Loss: 0.7883\n",
      "train_e_mae: 0.012353\n",
      "train_e_rmse: 0.014396\n",
      "train_f_mae: 0.004138\n",
      "train_f_rmse: 0.005750\n",
      "val_e_mae: 0.027103\n",
      "val_e_rmse: 0.027143\n",
      "val_f_mae: 0.005462\n",
      "val_f_rmse: 0.007182\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 3.0629, Val Loss: 0.6599\n",
      "train_e_mae: 0.019341\n",
      "train_e_rmse: 0.025554\n",
      "train_f_mae: 0.004564\n",
      "train_f_rmse: 0.006502\n",
      "val_e_mae: 0.024630\n",
      "val_e_rmse: 0.024675\n",
      "val_f_mae: 0.005435\n",
      "val_f_rmse: 0.007148\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2297, Val Loss: 0.5347\n",
      "train_e_mae: 0.030357\n",
      "train_e_rmse: 0.033152\n",
      "train_f_mae: 0.005177\n",
      "train_f_rmse: 0.007324\n",
      "val_e_mae: 0.021951\n",
      "val_e_rmse: 0.021997\n",
      "val_f_mae: 0.005417\n",
      "val_f_rmse: 0.007126\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2286, Val Loss: 0.4755\n",
      "train_e_mae: 0.023384\n",
      "train_e_rmse: 0.028282\n",
      "train_f_mae: 0.004322\n",
      "train_f_rmse: 0.005971\n",
      "val_e_mae: 0.020566\n",
      "val_e_rmse: 0.020614\n",
      "val_f_mae: 0.005402\n",
      "val_f_rmse: 0.007111\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2210, Val Loss: 0.4366\n",
      "train_e_mae: 0.009525\n",
      "train_e_rmse: 0.011085\n",
      "train_f_mae: 0.004328\n",
      "train_f_rmse: 0.005947\n",
      "val_e_mae: 0.019599\n",
      "val_e_rmse: 0.019650\n",
      "val_f_mae: 0.005395\n",
      "val_f_rmse: 0.007102\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 1.7722, Val Loss: 0.3974\n",
      "train_e_mae: 0.014527\n",
      "train_e_rmse: 0.019083\n",
      "train_f_mae: 0.004590\n",
      "train_f_rmse: 0.006297\n",
      "val_e_mae: 0.018574\n",
      "val_e_rmse: 0.018627\n",
      "val_f_mae: 0.005386\n",
      "val_f_rmse: 0.007102\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.0976, Val Loss: 0.3031\n",
      "train_e_mae: 0.023598\n",
      "train_e_rmse: 0.026433\n",
      "train_f_mae: 0.005306\n",
      "train_f_rmse: 0.007811\n",
      "val_e_mae: 0.015912\n",
      "val_e_rmse: 0.015969\n",
      "val_f_mae: 0.005263\n",
      "val_f_rmse: 0.006938\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.0930, Val Loss: 0.2609\n",
      "train_e_mae: 0.019951\n",
      "train_e_rmse: 0.024696\n",
      "train_f_mae: 0.004695\n",
      "train_f_rmse: 0.006613\n",
      "val_e_mae: 0.014541\n",
      "val_e_rmse: 0.014607\n",
      "val_f_mae: 0.005232\n",
      "val_f_rmse: 0.006894\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 1.4877, Val Loss: 0.2206\n",
      "train_e_mae: 0.019684\n",
      "train_e_rmse: 0.023016\n",
      "train_f_mae: 0.004422\n",
      "train_f_rmse: 0.006063\n",
      "val_e_mae: 0.013078\n",
      "val_e_rmse: 0.013150\n",
      "val_f_mae: 0.005236\n",
      "val_f_rmse: 0.006905\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2280, Val Loss: 0.1939\n",
      "train_e_mae: 0.012227\n",
      "train_e_rmse: 0.015822\n",
      "train_f_mae: 0.004496\n",
      "train_f_rmse: 0.006180\n",
      "val_e_mae: 0.012011\n",
      "val_e_rmse: 0.012090\n",
      "val_f_mae: 0.005235\n",
      "val_f_rmse: 0.006908\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.3180, Val Loss: 0.1801\n",
      "train_e_mae: 0.021066\n",
      "train_e_rmse: 0.023366\n",
      "train_f_mae: 0.004229\n",
      "train_f_rmse: 0.005816\n",
      "val_e_mae: 0.011425\n",
      "val_e_rmse: 0.011510\n",
      "val_f_mae: 0.005232\n",
      "val_f_rmse: 0.006900\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.4643, Val Loss: 0.1467\n",
      "train_e_mae: 0.019731\n",
      "train_e_rmse: 0.023167\n",
      "train_f_mae: 0.005422\n",
      "train_f_rmse: 0.007976\n",
      "val_e_mae: 0.009840\n",
      "val_e_rmse: 0.009934\n",
      "val_f_mae: 0.005259\n",
      "val_f_rmse: 0.006929\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 1.3763, Val Loss: 0.1407\n",
      "train_e_mae: 0.025948\n",
      "train_e_rmse: 0.028780\n",
      "train_f_mae: 0.005531\n",
      "train_f_rmse: 0.008004\n",
      "val_e_mae: 0.009493\n",
      "val_e_rmse: 0.009597\n",
      "val_f_mae: 0.005276\n",
      "val_f_rmse: 0.006967\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 1.6913, Val Loss: 0.1386\n",
      "train_e_mae: 0.024353\n",
      "train_e_rmse: 0.026940\n",
      "train_f_mae: 0.004470\n",
      "train_f_rmse: 0.006102\n",
      "val_e_mae: 0.009392\n",
      "val_e_rmse: 0.009495\n",
      "val_f_mae: 0.005265\n",
      "val_f_rmse: 0.006960\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.0423, Val Loss: 0.1293\n",
      "train_e_mae: 0.018369\n",
      "train_e_rmse: 0.022495\n",
      "train_f_mae: 0.004283\n",
      "train_f_rmse: 0.005901\n",
      "val_e_mae: 0.008901\n",
      "val_e_rmse: 0.009012\n",
      "val_f_mae: 0.005251\n",
      "val_f_rmse: 0.006932\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.1145, Val Loss: 0.1173\n",
      "train_e_mae: 0.009848\n",
      "train_e_rmse: 0.010755\n",
      "train_f_mae: 0.004173\n",
      "train_f_rmse: 0.005746\n",
      "val_e_mae: 0.008213\n",
      "val_e_rmse: 0.008332\n",
      "val_f_mae: 0.005237\n",
      "val_f_rmse: 0.006916\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.0942, Val Loss: 0.1147\n",
      "train_e_mae: 0.009118\n",
      "train_e_rmse: 0.010082\n",
      "train_f_mae: 0.004079\n",
      "train_f_rmse: 0.005706\n",
      "val_e_mae: 0.008052\n",
      "val_e_rmse: 0.008175\n",
      "val_f_mae: 0.005239\n",
      "val_f_rmse: 0.006920\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.1392, Val Loss: 0.1200\n",
      "train_e_mae: 0.008912\n",
      "train_e_rmse: 0.010306\n",
      "train_f_mae: 0.004063\n",
      "train_f_rmse: 0.005684\n",
      "val_e_mae: 0.008369\n",
      "val_e_rmse: 0.008489\n",
      "val_f_mae: 0.005243\n",
      "val_f_rmse: 0.006924\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.1968, Val Loss: 0.1239\n",
      "train_e_mae: 0.007375\n",
      "train_e_rmse: 0.009098\n",
      "train_f_mae: 0.004062\n",
      "train_f_rmse: 0.005673\n",
      "val_e_mae: 0.008599\n",
      "val_e_rmse: 0.008716\n",
      "val_f_mae: 0.005240\n",
      "val_f_rmse: 0.006920\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.0574, Val Loss: 0.1227\n",
      "train_e_mae: 0.005324\n",
      "train_e_rmse: 0.007131\n",
      "train_f_mae: 0.004295\n",
      "train_f_rmse: 0.006061\n",
      "val_e_mae: 0.008530\n",
      "val_e_rmse: 0.008652\n",
      "val_f_mae: 0.005239\n",
      "val_f_rmse: 0.006915\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.1634, Val Loss: 0.1186\n",
      "train_e_mae: 0.005544\n",
      "train_e_rmse: 0.006627\n",
      "train_f_mae: 0.004164\n",
      "train_f_rmse: 0.005789\n",
      "val_e_mae: 0.008275\n",
      "val_e_rmse: 0.008403\n",
      "val_f_mae: 0.005252\n",
      "val_f_rmse: 0.006925\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.0586, Val Loss: 0.1238\n",
      "train_e_mae: 0.011019\n",
      "train_e_rmse: 0.012848\n",
      "train_f_mae: 0.004174\n",
      "train_f_rmse: 0.005777\n",
      "val_e_mae: 0.008566\n",
      "val_e_rmse: 0.008692\n",
      "val_f_mae: 0.005271\n",
      "val_f_rmse: 0.006944\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.1500, Val Loss: 0.1163\n",
      "train_e_mae: 0.004660\n",
      "train_e_rmse: 0.005689\n",
      "train_f_mae: 0.004048\n",
      "train_f_rmse: 0.005653\n",
      "val_e_mae: 0.008121\n",
      "val_e_rmse: 0.008254\n",
      "val_f_mae: 0.005266\n",
      "val_f_rmse: 0.006937\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.0626, Val Loss: 0.1173\n",
      "train_e_mae: 0.010013\n",
      "train_e_rmse: 0.011364\n",
      "train_f_mae: 0.004075\n",
      "train_f_rmse: 0.005669\n",
      "val_e_mae: 0.008184\n",
      "val_e_rmse: 0.008317\n",
      "val_f_mae: 0.005270\n",
      "val_f_rmse: 0.006939\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.1462, Val Loss: 0.1104\n",
      "train_e_mae: 0.006053\n",
      "train_e_rmse: 0.007390\n",
      "train_f_mae: 0.004055\n",
      "train_f_rmse: 0.005651\n",
      "val_e_mae: 0.007754\n",
      "val_e_rmse: 0.007894\n",
      "val_f_mae: 0.005267\n",
      "val_f_rmse: 0.006934\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.0466, Val Loss: 0.1136\n",
      "train_e_mae: 0.007307\n",
      "train_e_rmse: 0.009138\n",
      "train_f_mae: 0.004053\n",
      "train_f_rmse: 0.005681\n",
      "val_e_mae: 0.007956\n",
      "val_e_rmse: 0.008093\n",
      "val_f_mae: 0.005267\n",
      "val_f_rmse: 0.006933\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.1554, Val Loss: 0.1124\n",
      "train_e_mae: 0.006200\n",
      "train_e_rmse: 0.007440\n",
      "train_f_mae: 0.004049\n",
      "train_f_rmse: 0.005645\n",
      "val_e_mae: 0.007878\n",
      "val_e_rmse: 0.008017\n",
      "val_f_mae: 0.005268\n",
      "val_f_rmse: 0.006934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.0355, Val Loss: 0.0994\n",
      "train_e_mae: 0.013801\n",
      "train_e_rmse: 0.015689\n",
      "train_f_mae: 0.004112\n",
      "train_f_rmse: 0.005696\n",
      "val_e_mae: 0.007014\n",
      "val_e_rmse: 0.007169\n",
      "val_f_mae: 0.005265\n",
      "val_f_rmse: 0.006929\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.0363, Val Loss: 0.0983\n",
      "train_e_mae: 0.004704\n",
      "train_e_rmse: 0.005609\n",
      "train_f_mae: 0.004043\n",
      "train_f_rmse: 0.005685\n",
      "val_e_mae: 0.006929\n",
      "val_e_rmse: 0.007088\n",
      "val_f_mae: 0.005268\n",
      "val_f_rmse: 0.006930\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2313, Val Loss: 0.0981\n",
      "train_e_mae: 0.007637\n",
      "train_e_rmse: 0.008868\n",
      "train_f_mae: 0.004106\n",
      "train_f_rmse: 0.005680\n",
      "val_e_mae: 0.006917\n",
      "val_e_rmse: 0.007073\n",
      "val_f_mae: 0.005267\n",
      "val_f_rmse: 0.006930\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.1074, Val Loss: 0.0902\n",
      "train_e_mae: 0.010611\n",
      "train_e_rmse: 0.012281\n",
      "train_f_mae: 0.004068\n",
      "train_f_rmse: 0.005667\n",
      "val_e_mae: 0.006325\n",
      "val_e_rmse: 0.006497\n",
      "val_f_mae: 0.005267\n",
      "val_f_rmse: 0.006928\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.0460, Val Loss: 0.0887\n",
      "train_e_mae: 0.003253\n",
      "train_e_rmse: 0.003879\n",
      "train_f_mae: 0.004037\n",
      "train_f_rmse: 0.005635\n",
      "val_e_mae: 0.006203\n",
      "val_e_rmse: 0.006379\n",
      "val_f_mae: 0.005267\n",
      "val_f_rmse: 0.006928\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.0430, Val Loss: 0.0875\n",
      "train_e_mae: 0.002300\n",
      "train_e_rmse: 0.002823\n",
      "train_f_mae: 0.004039\n",
      "train_f_rmse: 0.005625\n",
      "val_e_mae: 0.006107\n",
      "val_e_rmse: 0.006286\n",
      "val_f_mae: 0.005268\n",
      "val_f_rmse: 0.006927\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.0349, Val Loss: 0.0851\n",
      "train_e_mae: 0.002213\n",
      "train_e_rmse: 0.002688\n",
      "train_f_mae: 0.004044\n",
      "train_f_rmse: 0.005622\n",
      "val_e_mae: 0.005908\n",
      "val_e_rmse: 0.006091\n",
      "val_f_mae: 0.005269\n",
      "val_f_rmse: 0.006928\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.0467, Val Loss: 0.0826\n",
      "train_e_mae: 0.002710\n",
      "train_e_rmse: 0.003203\n",
      "train_f_mae: 0.004045\n",
      "train_f_rmse: 0.005620\n",
      "val_e_mae: 0.005695\n",
      "val_e_rmse: 0.005885\n",
      "val_f_mae: 0.005270\n",
      "val_f_rmse: 0.006928\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.0356, Val Loss: 0.0814\n",
      "train_e_mae: 0.001961\n",
      "train_e_rmse: 0.002365\n",
      "train_f_mae: 0.004041\n",
      "train_f_rmse: 0.005617\n",
      "val_e_mae: 0.005589\n",
      "val_e_rmse: 0.005782\n",
      "val_f_mae: 0.005271\n",
      "val_f_rmse: 0.006929\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.0349, Val Loss: 0.0813\n",
      "train_e_mae: 0.001672\n",
      "train_e_rmse: 0.002018\n",
      "train_f_mae: 0.004038\n",
      "train_f_rmse: 0.005614\n",
      "val_e_mae: 0.005576\n",
      "val_e_rmse: 0.005770\n",
      "val_f_mae: 0.005274\n",
      "val_f_rmse: 0.006930\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-3, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 20, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss_2, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=400, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e40b56d9",
   "metadata": {},
   "source": [
    "task.optimizer.param_groups[0]['lr'] = 2e-3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fba88fbf",
   "metadata": {},
   "source": [
    "task.fit(train_loader, valid_loader, epochs=300, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acfc67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.save_model('model-tmp.pth')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deaf4a73",
   "metadata": {},
   "source": [
    "torch.save(combo_p, 'model-tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93e7b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = cace.tasks.EvaluateTask(model_path='model-tmp.pth', device='cpu',\n",
    "                                    energy_key='CACE_energy', #'ewald_potential',\n",
    "                                    forces_key='CACE_forces',\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62ad042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = evaluator(train_ase_xyz)\n",
    "pred_test = evaluator(test_ase_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62f4b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_true  = np.array([ xyz.get_array('forces') for xyz in train_ase_xyz]).reshape(-1,3)\n",
    "test_f_true  = np.array([ xyz.get_array('forces') for xyz in test_ase_xyz]).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d18d8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(3, 3))\n",
    "\n",
    "ax1.plot(train_f_true[:,0], pred_train['forces'][:,0], '.', color='blue', label='Train')\n",
    "\n",
    "ax1.plot(test_f_true[:,0], pred_test['forces'][:,0], '.', color='red', label='Test')\n",
    "\n",
    "ax1.set_xlabel('Forces [$eV/\\mathrm{\\AA}$]')\n",
    "\n",
    "ax1.set_ylabel('MLP-LR Forces [$eV/\\mathrm{\\AA}$]')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8864de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property(atoms, info_name):\n",
    "    return np.array([a.info[info_name] for a in atoms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "404065a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(3, 2))\n",
    "\n",
    "# Upper panel\n",
    "ax1.plot(get_property(train_ase_xyz, 'distance'), get_property(train_ase_xyz, 'inter_energy'), \n",
    "         'o',color='blue', markerfacecolor='white')\n",
    "ax1.plot(get_property(test_ase_xyz, 'distance'), get_property(test_ase_xyz, 'inter_energy'), \n",
    "         'o',color='red', markerfacecolor='white', label='True')\n",
    "\n",
    "ax1.plot(get_property(train_ase_xyz, 'distance'), pred_train['energy'], 'x', color='b')\n",
    "ax1.plot(get_property(test_ase_xyz, 'distance'), pred_test['energy'], 'x', color='r', label='MLP-LR')\n",
    "#ax1.set_title('Energy')\n",
    "ax1.set_xlabel('Distance [$\\mathrm{\\AA}$]')\n",
    "\n",
    "ax1.set_ylabel('Energy [eV]')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ab850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
