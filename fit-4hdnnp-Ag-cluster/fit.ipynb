{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ef67233e",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83178f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06b72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cace\n",
    "from cace.representations.cace_representation import Cace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b78f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read,write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4afc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5.29\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xyz_dir = '../4HDNNP-data/datasets/Ag_cluster/Ag-cluster.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a746a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = cace.tasks.get_dataset_from_xyz(train_path=train_xyz_dir,\n",
    "                                 valid_fraction=0.1,\n",
    "                                cutoff=cutoff,\n",
    "                                 data_key={'energy': 'energy', \n",
    "                                           'forces': 'forces',\n",
    "                                           'charge_states': 'charge_states'\n",
    "                                          },\n",
    "                                   atomic_energies = {47: -146385.11440723907}\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41391e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = cace.tasks.load_data_loader(collection=collection,\n",
    "                              data_type='train',\n",
    "                              batch_size=batch_size,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b676f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = cace.tasks.load_data_loader(collection=collection,\n",
    "                              data_type='valid',\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fc6163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cace.tools.init_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59cc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb2e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4034789d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(atomic_numbers=[300], batch=[300], cell=[300, 3], charge_states=[300], edge_index=[2, 556], energy=[100], forces=[300, 3], positions=[300, 3], ptr=[101], shifts=[556, 3], unit_shifts=[556, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32e7c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.modules import CosineCutoff, MollifierCutoff, PolynomialCutoff\n",
    "from cace.modules import BesselRBF, GaussianRBF, GaussianRBFCentered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692c8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_basis = BesselRBF(cutoff=cutoff, n_rbf=6, trainable=False)\n",
    "cutoff_fn = PolynomialCutoff(cutoff=cutoff, p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f97e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cace_representation = Cace(\n",
    "    zs=[47],\n",
    "    n_atom_basis=1,\n",
    "    cutoff=cutoff,\n",
    "    cutoff_fn=cutoff_fn,\n",
    "    radial_basis=radial_basis,\n",
    "    n_radial_basis=8,\n",
    "    max_l=3,\n",
    "    max_nu=3,\n",
    "    num_message_passing=0,\n",
    "    device=device,\n",
    "    timeit=False,\n",
    "    forward_features=['charge_states']\n",
    "           )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69931833",
   "metadata": {},
   "source": [
    "q = cace.modules.Atomwise(\n",
    "    n_layers=3,\n",
    "    n_hidden=[24,12],\n",
    "    n_out=3,\n",
    "    feature_key = ['node_feats'], \n",
    "    per_atom_output_key='q',\n",
    "    output_key = 'tot_q',\n",
    "    residual=False,\n",
    "    add_linear_nn=False,\n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2590d1e1",
   "metadata": {},
   "source": [
    "from cace.modules import EwaldPotential"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73c164f4",
   "metadata": {},
   "source": [
    "ep = EwaldPotential(dl=3.,\n",
    "                    sigma=1.0,\n",
    "                    feature_key='q',\n",
    "                    output_key='ewald_potential',\n",
    "                    aggregation_mode='sum')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71157cda",
   "metadata": {},
   "source": [
    "feat_field = cace.modules.feature_mix.FeatureInteract(\n",
    "    feature1_key='node_feats',\n",
    "    feature2_key='q_field',\n",
    "    output_key='node_feats_q_field'\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f663a82",
   "metadata": {},
   "source": [
    "q2 = cace.modules.Atomwise(\n",
    "    n_layers=3,\n",
    "    n_hidden=[24,12],\n",
    "    n_out=3,\n",
    "    feature_key = ['node_feats_q_field'], \n",
    "    per_atom_output_key='q2',\n",
    "    output_key = 'tot_q2',\n",
    "    residual=False,\n",
    "    add_linear_nn=False,\n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb2d5b70",
   "metadata": {},
   "source": [
    "q_sum = cace.modules.feature_mix.FeatureAdd(\n",
    "    feature_keys=['q','q2'],\n",
    "    output_key='q_sum')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f75d2d75",
   "metadata": {},
   "source": [
    "ep_2 = EwaldPotential(dl=3.,\n",
    "                    sigma=1.0,\n",
    "                    feature_key='q_sum',\n",
    "                    output_key='ewald_potential_2',\n",
    "                    aggregation_mode='sum')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ef09867",
   "metadata": {},
   "source": [
    "forces_lr = cace.modules.Forces(energy_key='ewald_potential_2',\n",
    "                                    forces_key='ewald_forces_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54142124-26c8-4748-ac65-c74e03b2dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models.atomistic import NeuralNetworkPotential"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f534bb58",
   "metadata": {},
   "source": [
    "nnp_lr = NeuralNetworkPotential(\n",
    "    input_modules=None,\n",
    "    representation=cace_representation,\n",
    "    output_modules=[q, ep, feat_field, q2, q_sum, ep_2, forces_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ca9d90d",
   "metadata": {},
   "source": [
    "res = nnp_lr(sampled_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa1b476f",
   "metadata": {},
   "source": [
    "trainable_params = sum(p.numel() for p in nnp_lr.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22656d6c",
   "metadata": {},
   "source": [
    "feat_field = cace.modules.feature_mix.FeatureInteract(\n",
    "    feature1_key='node_feats',\n",
    "    feature2_key= 'charge_states',\n",
    "    output_key='node_feats_charge'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13195a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomwise = cace.modules.Atomwise(\n",
    "    feature_key = ['node_feats', 'charge_states'], \n",
    "    n_layers=3,\n",
    "    n_hidden=[24,12],\n",
    "    n_out=1,\n",
    "    output_key='CACE_energy_intra',\n",
    "    residual=False,\n",
    "    add_linear_nn=True,\n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "forces = cace.modules.Forces(energy_key='CACE_energy_intra',\n",
    "                                    forces_key='CACE_forces_intra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cea892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models.atomistic import NeuralNetworkPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09165246",
   "metadata": {},
   "outputs": [],
   "source": [
    "cace_nnp_intra = NeuralNetworkPotential(\n",
    "    input_modules=None,\n",
    "    representation=cace_representation,\n",
    "    output_modules=[atomwise,  forces]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1369dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cace_nnp_intra(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877e1400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(atomic_numbers=[300], batch=[300], cell=[300, 3], charge_states=[300], edge_index=[2, 556], energy=[100], forces=[300, 3], positions=[300, 3], ptr=[101], shifts=[556, 3], unit_shifts=[556, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7225b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.models import CombinePotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "222dab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot1 = {'CACE_energy': 'ewald_potential_2', \n",
    "        'CACE_forces': 'ewald_forces_2',\n",
    "        'weight': 1.\n",
    "       }\n",
    "\n",
    "pot2 = {'CACE_energy': 'CACE_energy_intra', \n",
    "        'CACE_forces': 'CACE_forces_intra',\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd937dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combo_p = CombinePotential([nnp_lr, cace_nnp_intra], [pot1,pot2])\n",
    "combo_p = CombinePotential([cace_nnp_intra], [pot2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bdc3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tasks import GetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d782978",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff4cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_2 = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2100797",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_3 = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f181e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_4 = GetLoss(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7564e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_loss = GetLoss(\n",
    "    target_name='forces',\n",
    "    predict_name= 'CACE_forces',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a667d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_loss_2 = GetLoss(\n",
    "    target_name='forces',\n",
    "    predict_name= 'CACE_forces',\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "294f2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tools import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddc2c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_metric = Metrics(\n",
    "    target_name='energy',\n",
    "    predict_name='CACE_energy',\n",
    "    name='e/atom',\n",
    "    per_atom=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a23ccecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metric = Metrics(\n",
    "    target_name='forces',\n",
    "    predict_name='CACE_forces',\n",
    "    name='f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc083f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dict = sampled_data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c25f5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_result = combo_p(sampled_dict, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4100cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1741124.3750, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_loss_3(sampled_data_result, sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "248757a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(212.1026, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_loss(sampled_data_result, sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ccfcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cace.tasks.train import TrainingTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "642ab2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 1.4655, Val Loss: 1.2606\n",
      "train_e/atom_mae: 0.802953\n",
      "train_e/atom_rmse: 0.931452\n",
      "train_f_mae: 0.075659\n",
      "train_f_rmse: 0.180008\n",
      "val_e/atom_mae: 0.111474\n",
      "val_e/atom_rmse: 0.120152\n",
      "val_f_mae: 0.019689\n",
      "val_f_rmse: 0.036549\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.5021, Val Loss: 0.3799\n",
      "train_e/atom_mae: 0.021222\n",
      "train_e/atom_rmse: 0.033057\n",
      "train_f_mae: 0.013555\n",
      "train_f_rmse: 0.026096\n",
      "val_e/atom_mae: 0.011850\n",
      "val_e/atom_rmse: 0.015774\n",
      "val_f_mae: 0.009559\n",
      "val_f_rmse: 0.020248\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.4252, Val Loss: 0.2961\n",
      "train_e/atom_mae: 0.010914\n",
      "train_e/atom_rmse: 0.014439\n",
      "train_f_mae: 0.011059\n",
      "train_f_rmse: 0.020924\n",
      "val_e/atom_mae: 0.009948\n",
      "val_e/atom_rmse: 0.013724\n",
      "val_f_mae: 0.008450\n",
      "val_f_rmse: 0.017881\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.3773, Val Loss: 0.2654\n",
      "train_e/atom_mae: 0.008623\n",
      "train_e/atom_rmse: 0.012183\n",
      "train_f_mae: 0.010390\n",
      "train_f_rmse: 0.019677\n",
      "val_e/atom_mae: 0.008021\n",
      "val_e/atom_rmse: 0.011585\n",
      "val_f_mae: 0.007902\n",
      "val_f_rmse: 0.016929\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.3197, Val Loss: 0.2449\n",
      "train_e/atom_mae: 0.008024\n",
      "train_e/atom_rmse: 0.011539\n",
      "train_f_mae: 0.009521\n",
      "train_f_rmse: 0.018547\n",
      "val_e/atom_mae: 0.007384\n",
      "val_e/atom_rmse: 0.011117\n",
      "val_f_mae: 0.007545\n",
      "val_f_rmse: 0.016264\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.3222, Val Loss: 0.2291\n",
      "train_e/atom_mae: 0.007665\n",
      "train_e/atom_rmse: 0.011193\n",
      "train_f_mae: 0.009255\n",
      "train_f_rmse: 0.018038\n",
      "val_e/atom_mae: 0.006883\n",
      "val_e/atom_rmse: 0.010470\n",
      "val_f_mae: 0.007230\n",
      "val_f_rmse: 0.015724\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.3728, Val Loss: 0.2227\n",
      "train_e/atom_mae: 0.007911\n",
      "train_e/atom_rmse: 0.010797\n",
      "train_f_mae: 0.010237\n",
      "train_f_rmse: 0.018982\n",
      "val_e/atom_mae: 0.008015\n",
      "val_e/atom_rmse: 0.010346\n",
      "val_f_mae: 0.007091\n",
      "val_f_rmse: 0.015525\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2875, Val Loss: 0.2157\n",
      "train_e/atom_mae: 0.007722\n",
      "train_e/atom_rmse: 0.011118\n",
      "train_f_mae: 0.008852\n",
      "train_f_rmse: 0.017549\n",
      "val_e/atom_mae: 0.006417\n",
      "val_e/atom_rmse: 0.010447\n",
      "val_f_mae: 0.006947\n",
      "val_f_rmse: 0.015252\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2718, Val Loss: 0.2088\n",
      "train_e/atom_mae: 0.007324\n",
      "train_e/atom_rmse: 0.010791\n",
      "train_f_mae: 0.008183\n",
      "train_f_rmse: 0.016829\n",
      "val_e/atom_mae: 0.006537\n",
      "val_e/atom_rmse: 0.010280\n",
      "val_f_mae: 0.006755\n",
      "val_f_rmse: 0.015010\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2665, Val Loss: 0.2031\n",
      "train_e/atom_mae: 0.007421\n",
      "train_e/atom_rmse: 0.010919\n",
      "train_f_mae: 0.007854\n",
      "train_f_rmse: 0.016501\n",
      "val_e/atom_mae: 0.006609\n",
      "val_e/atom_rmse: 0.010128\n",
      "val_f_mae: 0.006623\n",
      "val_f_rmse: 0.014775\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2886, Val Loss: 0.1992\n",
      "train_e/atom_mae: 0.007501\n",
      "train_e/atom_rmse: 0.010724\n",
      "train_f_mae: 0.008160\n",
      "train_f_rmse: 0.016737\n",
      "val_e/atom_mae: 0.006682\n",
      "val_e/atom_rmse: 0.010077\n",
      "val_f_mae: 0.006514\n",
      "val_f_rmse: 0.014663\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2567, Val Loss: 0.1960\n",
      "train_e/atom_mae: 0.007353\n",
      "train_e/atom_rmse: 0.010743\n",
      "train_f_mae: 0.007453\n",
      "train_f_rmse: 0.016130\n",
      "val_e/atom_mae: 0.006588\n",
      "val_e/atom_rmse: 0.010103\n",
      "val_f_mae: 0.006434\n",
      "val_f_rmse: 0.014528\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2560, Val Loss: 0.1931\n",
      "train_e/atom_mae: 0.007243\n",
      "train_e/atom_rmse: 0.010621\n",
      "train_f_mae: 0.007525\n",
      "train_f_rmse: 0.016149\n",
      "val_e/atom_mae: 0.006523\n",
      "val_e/atom_rmse: 0.010085\n",
      "val_f_mae: 0.006343\n",
      "val_f_rmse: 0.014417\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2505, Val Loss: 0.1902\n",
      "train_e/atom_mae: 0.007197\n",
      "train_e/atom_rmse: 0.010498\n",
      "train_f_mae: 0.007294\n",
      "train_f_rmse: 0.015926\n",
      "val_e/atom_mae: 0.006526\n",
      "val_e/atom_rmse: 0.010130\n",
      "val_f_mae: 0.006288\n",
      "val_f_rmse: 0.014330\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2497, Val Loss: 0.1895\n",
      "train_e/atom_mae: 0.007115\n",
      "train_e/atom_rmse: 0.010476\n",
      "train_f_mae: 0.007174\n",
      "train_f_rmse: 0.015834\n",
      "val_e/atom_mae: 0.006459\n",
      "val_e/atom_rmse: 0.009976\n",
      "val_f_mae: 0.006232\n",
      "val_f_rmse: 0.014294\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2534, Val Loss: 0.1867\n",
      "train_e/atom_mae: 0.007216\n",
      "train_e/atom_rmse: 0.010458\n",
      "train_f_mae: 0.007092\n",
      "train_f_rmse: 0.015765\n",
      "val_e/atom_mae: 0.006704\n",
      "val_e/atom_rmse: 0.010055\n",
      "val_f_mae: 0.006172\n",
      "val_f_rmse: 0.014203\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2421, Val Loss: 0.1855\n",
      "train_e/atom_mae: 0.007205\n",
      "train_e/atom_rmse: 0.010544\n",
      "train_f_mae: 0.006856\n",
      "train_f_rmse: 0.015591\n",
      "val_e/atom_mae: 0.006858\n",
      "val_e/atom_rmse: 0.010150\n",
      "val_f_mae: 0.006130\n",
      "val_f_rmse: 0.014142\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2410, Val Loss: 0.1842\n",
      "train_e/atom_mae: 0.007007\n",
      "train_e/atom_rmse: 0.010361\n",
      "train_f_mae: 0.006737\n",
      "train_f_rmse: 0.015477\n",
      "val_e/atom_mae: 0.006553\n",
      "val_e/atom_rmse: 0.010072\n",
      "val_f_mae: 0.006082\n",
      "val_f_rmse: 0.014102\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2372, Val Loss: 0.1835\n",
      "train_e/atom_mae: 0.007143\n",
      "train_e/atom_rmse: 0.010468\n",
      "train_f_mae: 0.006751\n",
      "train_f_rmse: 0.015491\n",
      "val_e/atom_mae: 0.006408\n",
      "val_e/atom_rmse: 0.010001\n",
      "val_f_mae: 0.006067\n",
      "val_f_rmse: 0.014072\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2375, Val Loss: 0.1820\n",
      "train_e/atom_mae: 0.007151\n",
      "train_e/atom_rmse: 0.010426\n",
      "train_f_mae: 0.006715\n",
      "train_f_rmse: 0.015461\n",
      "val_e/atom_mae: 0.006732\n",
      "val_e/atom_rmse: 0.010081\n",
      "val_f_mae: 0.006020\n",
      "val_f_rmse: 0.014027\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2358, Val Loss: 0.1812\n",
      "train_e/atom_mae: 0.007163\n",
      "train_e/atom_rmse: 0.010339\n",
      "train_f_mae: 0.006573\n",
      "train_f_rmse: 0.015345\n",
      "val_e/atom_mae: 0.006660\n",
      "val_e/atom_rmse: 0.010078\n",
      "val_f_mae: 0.005993\n",
      "val_f_rmse: 0.013996\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2349, Val Loss: 0.1804\n",
      "train_e/atom_mae: 0.007044\n",
      "train_e/atom_rmse: 0.010316\n",
      "train_f_mae: 0.006547\n",
      "train_f_rmse: 0.015317\n",
      "val_e/atom_mae: 0.006365\n",
      "val_e/atom_rmse: 0.010034\n",
      "val_f_mae: 0.005951\n",
      "val_f_rmse: 0.013963\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2345, Val Loss: 0.1792\n",
      "train_e/atom_mae: 0.007050\n",
      "train_e/atom_rmse: 0.010260\n",
      "train_f_mae: 0.006462\n",
      "train_f_rmse: 0.015268\n",
      "val_e/atom_mae: 0.006772\n",
      "val_e/atom_rmse: 0.010138\n",
      "val_f_mae: 0.005934\n",
      "val_f_rmse: 0.013920\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2326, Val Loss: 0.1786\n",
      "train_e/atom_mae: 0.006997\n",
      "train_e/atom_rmse: 0.010312\n",
      "train_f_mae: 0.006460\n",
      "train_f_rmse: 0.015263\n",
      "val_e/atom_mae: 0.006787\n",
      "val_e/atom_rmse: 0.010140\n",
      "val_f_mae: 0.005906\n",
      "val_f_rmse: 0.013899\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2326, Val Loss: 0.1783\n",
      "train_e/atom_mae: 0.006911\n",
      "train_e/atom_rmse: 0.010189\n",
      "train_f_mae: 0.006406\n",
      "train_f_rmse: 0.015206\n",
      "val_e/atom_mae: 0.006672\n",
      "val_e/atom_rmse: 0.010060\n",
      "val_f_mae: 0.005885\n",
      "val_f_rmse: 0.013888\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2326, Val Loss: 0.1779\n",
      "train_e/atom_mae: 0.007081\n",
      "train_e/atom_rmse: 0.010263\n",
      "train_f_mae: 0.006395\n",
      "train_f_rmse: 0.015197\n",
      "val_e/atom_mae: 0.006628\n",
      "val_e/atom_rmse: 0.010026\n",
      "val_f_mae: 0.005876\n",
      "val_f_rmse: 0.013871\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2328, Val Loss: 0.1772\n",
      "train_e/atom_mae: 0.006860\n",
      "train_e/atom_rmse: 0.010200\n",
      "train_f_mae: 0.006345\n",
      "train_f_rmse: 0.015168\n",
      "val_e/atom_mae: 0.006520\n",
      "val_e/atom_rmse: 0.010021\n",
      "val_f_mae: 0.005846\n",
      "val_f_rmse: 0.013845\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2306, Val Loss: 0.1766\n",
      "train_e/atom_mae: 0.006925\n",
      "train_e/atom_rmse: 0.010173\n",
      "train_f_mae: 0.006309\n",
      "train_f_rmse: 0.015131\n",
      "val_e/atom_mae: 0.006623\n",
      "val_e/atom_rmse: 0.010029\n",
      "val_f_mae: 0.005837\n",
      "val_f_rmse: 0.013824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2267, Val Loss: 0.1762\n",
      "train_e/atom_mae: 0.006779\n",
      "train_e/atom_rmse: 0.010108\n",
      "train_f_mae: 0.006231\n",
      "train_f_rmse: 0.015077\n",
      "val_e/atom_mae: 0.006542\n",
      "val_e/atom_rmse: 0.010014\n",
      "val_f_mae: 0.005818\n",
      "val_f_rmse: 0.013806\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2253, Val Loss: 0.1759\n",
      "train_e/atom_mae: 0.007019\n",
      "train_e/atom_rmse: 0.010267\n",
      "train_f_mae: 0.006197\n",
      "train_f_rmse: 0.015050\n",
      "val_e/atom_mae: 0.006668\n",
      "val_e/atom_rmse: 0.010065\n",
      "val_f_mae: 0.005807\n",
      "val_f_rmse: 0.013796\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2231, Val Loss: 0.1761\n",
      "train_e/atom_mae: 0.006833\n",
      "train_e/atom_rmse: 0.010143\n",
      "train_f_mae: 0.006194\n",
      "train_f_rmse: 0.015037\n",
      "val_e/atom_mae: 0.006527\n",
      "val_e/atom_rmse: 0.009970\n",
      "val_f_mae: 0.005792\n",
      "val_f_rmse: 0.013806\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2255, Val Loss: 0.1756\n",
      "train_e/atom_mae: 0.006817\n",
      "train_e/atom_rmse: 0.010127\n",
      "train_f_mae: 0.006200\n",
      "train_f_rmse: 0.015039\n",
      "val_e/atom_mae: 0.006609\n",
      "val_e/atom_rmse: 0.010007\n",
      "val_f_mae: 0.005787\n",
      "val_f_rmse: 0.013786\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2232, Val Loss: 0.1750\n",
      "train_e/atom_mae: 0.006806\n",
      "train_e/atom_rmse: 0.010094\n",
      "train_f_mae: 0.006129\n",
      "train_f_rmse: 0.014991\n",
      "val_e/atom_mae: 0.006459\n",
      "val_e/atom_rmse: 0.009979\n",
      "val_f_mae: 0.005774\n",
      "val_f_rmse: 0.013767\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2244, Val Loss: 0.1746\n",
      "train_e/atom_mae: 0.006916\n",
      "train_e/atom_rmse: 0.010193\n",
      "train_f_mae: 0.006101\n",
      "train_f_rmse: 0.014975\n",
      "val_e/atom_mae: 0.006585\n",
      "val_e/atom_rmse: 0.010002\n",
      "val_f_mae: 0.005753\n",
      "val_f_rmse: 0.013753\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2239, Val Loss: 0.1744\n",
      "train_e/atom_mae: 0.006769\n",
      "train_e/atom_rmse: 0.010087\n",
      "train_f_mae: 0.006112\n",
      "train_f_rmse: 0.014973\n",
      "val_e/atom_mae: 0.006503\n",
      "val_e/atom_rmse: 0.010008\n",
      "val_f_mae: 0.005748\n",
      "val_f_rmse: 0.013745\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2208, Val Loss: 0.1738\n",
      "train_e/atom_mae: 0.006833\n",
      "train_e/atom_rmse: 0.010099\n",
      "train_f_mae: 0.006086\n",
      "train_f_rmse: 0.014939\n",
      "val_e/atom_mae: 0.006548\n",
      "val_e/atom_rmse: 0.010065\n",
      "val_f_mae: 0.005738\n",
      "val_f_rmse: 0.013721\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2239, Val Loss: 0.1737\n",
      "train_e/atom_mae: 0.006785\n",
      "train_e/atom_rmse: 0.010073\n",
      "train_f_mae: 0.006028\n",
      "train_f_rmse: 0.014918\n",
      "val_e/atom_mae: 0.006446\n",
      "val_e/atom_rmse: 0.010021\n",
      "val_f_mae: 0.005727\n",
      "val_f_rmse: 0.013716\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2224, Val Loss: 0.1736\n",
      "train_e/atom_mae: 0.006828\n",
      "train_e/atom_rmse: 0.010070\n",
      "train_f_mae: 0.006021\n",
      "train_f_rmse: 0.014906\n",
      "val_e/atom_mae: 0.006593\n",
      "val_e/atom_rmse: 0.010008\n",
      "val_f_mae: 0.005722\n",
      "val_f_rmse: 0.013717\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2202, Val Loss: 0.1735\n",
      "train_e/atom_mae: 0.006717\n",
      "train_e/atom_rmse: 0.010037\n",
      "train_f_mae: 0.006031\n",
      "train_f_rmse: 0.014911\n",
      "val_e/atom_mae: 0.006527\n",
      "val_e/atom_rmse: 0.010018\n",
      "val_f_mae: 0.005715\n",
      "val_f_rmse: 0.013709\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2215, Val Loss: 0.1735\n",
      "train_e/atom_mae: 0.006755\n",
      "train_e/atom_rmse: 0.010069\n",
      "train_f_mae: 0.006025\n",
      "train_f_rmse: 0.014901\n",
      "val_e/atom_mae: 0.006446\n",
      "val_e/atom_rmse: 0.009976\n",
      "val_f_mae: 0.005718\n",
      "val_f_rmse: 0.013707\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.3249, Val Loss: 0.6091\n",
      "train_e/atom_mae: 0.007767\n",
      "train_e/atom_rmse: 0.010839\n",
      "train_f_mae: 0.007320\n",
      "train_f_rmse: 0.015961\n",
      "val_e/atom_mae: 0.007825\n",
      "val_e/atom_rmse: 0.012376\n",
      "val_f_mae: 0.015128\n",
      "val_f_rmse: 0.023456\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.3739, Val Loss: 0.1753\n",
      "train_e/atom_mae: 0.008047\n",
      "train_e/atom_rmse: 0.011144\n",
      "train_f_mae: 0.011155\n",
      "train_f_rmse: 0.019825\n",
      "val_e/atom_mae: 0.006859\n",
      "val_e/atom_rmse: 0.010201\n",
      "val_f_mae: 0.005832\n",
      "val_f_rmse: 0.013806\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2687, Val Loss: 0.1747\n",
      "train_e/atom_mae: 0.008325\n",
      "train_e/atom_rmse: 0.011484\n",
      "train_f_mae: 0.008473\n",
      "train_f_rmse: 0.016994\n",
      "val_e/atom_mae: 0.006472\n",
      "val_e/atom_rmse: 0.010074\n",
      "val_f_mae: 0.005787\n",
      "val_f_rmse: 0.013756\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2605, Val Loss: 0.1749\n",
      "train_e/atom_mae: 0.008815\n",
      "train_e/atom_rmse: 0.012211\n",
      "train_f_mae: 0.007654\n",
      "train_f_rmse: 0.016162\n",
      "val_e/atom_mae: 0.006582\n",
      "val_e/atom_rmse: 0.010077\n",
      "val_f_mae: 0.005776\n",
      "val_f_rmse: 0.013770\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2466, Val Loss: 0.1735\n",
      "train_e/atom_mae: 0.008343\n",
      "train_e/atom_rmse: 0.011693\n",
      "train_f_mae: 0.007141\n",
      "train_f_rmse: 0.015710\n",
      "val_e/atom_mae: 0.006617\n",
      "val_e/atom_rmse: 0.010009\n",
      "val_f_mae: 0.005748\n",
      "val_f_rmse: 0.013707\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2500, Val Loss: 0.1730\n",
      "train_e/atom_mae: 0.009059\n",
      "train_e/atom_rmse: 0.012362\n",
      "train_f_mae: 0.007156\n",
      "train_f_rmse: 0.015751\n",
      "val_e/atom_mae: 0.006559\n",
      "val_e/atom_rmse: 0.010068\n",
      "val_f_mae: 0.005726\n",
      "val_f_rmse: 0.013695\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2395, Val Loss: 0.1717\n",
      "train_e/atom_mae: 0.008461\n",
      "train_e/atom_rmse: 0.011782\n",
      "train_f_mae: 0.006860\n",
      "train_f_rmse: 0.015484\n",
      "val_e/atom_mae: 0.006643\n",
      "val_e/atom_rmse: 0.010079\n",
      "val_f_mae: 0.005689\n",
      "val_f_rmse: 0.013641\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2312, Val Loss: 0.1717\n",
      "train_e/atom_mae: 0.008279\n",
      "train_e/atom_rmse: 0.011625\n",
      "train_f_mae: 0.006825\n",
      "train_f_rmse: 0.015462\n",
      "val_e/atom_mae: 0.006551\n",
      "val_e/atom_rmse: 0.009974\n",
      "val_f_mae: 0.005679\n",
      "val_f_rmse: 0.013635\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2387, Val Loss: 0.1704\n",
      "train_e/atom_mae: 0.008578\n",
      "train_e/atom_rmse: 0.011863\n",
      "train_f_mae: 0.006707\n",
      "train_f_rmse: 0.015374\n",
      "val_e/atom_mae: 0.006499\n",
      "val_e/atom_rmse: 0.010014\n",
      "val_f_mae: 0.005637\n",
      "val_f_rmse: 0.013588\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2368, Val Loss: 0.1702\n",
      "train_e/atom_mae: 0.008237\n",
      "train_e/atom_rmse: 0.011600\n",
      "train_f_mae: 0.006669\n",
      "train_f_rmse: 0.015322\n",
      "val_e/atom_mae: 0.006444\n",
      "val_e/atom_rmse: 0.010055\n",
      "val_f_mae: 0.005637\n",
      "val_f_rmse: 0.013580\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2237, Val Loss: 0.1702\n",
      "train_e/atom_mae: 0.007978\n",
      "train_e/atom_rmse: 0.011396\n",
      "train_f_mae: 0.006530\n",
      "train_f_rmse: 0.015221\n",
      "val_e/atom_mae: 0.006582\n",
      "val_e/atom_rmse: 0.010054\n",
      "val_f_mae: 0.005622\n",
      "val_f_rmse: 0.013584\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2289, Val Loss: 0.1700\n",
      "train_e/atom_mae: 0.007767\n",
      "train_e/atom_rmse: 0.011158\n",
      "train_f_mae: 0.006444\n",
      "train_f_rmse: 0.015129\n",
      "val_e/atom_mae: 0.006428\n",
      "val_e/atom_rmse: 0.009958\n",
      "val_f_mae: 0.005611\n",
      "val_f_rmse: 0.013569\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2251, Val Loss: 0.1691\n",
      "train_e/atom_mae: 0.007774\n",
      "train_e/atom_rmse: 0.011162\n",
      "train_f_mae: 0.006330\n",
      "train_f_rmse: 0.015051\n",
      "val_e/atom_mae: 0.006549\n",
      "val_e/atom_rmse: 0.010013\n",
      "val_f_mae: 0.005581\n",
      "val_f_rmse: 0.013546\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2291, Val Loss: 0.1684\n",
      "train_e/atom_mae: 0.007523\n",
      "train_e/atom_rmse: 0.010766\n",
      "train_f_mae: 0.006372\n",
      "train_f_rmse: 0.015057\n",
      "val_e/atom_mae: 0.006429\n",
      "val_e/atom_rmse: 0.010095\n",
      "val_f_mae: 0.005573\n",
      "val_f_rmse: 0.013508\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2234, Val Loss: 0.1682\n",
      "train_e/atom_mae: 0.007629\n",
      "train_e/atom_rmse: 0.010766\n",
      "train_f_mae: 0.006295\n",
      "train_f_rmse: 0.014995\n",
      "val_e/atom_mae: 0.006520\n",
      "val_e/atom_rmse: 0.009960\n",
      "val_f_mae: 0.005556\n",
      "val_f_rmse: 0.013512\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2208, Val Loss: 0.1676\n",
      "train_e/atom_mae: 0.007558\n",
      "train_e/atom_rmse: 0.010830\n",
      "train_f_mae: 0.006178\n",
      "train_f_rmse: 0.014917\n",
      "val_e/atom_mae: 0.006490\n",
      "val_e/atom_rmse: 0.010045\n",
      "val_f_mae: 0.005545\n",
      "val_f_rmse: 0.013483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2226, Val Loss: 0.1669\n",
      "train_e/atom_mae: 0.007419\n",
      "train_e/atom_rmse: 0.010762\n",
      "train_f_mae: 0.006129\n",
      "train_f_rmse: 0.014878\n",
      "val_e/atom_mae: 0.006519\n",
      "val_e/atom_rmse: 0.010066\n",
      "val_f_mae: 0.005516\n",
      "val_f_rmse: 0.013457\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2220, Val Loss: 0.1671\n",
      "train_e/atom_mae: 0.007451\n",
      "train_e/atom_rmse: 0.010659\n",
      "train_f_mae: 0.006088\n",
      "train_f_rmse: 0.014833\n",
      "val_e/atom_mae: 0.006470\n",
      "val_e/atom_rmse: 0.010017\n",
      "val_f_mae: 0.005529\n",
      "val_f_rmse: 0.013466\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2191, Val Loss: 0.1665\n",
      "train_e/atom_mae: 0.007348\n",
      "train_e/atom_rmse: 0.010593\n",
      "train_f_mae: 0.006055\n",
      "train_f_rmse: 0.014816\n",
      "val_e/atom_mae: 0.006471\n",
      "val_e/atom_rmse: 0.010032\n",
      "val_f_mae: 0.005514\n",
      "val_f_rmse: 0.013439\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2176, Val Loss: 0.1661\n",
      "train_e/atom_mae: 0.007468\n",
      "train_e/atom_rmse: 0.010740\n",
      "train_f_mae: 0.006021\n",
      "train_f_rmse: 0.014778\n",
      "val_e/atom_mae: 0.006499\n",
      "val_e/atom_rmse: 0.010077\n",
      "val_f_mae: 0.005502\n",
      "val_f_rmse: 0.013429\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2169, Val Loss: 0.1662\n",
      "train_e/atom_mae: 0.007352\n",
      "train_e/atom_rmse: 0.010608\n",
      "train_f_mae: 0.005952\n",
      "train_f_rmse: 0.014740\n",
      "val_e/atom_mae: 0.006545\n",
      "val_e/atom_rmse: 0.010035\n",
      "val_f_mae: 0.005504\n",
      "val_f_rmse: 0.013432\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2190, Val Loss: 0.1657\n",
      "train_e/atom_mae: 0.007154\n",
      "train_e/atom_rmse: 0.010447\n",
      "train_f_mae: 0.006009\n",
      "train_f_rmse: 0.014749\n",
      "val_e/atom_mae: 0.006570\n",
      "val_e/atom_rmse: 0.010072\n",
      "val_f_mae: 0.005495\n",
      "val_f_rmse: 0.013415\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2168, Val Loss: 0.1651\n",
      "train_e/atom_mae: 0.007273\n",
      "train_e/atom_rmse: 0.010465\n",
      "train_f_mae: 0.005949\n",
      "train_f_rmse: 0.014683\n",
      "val_e/atom_mae: 0.006658\n",
      "val_e/atom_rmse: 0.010153\n",
      "val_f_mae: 0.005480\n",
      "val_f_rmse: 0.013391\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2163, Val Loss: 0.1654\n",
      "train_e/atom_mae: 0.007017\n",
      "train_e/atom_rmse: 0.010298\n",
      "train_f_mae: 0.005963\n",
      "train_f_rmse: 0.014711\n",
      "val_e/atom_mae: 0.006549\n",
      "val_e/atom_rmse: 0.010062\n",
      "val_f_mae: 0.005483\n",
      "val_f_rmse: 0.013397\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2141, Val Loss: 0.1652\n",
      "train_e/atom_mae: 0.007192\n",
      "train_e/atom_rmse: 0.010466\n",
      "train_f_mae: 0.005893\n",
      "train_f_rmse: 0.014646\n",
      "val_e/atom_mae: 0.006545\n",
      "val_e/atom_rmse: 0.010062\n",
      "val_f_mae: 0.005483\n",
      "val_f_rmse: 0.013388\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2140, Val Loss: 0.1649\n",
      "train_e/atom_mae: 0.007151\n",
      "train_e/atom_rmse: 0.010401\n",
      "train_f_mae: 0.005864\n",
      "train_f_rmse: 0.014625\n",
      "val_e/atom_mae: 0.006505\n",
      "val_e/atom_rmse: 0.010071\n",
      "val_f_mae: 0.005478\n",
      "val_f_rmse: 0.013382\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2145, Val Loss: 0.1643\n",
      "train_e/atom_mae: 0.007130\n",
      "train_e/atom_rmse: 0.010382\n",
      "train_f_mae: 0.005865\n",
      "train_f_rmse: 0.014621\n",
      "val_e/atom_mae: 0.006707\n",
      "val_e/atom_rmse: 0.010175\n",
      "val_f_mae: 0.005461\n",
      "val_f_rmse: 0.013356\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2133, Val Loss: 0.1642\n",
      "train_e/atom_mae: 0.007091\n",
      "train_e/atom_rmse: 0.010358\n",
      "train_f_mae: 0.005849\n",
      "train_f_rmse: 0.014606\n",
      "val_e/atom_mae: 0.006551\n",
      "val_e/atom_rmse: 0.010107\n",
      "val_f_mae: 0.005453\n",
      "val_f_rmse: 0.013352\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2115, Val Loss: 0.1643\n",
      "train_e/atom_mae: 0.006880\n",
      "train_e/atom_rmse: 0.010159\n",
      "train_f_mae: 0.005797\n",
      "train_f_rmse: 0.014564\n",
      "val_e/atom_mae: 0.006554\n",
      "val_e/atom_rmse: 0.010069\n",
      "val_f_mae: 0.005458\n",
      "val_f_rmse: 0.013356\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2117, Val Loss: 0.1639\n",
      "train_e/atom_mae: 0.007274\n",
      "train_e/atom_rmse: 0.010500\n",
      "train_f_mae: 0.005785\n",
      "train_f_rmse: 0.014548\n",
      "val_e/atom_mae: 0.006552\n",
      "val_e/atom_rmse: 0.010088\n",
      "val_f_mae: 0.005453\n",
      "val_f_rmse: 0.013340\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2112, Val Loss: 0.1638\n",
      "train_e/atom_mae: 0.007193\n",
      "train_e/atom_rmse: 0.010358\n",
      "train_f_mae: 0.005778\n",
      "train_f_rmse: 0.014532\n",
      "val_e/atom_mae: 0.006701\n",
      "val_e/atom_rmse: 0.010140\n",
      "val_f_mae: 0.005451\n",
      "val_f_rmse: 0.013338\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2117, Val Loss: 0.1637\n",
      "train_e/atom_mae: 0.006987\n",
      "train_e/atom_rmse: 0.010261\n",
      "train_f_mae: 0.005771\n",
      "train_f_rmse: 0.014533\n",
      "val_e/atom_mae: 0.006540\n",
      "val_e/atom_rmse: 0.010116\n",
      "val_f_mae: 0.005452\n",
      "val_f_rmse: 0.013332\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2102, Val Loss: 0.1635\n",
      "train_e/atom_mae: 0.007108\n",
      "train_e/atom_rmse: 0.010366\n",
      "train_f_mae: 0.005758\n",
      "train_f_rmse: 0.014504\n",
      "val_e/atom_mae: 0.006576\n",
      "val_e/atom_rmse: 0.010101\n",
      "val_f_mae: 0.005447\n",
      "val_f_rmse: 0.013323\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2120, Val Loss: 0.1634\n",
      "train_e/atom_mae: 0.006879\n",
      "train_e/atom_rmse: 0.010178\n",
      "train_f_mae: 0.005760\n",
      "train_f_rmse: 0.014516\n",
      "val_e/atom_mae: 0.006477\n",
      "val_e/atom_rmse: 0.010086\n",
      "val_f_mae: 0.005447\n",
      "val_f_rmse: 0.013324\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2101, Val Loss: 0.1631\n",
      "train_e/atom_mae: 0.006914\n",
      "train_e/atom_rmse: 0.010148\n",
      "train_f_mae: 0.005722\n",
      "train_f_rmse: 0.014481\n",
      "val_e/atom_mae: 0.006643\n",
      "val_e/atom_rmse: 0.010154\n",
      "val_f_mae: 0.005438\n",
      "val_f_rmse: 0.013306\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2097, Val Loss: 0.1629\n",
      "train_e/atom_mae: 0.007000\n",
      "train_e/atom_rmse: 0.010271\n",
      "train_f_mae: 0.005710\n",
      "train_f_rmse: 0.014475\n",
      "val_e/atom_mae: 0.006654\n",
      "val_e/atom_rmse: 0.010165\n",
      "val_f_mae: 0.005431\n",
      "val_f_rmse: 0.013302\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2093, Val Loss: 0.1627\n",
      "train_e/atom_mae: 0.006911\n",
      "train_e/atom_rmse: 0.010211\n",
      "train_f_mae: 0.005707\n",
      "train_f_rmse: 0.014470\n",
      "val_e/atom_mae: 0.006570\n",
      "val_e/atom_rmse: 0.010123\n",
      "val_f_mae: 0.005430\n",
      "val_f_rmse: 0.013295\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2086, Val Loss: 0.1630\n",
      "train_e/atom_mae: 0.006835\n",
      "train_e/atom_rmse: 0.010144\n",
      "train_f_mae: 0.005700\n",
      "train_f_rmse: 0.014454\n",
      "val_e/atom_mae: 0.006671\n",
      "val_e/atom_rmse: 0.010143\n",
      "val_f_mae: 0.005434\n",
      "val_f_rmse: 0.013306\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2087, Val Loss: 0.1628\n",
      "train_e/atom_mae: 0.006844\n",
      "train_e/atom_rmse: 0.010192\n",
      "train_f_mae: 0.005680\n",
      "train_f_rmse: 0.014443\n",
      "val_e/atom_mae: 0.006635\n",
      "val_e/atom_rmse: 0.010101\n",
      "val_f_mae: 0.005428\n",
      "val_f_rmse: 0.013299\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2082, Val Loss: 0.1626\n",
      "train_e/atom_mae: 0.006780\n",
      "train_e/atom_rmse: 0.010118\n",
      "train_f_mae: 0.005673\n",
      "train_f_rmse: 0.014434\n",
      "val_e/atom_mae: 0.006583\n",
      "val_e/atom_rmse: 0.010133\n",
      "val_f_mae: 0.005425\n",
      "val_f_rmse: 0.013288\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.2494, Val Loss: 0.2242\n",
      "train_e/atom_mae: 0.007719\n",
      "train_e/atom_rmse: 0.010824\n",
      "train_f_mae: 0.006370\n",
      "train_f_rmse: 0.014933\n",
      "val_e/atom_mae: 0.010897\n",
      "val_e/atom_rmse: 0.012649\n",
      "val_f_mae: 0.008035\n",
      "val_f_rmse: 0.015053\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.2338, Val Loss: 0.1628\n",
      "train_e/atom_mae: 0.008185\n",
      "train_e/atom_rmse: 0.011411\n",
      "train_f_mae: 0.007514\n",
      "train_f_rmse: 0.015921\n",
      "val_e/atom_mae: 0.006638\n",
      "val_e/atom_rmse: 0.010194\n",
      "val_f_mae: 0.005430\n",
      "val_f_rmse: 0.013306\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2403, Val Loss: 0.1630\n",
      "train_e/atom_mae: 0.008617\n",
      "train_e/atom_rmse: 0.012035\n",
      "train_f_mae: 0.006983\n",
      "train_f_rmse: 0.015415\n",
      "val_e/atom_mae: 0.006661\n",
      "val_e/atom_rmse: 0.010157\n",
      "val_f_mae: 0.005441\n",
      "val_f_rmse: 0.013300\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2325, Val Loss: 0.1630\n",
      "train_e/atom_mae: 0.008481\n",
      "train_e/atom_rmse: 0.011776\n",
      "train_f_mae: 0.006906\n",
      "train_f_rmse: 0.015328\n",
      "val_e/atom_mae: 0.006592\n",
      "val_e/atom_rmse: 0.010087\n",
      "val_f_mae: 0.005438\n",
      "val_f_rmse: 0.013313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2345, Val Loss: 0.1625\n",
      "train_e/atom_mae: 0.008119\n",
      "train_e/atom_rmse: 0.011498\n",
      "train_f_mae: 0.006666\n",
      "train_f_rmse: 0.015138\n",
      "val_e/atom_mae: 0.006680\n",
      "val_e/atom_rmse: 0.010134\n",
      "val_f_mae: 0.005436\n",
      "val_f_rmse: 0.013290\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2247, Val Loss: 0.1624\n",
      "train_e/atom_mae: 0.008344\n",
      "train_e/atom_rmse: 0.011565\n",
      "train_f_mae: 0.006672\n",
      "train_f_rmse: 0.015176\n",
      "val_e/atom_mae: 0.006600\n",
      "val_e/atom_rmse: 0.010152\n",
      "val_f_mae: 0.005426\n",
      "val_f_rmse: 0.013281\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2286, Val Loss: 0.1620\n",
      "train_e/atom_mae: 0.007946\n",
      "train_e/atom_rmse: 0.011320\n",
      "train_f_mae: 0.006519\n",
      "train_f_rmse: 0.015009\n",
      "val_e/atom_mae: 0.007028\n",
      "val_e/atom_rmse: 0.010246\n",
      "val_f_mae: 0.005411\n",
      "val_f_rmse: 0.013265\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2211, Val Loss: 0.1621\n",
      "train_e/atom_mae: 0.008300\n",
      "train_e/atom_rmse: 0.011545\n",
      "train_f_mae: 0.006382\n",
      "train_f_rmse: 0.014904\n",
      "val_e/atom_mae: 0.006499\n",
      "val_e/atom_rmse: 0.010108\n",
      "val_f_mae: 0.005409\n",
      "val_f_rmse: 0.013271\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2207, Val Loss: 0.1615\n",
      "train_e/atom_mae: 0.008089\n",
      "train_e/atom_rmse: 0.011234\n",
      "train_f_mae: 0.006284\n",
      "train_f_rmse: 0.014817\n",
      "val_e/atom_mae: 0.006564\n",
      "val_e/atom_rmse: 0.010062\n",
      "val_f_mae: 0.005395\n",
      "val_f_rmse: 0.013245\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2192, Val Loss: 0.1613\n",
      "train_e/atom_mae: 0.008031\n",
      "train_e/atom_rmse: 0.011492\n",
      "train_f_mae: 0.006252\n",
      "train_f_rmse: 0.014794\n",
      "val_e/atom_mae: 0.006575\n",
      "val_e/atom_rmse: 0.010150\n",
      "val_f_mae: 0.005389\n",
      "val_f_rmse: 0.013243\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2161, Val Loss: 0.1614\n",
      "train_e/atom_mae: 0.007751\n",
      "train_e/atom_rmse: 0.011137\n",
      "train_f_mae: 0.006131\n",
      "train_f_rmse: 0.014672\n",
      "val_e/atom_mae: 0.006660\n",
      "val_e/atom_rmse: 0.010179\n",
      "val_f_mae: 0.005393\n",
      "val_f_rmse: 0.013244\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2204, Val Loss: 0.1608\n",
      "train_e/atom_mae: 0.007672\n",
      "train_e/atom_rmse: 0.011016\n",
      "train_f_mae: 0.006200\n",
      "train_f_rmse: 0.014719\n",
      "val_e/atom_mae: 0.006591\n",
      "val_e/atom_rmse: 0.010163\n",
      "val_f_mae: 0.005381\n",
      "val_f_rmse: 0.013218\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2125, Val Loss: 0.1609\n",
      "train_e/atom_mae: 0.007779\n",
      "train_e/atom_rmse: 0.011151\n",
      "train_f_mae: 0.006023\n",
      "train_f_rmse: 0.014605\n",
      "val_e/atom_mae: 0.006592\n",
      "val_e/atom_rmse: 0.010191\n",
      "val_f_mae: 0.005391\n",
      "val_f_rmse: 0.013220\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2110, Val Loss: 0.1604\n",
      "train_e/atom_mae: 0.007521\n",
      "train_e/atom_rmse: 0.010778\n",
      "train_f_mae: 0.006006\n",
      "train_f_rmse: 0.014567\n",
      "val_e/atom_mae: 0.006660\n",
      "val_e/atom_rmse: 0.010197\n",
      "val_f_mae: 0.005373\n",
      "val_f_rmse: 0.013201\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2119, Val Loss: 0.1604\n",
      "train_e/atom_mae: 0.007475\n",
      "train_e/atom_rmse: 0.010773\n",
      "train_f_mae: 0.005910\n",
      "train_f_rmse: 0.014493\n",
      "val_e/atom_mae: 0.006694\n",
      "val_e/atom_rmse: 0.010159\n",
      "val_f_mae: 0.005377\n",
      "val_f_rmse: 0.013203\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2117, Val Loss: 0.1602\n",
      "train_e/atom_mae: 0.007600\n",
      "train_e/atom_rmse: 0.010927\n",
      "train_f_mae: 0.005972\n",
      "train_f_rmse: 0.014541\n",
      "val_e/atom_mae: 0.006732\n",
      "val_e/atom_rmse: 0.010196\n",
      "val_f_mae: 0.005372\n",
      "val_f_rmse: 0.013195\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2094, Val Loss: 0.1600\n",
      "train_e/atom_mae: 0.007345\n",
      "train_e/atom_rmse: 0.010765\n",
      "train_f_mae: 0.005880\n",
      "train_f_rmse: 0.014467\n",
      "val_e/atom_mae: 0.006628\n",
      "val_e/atom_rmse: 0.010203\n",
      "val_f_mae: 0.005368\n",
      "val_f_rmse: 0.013183\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2100, Val Loss: 0.1603\n",
      "train_e/atom_mae: 0.007391\n",
      "train_e/atom_rmse: 0.010659\n",
      "train_f_mae: 0.005884\n",
      "train_f_rmse: 0.014452\n",
      "val_e/atom_mae: 0.006623\n",
      "val_e/atom_rmse: 0.010207\n",
      "val_f_mae: 0.005378\n",
      "val_f_rmse: 0.013198\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2084, Val Loss: 0.1599\n",
      "train_e/atom_mae: 0.007265\n",
      "train_e/atom_rmse: 0.010528\n",
      "train_f_mae: 0.005821\n",
      "train_f_rmse: 0.014400\n",
      "val_e/atom_mae: 0.006588\n",
      "val_e/atom_rmse: 0.010091\n",
      "val_f_mae: 0.005359\n",
      "val_f_rmse: 0.013185\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2066, Val Loss: 0.1594\n",
      "train_e/atom_mae: 0.007381\n",
      "train_e/atom_rmse: 0.010662\n",
      "train_f_mae: 0.005783\n",
      "train_f_rmse: 0.014371\n",
      "val_e/atom_mae: 0.006668\n",
      "val_e/atom_rmse: 0.010241\n",
      "val_f_mae: 0.005357\n",
      "val_f_rmse: 0.013162\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2057, Val Loss: 0.1595\n",
      "train_e/atom_mae: 0.007320\n",
      "train_e/atom_rmse: 0.010742\n",
      "train_f_mae: 0.005783\n",
      "train_f_rmse: 0.014371\n",
      "val_e/atom_mae: 0.006632\n",
      "val_e/atom_rmse: 0.010257\n",
      "val_f_mae: 0.005356\n",
      "val_f_rmse: 0.013163\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2062, Val Loss: 0.1596\n",
      "train_e/atom_mae: 0.007363\n",
      "train_e/atom_rmse: 0.010668\n",
      "train_f_mae: 0.005796\n",
      "train_f_rmse: 0.014374\n",
      "val_e/atom_mae: 0.006639\n",
      "val_e/atom_rmse: 0.010162\n",
      "val_f_mae: 0.005358\n",
      "val_f_rmse: 0.013169\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2037, Val Loss: 0.1592\n",
      "train_e/atom_mae: 0.007260\n",
      "train_e/atom_rmse: 0.010493\n",
      "train_f_mae: 0.005720\n",
      "train_f_rmse: 0.014302\n",
      "val_e/atom_mae: 0.006729\n",
      "val_e/atom_rmse: 0.010258\n",
      "val_f_mae: 0.005343\n",
      "val_f_rmse: 0.013151\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2065, Val Loss: 0.1597\n",
      "train_e/atom_mae: 0.007293\n",
      "train_e/atom_rmse: 0.010589\n",
      "train_f_mae: 0.005773\n",
      "train_f_rmse: 0.014349\n",
      "val_e/atom_mae: 0.006660\n",
      "val_e/atom_rmse: 0.010153\n",
      "val_f_mae: 0.005356\n",
      "val_f_rmse: 0.013172\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2056, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.007161\n",
      "train_e/atom_rmse: 0.010458\n",
      "train_f_mae: 0.005689\n",
      "train_f_rmse: 0.014305\n",
      "val_e/atom_mae: 0.006676\n",
      "val_e/atom_rmse: 0.010237\n",
      "val_f_mae: 0.005344\n",
      "val_f_rmse: 0.013149\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2048, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.007212\n",
      "train_e/atom_rmse: 0.010502\n",
      "train_f_mae: 0.005683\n",
      "train_f_rmse: 0.014281\n",
      "val_e/atom_mae: 0.006651\n",
      "val_e/atom_rmse: 0.010217\n",
      "val_f_mae: 0.005343\n",
      "val_f_rmse: 0.013146\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2047, Val Loss: 0.1592\n",
      "train_e/atom_mae: 0.007020\n",
      "train_e/atom_rmse: 0.010422\n",
      "train_f_mae: 0.005666\n",
      "train_f_rmse: 0.014279\n",
      "val_e/atom_mae: 0.006639\n",
      "val_e/atom_rmse: 0.010209\n",
      "val_f_mae: 0.005345\n",
      "val_f_rmse: 0.013152\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2043, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.007076\n",
      "train_e/atom_rmse: 0.010424\n",
      "train_f_mae: 0.005660\n",
      "train_f_rmse: 0.014256\n",
      "val_e/atom_mae: 0.006721\n",
      "val_e/atom_rmse: 0.010239\n",
      "val_f_mae: 0.005341\n",
      "val_f_rmse: 0.013147\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2023, Val Loss: 0.1592\n",
      "train_e/atom_mae: 0.006876\n",
      "train_e/atom_rmse: 0.010289\n",
      "train_f_mae: 0.005620\n",
      "train_f_rmse: 0.014231\n",
      "val_e/atom_mae: 0.006564\n",
      "val_e/atom_rmse: 0.010187\n",
      "val_f_mae: 0.005347\n",
      "val_f_rmse: 0.013152\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2030, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.006982\n",
      "train_e/atom_rmse: 0.010332\n",
      "train_f_mae: 0.005618\n",
      "train_f_rmse: 0.014248\n",
      "val_e/atom_mae: 0.006570\n",
      "val_e/atom_rmse: 0.010206\n",
      "val_f_mae: 0.005342\n",
      "val_f_rmse: 0.013144\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2024, Val Loss: 0.1588\n",
      "train_e/atom_mae: 0.007043\n",
      "train_e/atom_rmse: 0.010374\n",
      "train_f_mae: 0.005603\n",
      "train_f_rmse: 0.014223\n",
      "val_e/atom_mae: 0.006674\n",
      "val_e/atom_rmse: 0.010208\n",
      "val_f_mae: 0.005334\n",
      "val_f_rmse: 0.013137\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2019, Val Loss: 0.1590\n",
      "train_e/atom_mae: 0.006981\n",
      "train_e/atom_rmse: 0.010338\n",
      "train_f_mae: 0.005596\n",
      "train_f_rmse: 0.014212\n",
      "val_e/atom_mae: 0.006585\n",
      "val_e/atom_rmse: 0.010217\n",
      "val_f_mae: 0.005341\n",
      "val_f_rmse: 0.013143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2018, Val Loss: 0.1588\n",
      "train_e/atom_mae: 0.007042\n",
      "train_e/atom_rmse: 0.010330\n",
      "train_f_mae: 0.005589\n",
      "train_f_rmse: 0.014204\n",
      "val_e/atom_mae: 0.006601\n",
      "val_e/atom_rmse: 0.010218\n",
      "val_f_mae: 0.005338\n",
      "val_f_rmse: 0.013137\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2022, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.006933\n",
      "train_e/atom_rmse: 0.010311\n",
      "train_f_mae: 0.005607\n",
      "train_f_rmse: 0.014215\n",
      "val_e/atom_mae: 0.006660\n",
      "val_e/atom_rmse: 0.010201\n",
      "val_f_mae: 0.005347\n",
      "val_f_rmse: 0.013145\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2016, Val Loss: 0.1591\n",
      "train_e/atom_mae: 0.006876\n",
      "train_e/atom_rmse: 0.010251\n",
      "train_f_mae: 0.005566\n",
      "train_f_rmse: 0.014181\n",
      "val_e/atom_mae: 0.006620\n",
      "val_e/atom_rmse: 0.010144\n",
      "val_f_mae: 0.005341\n",
      "val_f_rmse: 0.013146\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2009, Val Loss: 0.1590\n",
      "train_e/atom_mae: 0.006981\n",
      "train_e/atom_rmse: 0.010303\n",
      "train_f_mae: 0.005557\n",
      "train_f_rmse: 0.014170\n",
      "val_e/atom_mae: 0.006610\n",
      "val_e/atom_rmse: 0.010210\n",
      "val_f_mae: 0.005340\n",
      "val_f_rmse: 0.013141\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2012, Val Loss: 0.1588\n",
      "train_e/atom_mae: 0.006881\n",
      "train_e/atom_rmse: 0.010220\n",
      "train_f_mae: 0.005548\n",
      "train_f_rmse: 0.014165\n",
      "val_e/atom_mae: 0.006653\n",
      "val_e/atom_rmse: 0.010229\n",
      "val_f_mae: 0.005335\n",
      "val_f_rmse: 0.013135\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2014, Val Loss: 0.1589\n",
      "train_e/atom_mae: 0.006841\n",
      "train_e/atom_rmse: 0.010237\n",
      "train_f_mae: 0.005540\n",
      "train_f_rmse: 0.014166\n",
      "val_e/atom_mae: 0.006610\n",
      "val_e/atom_rmse: 0.010199\n",
      "val_f_mae: 0.005336\n",
      "val_f_rmse: 0.013138\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2005, Val Loss: 0.1587\n",
      "train_e/atom_mae: 0.006867\n",
      "train_e/atom_rmse: 0.010238\n",
      "train_f_mae: 0.005533\n",
      "train_f_rmse: 0.014159\n",
      "val_e/atom_mae: 0.006656\n",
      "val_e/atom_rmse: 0.010275\n",
      "val_f_mae: 0.005333\n",
      "val_f_rmse: 0.013129\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2009, Val Loss: 0.1588\n",
      "train_e/atom_mae: 0.006874\n",
      "train_e/atom_rmse: 0.010206\n",
      "train_f_mae: 0.005535\n",
      "train_f_rmse: 0.014155\n",
      "val_e/atom_mae: 0.006677\n",
      "val_e/atom_rmse: 0.010222\n",
      "val_f_mae: 0.005334\n",
      "val_f_rmse: 0.013136\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-3, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 20, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=400, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58c5567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.2972, Val Loss: 0.2938\n",
      "train_e/atom_mae: 0.004893\n",
      "train_e/atom_rmse: 0.007341\n",
      "train_f_mae: 0.006718\n",
      "train_f_rmse: 0.015093\n",
      "val_e/atom_mae: 0.006730\n",
      "val_e/atom_rmse: 0.008244\n",
      "val_f_mae: 0.008561\n",
      "val_f_rmse: 0.015613\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.2860, Val Loss: 0.2175\n",
      "train_e/atom_mae: 0.005032\n",
      "train_e/atom_rmse: 0.007455\n",
      "train_f_mae: 0.007283\n",
      "train_f_rmse: 0.015569\n",
      "val_e/atom_mae: 0.004416\n",
      "val_e/atom_rmse: 0.006965\n",
      "val_f_mae: 0.006039\n",
      "val_f_rmse: 0.013887\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2780, Val Loss: 0.2172\n",
      "train_e/atom_mae: 0.004931\n",
      "train_e/atom_rmse: 0.007319\n",
      "train_f_mae: 0.007089\n",
      "train_f_rmse: 0.015404\n",
      "val_e/atom_mae: 0.004403\n",
      "val_e/atom_rmse: 0.006900\n",
      "val_f_mae: 0.006035\n",
      "val_f_rmse: 0.013904\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2885, Val Loss: 0.2169\n",
      "train_e/atom_mae: 0.004927\n",
      "train_e/atom_rmse: 0.007330\n",
      "train_f_mae: 0.007041\n",
      "train_f_rmse: 0.015368\n",
      "val_e/atom_mae: 0.004460\n",
      "val_e/atom_rmse: 0.007007\n",
      "val_f_mae: 0.006010\n",
      "val_f_rmse: 0.013839\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2749, Val Loss: 0.2167\n",
      "train_e/atom_mae: 0.004854\n",
      "train_e/atom_rmse: 0.007258\n",
      "train_f_mae: 0.006825\n",
      "train_f_rmse: 0.015214\n",
      "val_e/atom_mae: 0.004410\n",
      "val_e/atom_rmse: 0.006974\n",
      "val_f_mae: 0.005991\n",
      "val_f_rmse: 0.013846\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2810, Val Loss: 0.2162\n",
      "train_e/atom_mae: 0.004854\n",
      "train_e/atom_rmse: 0.007260\n",
      "train_f_mae: 0.006876\n",
      "train_f_rmse: 0.015256\n",
      "val_e/atom_mae: 0.004435\n",
      "val_e/atom_rmse: 0.006951\n",
      "val_f_mae: 0.005980\n",
      "val_f_rmse: 0.013839\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2720, Val Loss: 0.2159\n",
      "train_e/atom_mae: 0.004789\n",
      "train_e/atom_rmse: 0.007184\n",
      "train_f_mae: 0.006700\n",
      "train_f_rmse: 0.015117\n",
      "val_e/atom_mae: 0.004480\n",
      "val_e/atom_rmse: 0.007040\n",
      "val_f_mae: 0.005949\n",
      "val_f_rmse: 0.013790\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2737, Val Loss: 0.2166\n",
      "train_e/atom_mae: 0.004797\n",
      "train_e/atom_rmse: 0.007184\n",
      "train_f_mae: 0.006735\n",
      "train_f_rmse: 0.015127\n",
      "val_e/atom_mae: 0.004368\n",
      "val_e/atom_rmse: 0.006867\n",
      "val_f_mae: 0.006026\n",
      "val_f_rmse: 0.013895\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2715, Val Loss: 0.2163\n",
      "train_e/atom_mae: 0.004735\n",
      "train_e/atom_rmse: 0.007114\n",
      "train_f_mae: 0.006650\n",
      "train_f_rmse: 0.015074\n",
      "val_e/atom_mae: 0.004337\n",
      "val_e/atom_rmse: 0.006879\n",
      "val_f_mae: 0.006021\n",
      "val_f_rmse: 0.013874\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2742, Val Loss: 0.2162\n",
      "train_e/atom_mae: 0.004781\n",
      "train_e/atom_rmse: 0.007190\n",
      "train_f_mae: 0.006621\n",
      "train_f_rmse: 0.015072\n",
      "val_e/atom_mae: 0.004436\n",
      "val_e/atom_rmse: 0.006949\n",
      "val_f_mae: 0.005967\n",
      "val_f_rmse: 0.013835\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2701, Val Loss: 0.2161\n",
      "train_e/atom_mae: 0.004712\n",
      "train_e/atom_rmse: 0.007124\n",
      "train_f_mae: 0.006552\n",
      "train_f_rmse: 0.015003\n",
      "val_e/atom_mae: 0.004396\n",
      "val_e/atom_rmse: 0.006961\n",
      "val_f_mae: 0.005977\n",
      "val_f_rmse: 0.013825\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2709, Val Loss: 0.2158\n",
      "train_e/atom_mae: 0.004730\n",
      "train_e/atom_rmse: 0.007145\n",
      "train_f_mae: 0.006560\n",
      "train_f_rmse: 0.014979\n",
      "val_e/atom_mae: 0.004379\n",
      "val_e/atom_rmse: 0.006974\n",
      "val_f_mae: 0.005974\n",
      "val_f_rmse: 0.013812\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2683, Val Loss: 0.2156\n",
      "train_e/atom_mae: 0.004671\n",
      "train_e/atom_rmse: 0.007057\n",
      "train_f_mae: 0.006491\n",
      "train_f_rmse: 0.014943\n",
      "val_e/atom_mae: 0.004393\n",
      "val_e/atom_rmse: 0.006883\n",
      "val_f_mae: 0.005988\n",
      "val_f_rmse: 0.013846\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2648, Val Loss: 0.2159\n",
      "train_e/atom_mae: 0.004675\n",
      "train_e/atom_rmse: 0.007080\n",
      "train_f_mae: 0.006514\n",
      "train_f_rmse: 0.014945\n",
      "val_e/atom_mae: 0.004379\n",
      "val_e/atom_rmse: 0.006926\n",
      "val_f_mae: 0.005999\n",
      "val_f_rmse: 0.013839\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2659, Val Loss: 0.2155\n",
      "train_e/atom_mae: 0.004673\n",
      "train_e/atom_rmse: 0.007068\n",
      "train_f_mae: 0.006477\n",
      "train_f_rmse: 0.014927\n",
      "val_e/atom_mae: 0.004442\n",
      "val_e/atom_rmse: 0.006965\n",
      "val_f_mae: 0.005954\n",
      "val_f_rmse: 0.013807\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2678, Val Loss: 0.2155\n",
      "train_e/atom_mae: 0.004648\n",
      "train_e/atom_rmse: 0.007028\n",
      "train_f_mae: 0.006484\n",
      "train_f_rmse: 0.014937\n",
      "val_e/atom_mae: 0.004385\n",
      "val_e/atom_rmse: 0.006909\n",
      "val_f_mae: 0.005975\n",
      "val_f_rmse: 0.013828\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2640, Val Loss: 0.2153\n",
      "train_e/atom_mae: 0.004654\n",
      "train_e/atom_rmse: 0.007041\n",
      "train_f_mae: 0.006398\n",
      "train_f_rmse: 0.014864\n",
      "val_e/atom_mae: 0.004389\n",
      "val_e/atom_rmse: 0.006919\n",
      "val_f_mae: 0.005985\n",
      "val_f_rmse: 0.013817\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2654, Val Loss: 0.2151\n",
      "train_e/atom_mae: 0.004627\n",
      "train_e/atom_rmse: 0.007020\n",
      "train_f_mae: 0.006410\n",
      "train_f_rmse: 0.014869\n",
      "val_e/atom_mae: 0.004439\n",
      "val_e/atom_rmse: 0.006994\n",
      "val_f_mae: 0.005951\n",
      "val_f_rmse: 0.013777\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2648, Val Loss: 0.2150\n",
      "train_e/atom_mae: 0.004611\n",
      "train_e/atom_rmse: 0.007003\n",
      "train_f_mae: 0.006342\n",
      "train_f_rmse: 0.014820\n",
      "val_e/atom_mae: 0.004439\n",
      "val_e/atom_rmse: 0.006995\n",
      "val_f_mae: 0.005944\n",
      "val_f_rmse: 0.013774\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2646, Val Loss: 0.2149\n",
      "train_e/atom_mae: 0.004605\n",
      "train_e/atom_rmse: 0.006991\n",
      "train_f_mae: 0.006368\n",
      "train_f_rmse: 0.014843\n",
      "val_e/atom_mae: 0.004397\n",
      "val_e/atom_rmse: 0.006915\n",
      "val_f_mae: 0.005961\n",
      "val_f_rmse: 0.013801\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2620, Val Loss: 0.2150\n",
      "train_e/atom_mae: 0.004565\n",
      "train_e/atom_rmse: 0.006974\n",
      "train_f_mae: 0.006323\n",
      "train_f_rmse: 0.014793\n",
      "val_e/atom_mae: 0.004392\n",
      "val_e/atom_rmse: 0.006915\n",
      "val_f_mae: 0.005965\n",
      "val_f_rmse: 0.013808\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2633, Val Loss: 0.2144\n",
      "train_e/atom_mae: 0.004594\n",
      "train_e/atom_rmse: 0.006986\n",
      "train_f_mae: 0.006312\n",
      "train_f_rmse: 0.014805\n",
      "val_e/atom_mae: 0.004390\n",
      "val_e/atom_rmse: 0.006909\n",
      "val_f_mae: 0.005955\n",
      "val_f_rmse: 0.013787\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2625, Val Loss: 0.2149\n",
      "train_e/atom_mae: 0.004581\n",
      "train_e/atom_rmse: 0.006968\n",
      "train_f_mae: 0.006278\n",
      "train_f_rmse: 0.014758\n",
      "val_e/atom_mae: 0.004454\n",
      "val_e/atom_rmse: 0.006983\n",
      "val_f_mae: 0.005942\n",
      "val_f_rmse: 0.013772\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2644, Val Loss: 0.2146\n",
      "train_e/atom_mae: 0.004574\n",
      "train_e/atom_rmse: 0.006960\n",
      "train_f_mae: 0.006286\n",
      "train_f_rmse: 0.014771\n",
      "val_e/atom_mae: 0.004398\n",
      "val_e/atom_rmse: 0.006899\n",
      "val_f_mae: 0.005947\n",
      "val_f_rmse: 0.013799\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2612, Val Loss: 0.2146\n",
      "train_e/atom_mae: 0.004554\n",
      "train_e/atom_rmse: 0.006950\n",
      "train_f_mae: 0.006248\n",
      "train_f_rmse: 0.014742\n",
      "val_e/atom_mae: 0.004390\n",
      "val_e/atom_rmse: 0.006892\n",
      "val_f_mae: 0.005968\n",
      "val_f_rmse: 0.013803\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2608, Val Loss: 0.2148\n",
      "train_e/atom_mae: 0.004575\n",
      "train_e/atom_rmse: 0.006950\n",
      "train_f_mae: 0.006256\n",
      "train_f_rmse: 0.014743\n",
      "val_e/atom_mae: 0.004402\n",
      "val_e/atom_rmse: 0.006927\n",
      "val_f_mae: 0.005946\n",
      "val_f_rmse: 0.013795\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2597, Val Loss: 0.2145\n",
      "train_e/atom_mae: 0.004547\n",
      "train_e/atom_rmse: 0.006945\n",
      "train_f_mae: 0.006261\n",
      "train_f_rmse: 0.014744\n",
      "val_e/atom_mae: 0.004404\n",
      "val_e/atom_rmse: 0.006905\n",
      "val_f_mae: 0.005956\n",
      "val_f_rmse: 0.013796\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2606, Val Loss: 0.2145\n",
      "train_e/atom_mae: 0.004546\n",
      "train_e/atom_rmse: 0.006956\n",
      "train_f_mae: 0.006232\n",
      "train_f_rmse: 0.014733\n",
      "val_e/atom_mae: 0.004417\n",
      "val_e/atom_rmse: 0.006964\n",
      "val_f_mae: 0.005924\n",
      "val_f_rmse: 0.013766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2619, Val Loss: 0.2144\n",
      "train_e/atom_mae: 0.004534\n",
      "train_e/atom_rmse: 0.006930\n",
      "train_f_mae: 0.006239\n",
      "train_f_rmse: 0.014726\n",
      "val_e/atom_mae: 0.004400\n",
      "val_e/atom_rmse: 0.006907\n",
      "val_f_mae: 0.005955\n",
      "val_f_rmse: 0.013788\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2614, Val Loss: 0.2145\n",
      "train_e/atom_mae: 0.004536\n",
      "train_e/atom_rmse: 0.006933\n",
      "train_f_mae: 0.006221\n",
      "train_f_rmse: 0.014700\n",
      "val_e/atom_mae: 0.004426\n",
      "val_e/atom_rmse: 0.006952\n",
      "val_f_mae: 0.005947\n",
      "val_f_rmse: 0.013773\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2593, Val Loss: 0.2141\n",
      "train_e/atom_mae: 0.004549\n",
      "train_e/atom_rmse: 0.006952\n",
      "train_f_mae: 0.006204\n",
      "train_f_rmse: 0.014701\n",
      "val_e/atom_mae: 0.004428\n",
      "val_e/atom_rmse: 0.006969\n",
      "val_f_mae: 0.005927\n",
      "val_f_rmse: 0.013748\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2588, Val Loss: 0.2142\n",
      "train_e/atom_mae: 0.004533\n",
      "train_e/atom_rmse: 0.006927\n",
      "train_f_mae: 0.006177\n",
      "train_f_rmse: 0.014686\n",
      "val_e/atom_mae: 0.004466\n",
      "val_e/atom_rmse: 0.007007\n",
      "val_f_mae: 0.005919\n",
      "val_f_rmse: 0.013734\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2583, Val Loss: 0.2144\n",
      "train_e/atom_mae: 0.004504\n",
      "train_e/atom_rmse: 0.006913\n",
      "train_f_mae: 0.006172\n",
      "train_f_rmse: 0.014675\n",
      "val_e/atom_mae: 0.004399\n",
      "val_e/atom_rmse: 0.006920\n",
      "val_f_mae: 0.005948\n",
      "val_f_rmse: 0.013782\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2584, Val Loss: 0.2144\n",
      "train_e/atom_mae: 0.004521\n",
      "train_e/atom_rmse: 0.006921\n",
      "train_f_mae: 0.006177\n",
      "train_f_rmse: 0.014671\n",
      "val_e/atom_mae: 0.004446\n",
      "val_e/atom_rmse: 0.006998\n",
      "val_f_mae: 0.005931\n",
      "val_f_rmse: 0.013746\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2570, Val Loss: 0.2141\n",
      "train_e/atom_mae: 0.004513\n",
      "train_e/atom_rmse: 0.006929\n",
      "train_f_mae: 0.006165\n",
      "train_f_rmse: 0.014664\n",
      "val_e/atom_mae: 0.004406\n",
      "val_e/atom_rmse: 0.006920\n",
      "val_f_mae: 0.005930\n",
      "val_f_rmse: 0.013769\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2576, Val Loss: 0.2143\n",
      "train_e/atom_mae: 0.004486\n",
      "train_e/atom_rmse: 0.006894\n",
      "train_f_mae: 0.006158\n",
      "train_f_rmse: 0.014656\n",
      "val_e/atom_mae: 0.004370\n",
      "val_e/atom_rmse: 0.006897\n",
      "val_f_mae: 0.005950\n",
      "val_f_rmse: 0.013788\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2572, Val Loss: 0.2145\n",
      "train_e/atom_mae: 0.004494\n",
      "train_e/atom_rmse: 0.006906\n",
      "train_f_mae: 0.006154\n",
      "train_f_rmse: 0.014653\n",
      "val_e/atom_mae: 0.004368\n",
      "val_e/atom_rmse: 0.006872\n",
      "val_f_mae: 0.005963\n",
      "val_f_rmse: 0.013806\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2571, Val Loss: 0.2140\n",
      "train_e/atom_mae: 0.004498\n",
      "train_e/atom_rmse: 0.006900\n",
      "train_f_mae: 0.006147\n",
      "train_f_rmse: 0.014646\n",
      "val_e/atom_mae: 0.004388\n",
      "val_e/atom_rmse: 0.006902\n",
      "val_f_mae: 0.005941\n",
      "val_f_rmse: 0.013775\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2562, Val Loss: 0.2136\n",
      "train_e/atom_mae: 0.004485\n",
      "train_e/atom_rmse: 0.006891\n",
      "train_f_mae: 0.006124\n",
      "train_f_rmse: 0.014630\n",
      "val_e/atom_mae: 0.004398\n",
      "val_e/atom_rmse: 0.006930\n",
      "val_f_mae: 0.005926\n",
      "val_f_rmse: 0.013746\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2566, Val Loss: 0.2138\n",
      "train_e/atom_mae: 0.004472\n",
      "train_e/atom_rmse: 0.006880\n",
      "train_f_mae: 0.006115\n",
      "train_f_rmse: 0.014630\n",
      "val_e/atom_mae: 0.004432\n",
      "val_e/atom_rmse: 0.006982\n",
      "val_f_mae: 0.005920\n",
      "val_f_rmse: 0.013731\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.2824, Val Loss: 0.2401\n",
      "train_e/atom_mae: 0.004711\n",
      "train_e/atom_rmse: 0.007108\n",
      "train_f_mae: 0.006570\n",
      "train_f_rmse: 0.014949\n",
      "val_e/atom_mae: 0.005496\n",
      "val_e/atom_rmse: 0.007319\n",
      "val_f_mae: 0.006796\n",
      "val_f_rmse: 0.014390\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.2802, Val Loss: 0.2142\n",
      "train_e/atom_mae: 0.004956\n",
      "train_e/atom_rmse: 0.007353\n",
      "train_f_mae: 0.006977\n",
      "train_f_rmse: 0.015245\n",
      "val_e/atom_mae: 0.004370\n",
      "val_e/atom_rmse: 0.006884\n",
      "val_f_mae: 0.005949\n",
      "val_f_rmse: 0.013789\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2728, Val Loss: 0.2135\n",
      "train_e/atom_mae: 0.004863\n",
      "train_e/atom_rmse: 0.007283\n",
      "train_f_mae: 0.006862\n",
      "train_f_rmse: 0.015160\n",
      "val_e/atom_mae: 0.004441\n",
      "val_e/atom_rmse: 0.007025\n",
      "val_f_mae: 0.005901\n",
      "val_f_rmse: 0.013699\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2760, Val Loss: 0.2142\n",
      "train_e/atom_mae: 0.004879\n",
      "train_e/atom_rmse: 0.007283\n",
      "train_f_mae: 0.006750\n",
      "train_f_rmse: 0.015066\n",
      "val_e/atom_mae: 0.004392\n",
      "val_e/atom_rmse: 0.006949\n",
      "val_f_mae: 0.005937\n",
      "val_f_rmse: 0.013757\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2704, Val Loss: 0.2136\n",
      "train_e/atom_mae: 0.004813\n",
      "train_e/atom_rmse: 0.007219\n",
      "train_f_mae: 0.006731\n",
      "train_f_rmse: 0.015023\n",
      "val_e/atom_mae: 0.004413\n",
      "val_e/atom_rmse: 0.006925\n",
      "val_f_mae: 0.005926\n",
      "val_f_rmse: 0.013745\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2730, Val Loss: 0.2138\n",
      "train_e/atom_mae: 0.004814\n",
      "train_e/atom_rmse: 0.007220\n",
      "train_f_mae: 0.006743\n",
      "train_f_rmse: 0.015047\n",
      "val_e/atom_mae: 0.004414\n",
      "val_e/atom_rmse: 0.006965\n",
      "val_f_mae: 0.005899\n",
      "val_f_rmse: 0.013738\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2683, Val Loss: 0.2130\n",
      "train_e/atom_mae: 0.004769\n",
      "train_e/atom_rmse: 0.007142\n",
      "train_f_mae: 0.006587\n",
      "train_f_rmse: 0.014951\n",
      "val_e/atom_mae: 0.004412\n",
      "val_e/atom_rmse: 0.006958\n",
      "val_f_mae: 0.005902\n",
      "val_f_rmse: 0.013710\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2719, Val Loss: 0.2134\n",
      "train_e/atom_mae: 0.004804\n",
      "train_e/atom_rmse: 0.007232\n",
      "train_f_mae: 0.006606\n",
      "train_f_rmse: 0.014928\n",
      "val_e/atom_mae: 0.004407\n",
      "val_e/atom_rmse: 0.006965\n",
      "val_f_mae: 0.005925\n",
      "val_f_rmse: 0.013720\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2639, Val Loss: 0.2129\n",
      "train_e/atom_mae: 0.004749\n",
      "train_e/atom_rmse: 0.007141\n",
      "train_f_mae: 0.006537\n",
      "train_f_rmse: 0.014898\n",
      "val_e/atom_mae: 0.004409\n",
      "val_e/atom_rmse: 0.006917\n",
      "val_f_mae: 0.005891\n",
      "val_f_rmse: 0.013720\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2736, Val Loss: 0.2132\n",
      "train_e/atom_mae: 0.004752\n",
      "train_e/atom_rmse: 0.007177\n",
      "train_f_mae: 0.006481\n",
      "train_f_rmse: 0.014864\n",
      "val_e/atom_mae: 0.004451\n",
      "val_e/atom_rmse: 0.006994\n",
      "val_f_mae: 0.005891\n",
      "val_f_rmse: 0.013698\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2639, Val Loss: 0.2128\n",
      "train_e/atom_mae: 0.004687\n",
      "train_e/atom_rmse: 0.007086\n",
      "train_f_mae: 0.006437\n",
      "train_f_rmse: 0.014838\n",
      "val_e/atom_mae: 0.004405\n",
      "val_e/atom_rmse: 0.006925\n",
      "val_f_mae: 0.005912\n",
      "val_f_rmse: 0.013717\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2621, Val Loss: 0.2124\n",
      "train_e/atom_mae: 0.004704\n",
      "train_e/atom_rmse: 0.007080\n",
      "train_f_mae: 0.006427\n",
      "train_f_rmse: 0.014791\n",
      "val_e/atom_mae: 0.004459\n",
      "val_e/atom_rmse: 0.007003\n",
      "val_f_mae: 0.005877\n",
      "val_f_rmse: 0.013662\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2646, Val Loss: 0.2128\n",
      "train_e/atom_mae: 0.004710\n",
      "train_e/atom_rmse: 0.007071\n",
      "train_f_mae: 0.006391\n",
      "train_f_rmse: 0.014769\n",
      "val_e/atom_mae: 0.004404\n",
      "val_e/atom_rmse: 0.006966\n",
      "val_f_mae: 0.005899\n",
      "val_f_rmse: 0.013697\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2605, Val Loss: 0.2124\n",
      "train_e/atom_mae: 0.004673\n",
      "train_e/atom_rmse: 0.007075\n",
      "train_f_mae: 0.006381\n",
      "train_f_rmse: 0.014749\n",
      "val_e/atom_mae: 0.004406\n",
      "val_e/atom_rmse: 0.006941\n",
      "val_f_mae: 0.005895\n",
      "val_f_rmse: 0.013692\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2598, Val Loss: 0.2126\n",
      "train_e/atom_mae: 0.004630\n",
      "train_e/atom_rmse: 0.007038\n",
      "train_f_mae: 0.006356\n",
      "train_f_rmse: 0.014746\n",
      "val_e/atom_mae: 0.004358\n",
      "val_e/atom_rmse: 0.006851\n",
      "val_f_mae: 0.005902\n",
      "val_f_rmse: 0.013741\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2615, Val Loss: 0.2123\n",
      "train_e/atom_mae: 0.004678\n",
      "train_e/atom_rmse: 0.007057\n",
      "train_f_mae: 0.006326\n",
      "train_f_rmse: 0.014714\n",
      "val_e/atom_mae: 0.004416\n",
      "val_e/atom_rmse: 0.007010\n",
      "val_f_mae: 0.005888\n",
      "val_f_rmse: 0.013654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2628, Val Loss: 0.2123\n",
      "train_e/atom_mae: 0.004643\n",
      "train_e/atom_rmse: 0.007045\n",
      "train_f_mae: 0.006300\n",
      "train_f_rmse: 0.014691\n",
      "val_e/atom_mae: 0.004419\n",
      "val_e/atom_rmse: 0.006924\n",
      "val_f_mae: 0.005889\n",
      "val_f_rmse: 0.013697\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2607, Val Loss: 0.2119\n",
      "train_e/atom_mae: 0.004611\n",
      "train_e/atom_rmse: 0.007035\n",
      "train_f_mae: 0.006312\n",
      "train_f_rmse: 0.014697\n",
      "val_e/atom_mae: 0.004450\n",
      "val_e/atom_rmse: 0.007009\n",
      "val_f_mae: 0.005859\n",
      "val_f_rmse: 0.013638\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2574, Val Loss: 0.2122\n",
      "train_e/atom_mae: 0.004590\n",
      "train_e/atom_rmse: 0.006996\n",
      "train_f_mae: 0.006261\n",
      "train_f_rmse: 0.014666\n",
      "val_e/atom_mae: 0.004390\n",
      "val_e/atom_rmse: 0.006928\n",
      "val_f_mae: 0.005881\n",
      "val_f_rmse: 0.013689\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2582, Val Loss: 0.2117\n",
      "train_e/atom_mae: 0.004596\n",
      "train_e/atom_rmse: 0.006995\n",
      "train_f_mae: 0.006242\n",
      "train_f_rmse: 0.014641\n",
      "val_e/atom_mae: 0.004435\n",
      "val_e/atom_rmse: 0.006976\n",
      "val_f_mae: 0.005870\n",
      "val_f_rmse: 0.013648\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2591, Val Loss: 0.2118\n",
      "train_e/atom_mae: 0.004578\n",
      "train_e/atom_rmse: 0.006997\n",
      "train_f_mae: 0.006227\n",
      "train_f_rmse: 0.014639\n",
      "val_e/atom_mae: 0.004391\n",
      "val_e/atom_rmse: 0.006933\n",
      "val_f_mae: 0.005879\n",
      "val_f_rmse: 0.013671\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2573, Val Loss: 0.2118\n",
      "train_e/atom_mae: 0.004573\n",
      "train_e/atom_rmse: 0.006976\n",
      "train_f_mae: 0.006235\n",
      "train_f_rmse: 0.014642\n",
      "val_e/atom_mae: 0.004404\n",
      "val_e/atom_rmse: 0.006934\n",
      "val_f_mae: 0.005885\n",
      "val_f_rmse: 0.013671\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2563, Val Loss: 0.2118\n",
      "train_e/atom_mae: 0.004556\n",
      "train_e/atom_rmse: 0.006979\n",
      "train_f_mae: 0.006215\n",
      "train_f_rmse: 0.014610\n",
      "val_e/atom_mae: 0.004421\n",
      "val_e/atom_rmse: 0.006973\n",
      "val_f_mae: 0.005891\n",
      "val_f_rmse: 0.013652\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2584, Val Loss: 0.2118\n",
      "train_e/atom_mae: 0.004566\n",
      "train_e/atom_rmse: 0.006984\n",
      "train_f_mae: 0.006205\n",
      "train_f_rmse: 0.014594\n",
      "val_e/atom_mae: 0.004380\n",
      "val_e/atom_rmse: 0.006879\n",
      "val_f_mae: 0.005893\n",
      "val_f_rmse: 0.013698\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2565, Val Loss: 0.2118\n",
      "train_e/atom_mae: 0.004559\n",
      "train_e/atom_rmse: 0.006963\n",
      "train_f_mae: 0.006177\n",
      "train_f_rmse: 0.014580\n",
      "val_e/atom_mae: 0.004387\n",
      "val_e/atom_rmse: 0.006917\n",
      "val_f_mae: 0.005888\n",
      "val_f_rmse: 0.013677\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2575, Val Loss: 0.2116\n",
      "train_e/atom_mae: 0.004552\n",
      "train_e/atom_rmse: 0.006954\n",
      "train_f_mae: 0.006165\n",
      "train_f_rmse: 0.014574\n",
      "val_e/atom_mae: 0.004430\n",
      "val_e/atom_rmse: 0.006979\n",
      "val_f_mae: 0.005877\n",
      "val_f_rmse: 0.013640\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2558, Val Loss: 0.2116\n",
      "train_e/atom_mae: 0.004536\n",
      "train_e/atom_rmse: 0.006958\n",
      "train_f_mae: 0.006151\n",
      "train_f_rmse: 0.014557\n",
      "val_e/atom_mae: 0.004354\n",
      "val_e/atom_rmse: 0.006876\n",
      "val_f_mae: 0.005889\n",
      "val_f_rmse: 0.013689\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2549, Val Loss: 0.2117\n",
      "train_e/atom_mae: 0.004545\n",
      "train_e/atom_rmse: 0.006950\n",
      "train_f_mae: 0.006130\n",
      "train_f_rmse: 0.014552\n",
      "val_e/atom_mae: 0.004367\n",
      "val_e/atom_rmse: 0.006904\n",
      "val_f_mae: 0.005883\n",
      "val_f_rmse: 0.013682\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2558, Val Loss: 0.2119\n",
      "train_e/atom_mae: 0.004543\n",
      "train_e/atom_rmse: 0.006926\n",
      "train_f_mae: 0.006140\n",
      "train_f_rmse: 0.014534\n",
      "val_e/atom_mae: 0.004378\n",
      "val_e/atom_rmse: 0.006889\n",
      "val_f_mae: 0.005901\n",
      "val_f_rmse: 0.013693\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2541, Val Loss: 0.2117\n",
      "train_e/atom_mae: 0.004537\n",
      "train_e/atom_rmse: 0.006937\n",
      "train_f_mae: 0.006131\n",
      "train_f_rmse: 0.014537\n",
      "val_e/atom_mae: 0.004364\n",
      "val_e/atom_rmse: 0.006885\n",
      "val_f_mae: 0.005885\n",
      "val_f_rmse: 0.013690\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2557, Val Loss: 0.2112\n",
      "train_e/atom_mae: 0.004556\n",
      "train_e/atom_rmse: 0.006977\n",
      "train_f_mae: 0.006124\n",
      "train_f_rmse: 0.014532\n",
      "val_e/atom_mae: 0.004425\n",
      "val_e/atom_rmse: 0.006950\n",
      "val_f_mae: 0.005884\n",
      "val_f_rmse: 0.013641\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2539, Val Loss: 0.2111\n",
      "train_e/atom_mae: 0.004522\n",
      "train_e/atom_rmse: 0.006937\n",
      "train_f_mae: 0.006108\n",
      "train_f_rmse: 0.014512\n",
      "val_e/atom_mae: 0.004405\n",
      "val_e/atom_rmse: 0.006954\n",
      "val_f_mae: 0.005874\n",
      "val_f_rmse: 0.013635\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2551, Val Loss: 0.2112\n",
      "train_e/atom_mae: 0.004526\n",
      "train_e/atom_rmse: 0.006941\n",
      "train_f_mae: 0.006092\n",
      "train_f_rmse: 0.014509\n",
      "val_e/atom_mae: 0.004388\n",
      "val_e/atom_rmse: 0.006942\n",
      "val_f_mae: 0.005871\n",
      "val_f_rmse: 0.013643\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2538, Val Loss: 0.2112\n",
      "train_e/atom_mae: 0.004524\n",
      "train_e/atom_rmse: 0.006929\n",
      "train_f_mae: 0.006106\n",
      "train_f_rmse: 0.014513\n",
      "val_e/atom_mae: 0.004412\n",
      "val_e/atom_rmse: 0.006958\n",
      "val_f_mae: 0.005872\n",
      "val_f_rmse: 0.013636\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2526, Val Loss: 0.2111\n",
      "train_e/atom_mae: 0.004504\n",
      "train_e/atom_rmse: 0.006921\n",
      "train_f_mae: 0.006067\n",
      "train_f_rmse: 0.014481\n",
      "val_e/atom_mae: 0.004417\n",
      "val_e/atom_rmse: 0.006960\n",
      "val_f_mae: 0.005854\n",
      "val_f_rmse: 0.013630\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2528, Val Loss: 0.2111\n",
      "train_e/atom_mae: 0.004495\n",
      "train_e/atom_rmse: 0.006894\n",
      "train_f_mae: 0.006084\n",
      "train_f_rmse: 0.014490\n",
      "val_e/atom_mae: 0.004378\n",
      "val_e/atom_rmse: 0.006922\n",
      "val_f_mae: 0.005872\n",
      "val_f_rmse: 0.013650\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2522, Val Loss: 0.2109\n",
      "train_e/atom_mae: 0.004510\n",
      "train_e/atom_rmse: 0.006920\n",
      "train_f_mae: 0.006069\n",
      "train_f_rmse: 0.014472\n",
      "val_e/atom_mae: 0.004424\n",
      "val_e/atom_rmse: 0.006962\n",
      "val_f_mae: 0.005845\n",
      "val_f_rmse: 0.013621\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2544, Val Loss: 0.2111\n",
      "train_e/atom_mae: 0.004503\n",
      "train_e/atom_rmse: 0.006931\n",
      "train_f_mae: 0.006064\n",
      "train_f_rmse: 0.014473\n",
      "val_e/atom_mae: 0.004439\n",
      "val_e/atom_rmse: 0.006998\n",
      "val_f_mae: 0.005850\n",
      "val_f_rmse: 0.013613\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2523, Val Loss: 0.2111\n",
      "train_e/atom_mae: 0.004489\n",
      "train_e/atom_rmse: 0.006910\n",
      "train_f_mae: 0.006062\n",
      "train_f_rmse: 0.014468\n",
      "val_e/atom_mae: 0.004402\n",
      "val_e/atom_rmse: 0.006946\n",
      "val_f_mae: 0.005865\n",
      "val_f_rmse: 0.013637\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2524, Val Loss: 0.2110\n",
      "train_e/atom_mae: 0.004475\n",
      "train_e/atom_rmse: 0.006894\n",
      "train_f_mae: 0.006053\n",
      "train_f_rmse: 0.014466\n",
      "val_e/atom_mae: 0.004377\n",
      "val_e/atom_rmse: 0.006936\n",
      "val_f_mae: 0.005861\n",
      "val_f_rmse: 0.013638\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 0.2746, Val Loss: 0.2161\n",
      "train_e/atom_mae: 0.004701\n",
      "train_e/atom_rmse: 0.007075\n",
      "train_f_mae: 0.006419\n",
      "train_f_rmse: 0.014724\n",
      "val_e/atom_mae: 0.004587\n",
      "val_e/atom_rmse: 0.007100\n",
      "val_f_mae: 0.006179\n",
      "val_f_rmse: 0.013684\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 0.2707, Val Loss: 0.2105\n",
      "train_e/atom_mae: 0.004907\n",
      "train_e/atom_rmse: 0.007351\n",
      "train_f_mae: 0.006916\n",
      "train_f_rmse: 0.015103\n",
      "val_e/atom_mae: 0.004439\n",
      "val_e/atom_rmse: 0.006978\n",
      "val_f_mae: 0.005836\n",
      "val_f_rmse: 0.013599\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 0.2762, Val Loss: 0.2102\n",
      "train_e/atom_mae: 0.004888\n",
      "train_e/atom_rmse: 0.007282\n",
      "train_f_mae: 0.006687\n",
      "train_f_rmse: 0.014914\n",
      "val_e/atom_mae: 0.004410\n",
      "val_e/atom_rmse: 0.006967\n",
      "val_f_mae: 0.005833\n",
      "val_f_rmse: 0.013593\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 0.2718, Val Loss: 0.2102\n",
      "train_e/atom_mae: 0.004836\n",
      "train_e/atom_rmse: 0.007249\n",
      "train_f_mae: 0.006676\n",
      "train_f_rmse: 0.014882\n",
      "val_e/atom_mae: 0.004438\n",
      "val_e/atom_rmse: 0.006977\n",
      "val_f_mae: 0.005818\n",
      "val_f_rmse: 0.013583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 0.2652, Val Loss: 0.2103\n",
      "train_e/atom_mae: 0.004776\n",
      "train_e/atom_rmse: 0.007188\n",
      "train_f_mae: 0.006605\n",
      "train_f_rmse: 0.014820\n",
      "val_e/atom_mae: 0.004442\n",
      "val_e/atom_rmse: 0.007008\n",
      "val_f_mae: 0.005832\n",
      "val_f_rmse: 0.013575\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 0.2673, Val Loss: 0.2099\n",
      "train_e/atom_mae: 0.004809\n",
      "train_e/atom_rmse: 0.007181\n",
      "train_f_mae: 0.006583\n",
      "train_f_rmse: 0.014802\n",
      "val_e/atom_mae: 0.004402\n",
      "val_e/atom_rmse: 0.006915\n",
      "val_f_mae: 0.005833\n",
      "val_f_rmse: 0.013602\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 0.2676, Val Loss: 0.2103\n",
      "train_e/atom_mae: 0.004785\n",
      "train_e/atom_rmse: 0.007167\n",
      "train_f_mae: 0.006516\n",
      "train_f_rmse: 0.014762\n",
      "val_e/atom_mae: 0.004430\n",
      "val_e/atom_rmse: 0.006957\n",
      "val_f_mae: 0.005860\n",
      "val_f_rmse: 0.013603\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 0.2627, Val Loss: 0.2099\n",
      "train_e/atom_mae: 0.004744\n",
      "train_e/atom_rmse: 0.007175\n",
      "train_f_mae: 0.006496\n",
      "train_f_rmse: 0.014758\n",
      "val_e/atom_mae: 0.004440\n",
      "val_e/atom_rmse: 0.007019\n",
      "val_f_mae: 0.005832\n",
      "val_f_rmse: 0.013553\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 0.2642, Val Loss: 0.2100\n",
      "train_e/atom_mae: 0.004744\n",
      "train_e/atom_rmse: 0.007141\n",
      "train_f_mae: 0.006434\n",
      "train_f_rmse: 0.014697\n",
      "val_e/atom_mae: 0.004394\n",
      "val_e/atom_rmse: 0.006915\n",
      "val_f_mae: 0.005838\n",
      "val_f_rmse: 0.013601\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 0.2584, Val Loss: 0.2096\n",
      "train_e/atom_mae: 0.004726\n",
      "train_e/atom_rmse: 0.007101\n",
      "train_f_mae: 0.006390\n",
      "train_f_rmse: 0.014682\n",
      "val_e/atom_mae: 0.004384\n",
      "val_e/atom_rmse: 0.006944\n",
      "val_f_mae: 0.005818\n",
      "val_f_rmse: 0.013575\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 0.2583, Val Loss: 0.2092\n",
      "train_e/atom_mae: 0.004701\n",
      "train_e/atom_rmse: 0.007100\n",
      "train_f_mae: 0.006359\n",
      "train_f_rmse: 0.014627\n",
      "val_e/atom_mae: 0.004402\n",
      "val_e/atom_rmse: 0.006916\n",
      "val_f_mae: 0.005833\n",
      "val_f_rmse: 0.013575\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 0.2576, Val Loss: 0.2092\n",
      "train_e/atom_mae: 0.004668\n",
      "train_e/atom_rmse: 0.007076\n",
      "train_f_mae: 0.006310\n",
      "train_f_rmse: 0.014599\n",
      "val_e/atom_mae: 0.004437\n",
      "val_e/atom_rmse: 0.006999\n",
      "val_f_mae: 0.005800\n",
      "val_f_rmse: 0.013537\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 0.2572, Val Loss: 0.2084\n",
      "train_e/atom_mae: 0.004659\n",
      "train_e/atom_rmse: 0.007050\n",
      "train_f_mae: 0.006259\n",
      "train_f_rmse: 0.014538\n",
      "val_e/atom_mae: 0.004453\n",
      "val_e/atom_rmse: 0.007033\n",
      "val_f_mae: 0.005779\n",
      "val_f_rmse: 0.013489\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 0.2552, Val Loss: 0.2088\n",
      "train_e/atom_mae: 0.004687\n",
      "train_e/atom_rmse: 0.007109\n",
      "train_f_mae: 0.006286\n",
      "train_f_rmse: 0.014550\n",
      "val_e/atom_mae: 0.004427\n",
      "val_e/atom_rmse: 0.006981\n",
      "val_f_mae: 0.005788\n",
      "val_f_rmse: 0.013531\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 0.2587, Val Loss: 0.2085\n",
      "train_e/atom_mae: 0.004668\n",
      "train_e/atom_rmse: 0.007065\n",
      "train_f_mae: 0.006262\n",
      "train_f_rmse: 0.014531\n",
      "val_e/atom_mae: 0.004423\n",
      "val_e/atom_rmse: 0.006977\n",
      "val_f_mae: 0.005783\n",
      "val_f_rmse: 0.013518\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 0.2559, Val Loss: 0.2087\n",
      "train_e/atom_mae: 0.004675\n",
      "train_e/atom_rmse: 0.007062\n",
      "train_f_mae: 0.006288\n",
      "train_f_rmse: 0.014549\n",
      "val_e/atom_mae: 0.004457\n",
      "val_e/atom_rmse: 0.006997\n",
      "val_f_mae: 0.005797\n",
      "val_f_rmse: 0.013517\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.2560, Val Loss: 0.2090\n",
      "train_e/atom_mae: 0.004619\n",
      "train_e/atom_rmse: 0.007014\n",
      "train_f_mae: 0.006235\n",
      "train_f_rmse: 0.014524\n",
      "val_e/atom_mae: 0.004397\n",
      "val_e/atom_rmse: 0.006936\n",
      "val_f_mae: 0.005824\n",
      "val_f_rmse: 0.013559\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 0.2542, Val Loss: 0.2087\n",
      "train_e/atom_mae: 0.004618\n",
      "train_e/atom_rmse: 0.007017\n",
      "train_f_mae: 0.006207\n",
      "train_f_rmse: 0.014486\n",
      "val_e/atom_mae: 0.004376\n",
      "val_e/atom_rmse: 0.006911\n",
      "val_f_mae: 0.005806\n",
      "val_f_rmse: 0.013555\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.2544, Val Loss: 0.2080\n",
      "train_e/atom_mae: 0.004614\n",
      "train_e/atom_rmse: 0.007036\n",
      "train_f_mae: 0.006182\n",
      "train_f_rmse: 0.014454\n",
      "val_e/atom_mae: 0.004378\n",
      "val_e/atom_rmse: 0.006928\n",
      "val_f_mae: 0.005790\n",
      "val_f_rmse: 0.013519\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 0.2519, Val Loss: 0.2083\n",
      "train_e/atom_mae: 0.004599\n",
      "train_e/atom_rmse: 0.007012\n",
      "train_f_mae: 0.006168\n",
      "train_f_rmse: 0.014460\n",
      "val_e/atom_mae: 0.004393\n",
      "val_e/atom_rmse: 0.006924\n",
      "val_f_mae: 0.005802\n",
      "val_f_rmse: 0.013535\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 0.2522, Val Loss: 0.2080\n",
      "train_e/atom_mae: 0.004600\n",
      "train_e/atom_rmse: 0.007020\n",
      "train_f_mae: 0.006141\n",
      "train_f_rmse: 0.014425\n",
      "val_e/atom_mae: 0.004418\n",
      "val_e/atom_rmse: 0.006971\n",
      "val_f_mae: 0.005794\n",
      "val_f_rmse: 0.013502\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.2506, Val Loss: 0.2082\n",
      "train_e/atom_mae: 0.004572\n",
      "train_e/atom_rmse: 0.006981\n",
      "train_f_mae: 0.006127\n",
      "train_f_rmse: 0.014410\n",
      "val_e/atom_mae: 0.004361\n",
      "val_e/atom_rmse: 0.006893\n",
      "val_f_mae: 0.005799\n",
      "val_f_rmse: 0.013542\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 0.2502, Val Loss: 0.2078\n",
      "train_e/atom_mae: 0.004584\n",
      "train_e/atom_rmse: 0.007008\n",
      "train_f_mae: 0.006080\n",
      "train_f_rmse: 0.014372\n",
      "val_e/atom_mae: 0.004441\n",
      "val_e/atom_rmse: 0.007021\n",
      "val_f_mae: 0.005769\n",
      "val_f_rmse: 0.013467\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.2520, Val Loss: 0.2079\n",
      "train_e/atom_mae: 0.004562\n",
      "train_e/atom_rmse: 0.006966\n",
      "train_f_mae: 0.006128\n",
      "train_f_rmse: 0.014398\n",
      "val_e/atom_mae: 0.004406\n",
      "val_e/atom_rmse: 0.006945\n",
      "val_f_mae: 0.005796\n",
      "val_f_rmse: 0.013507\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 0.2503, Val Loss: 0.2081\n",
      "train_e/atom_mae: 0.004569\n",
      "train_e/atom_rmse: 0.006970\n",
      "train_f_mae: 0.006089\n",
      "train_f_rmse: 0.014374\n",
      "val_e/atom_mae: 0.004352\n",
      "val_e/atom_rmse: 0.006868\n",
      "val_f_mae: 0.005808\n",
      "val_f_rmse: 0.013550\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.2505, Val Loss: 0.2076\n",
      "train_e/atom_mae: 0.004573\n",
      "train_e/atom_rmse: 0.007005\n",
      "train_f_mae: 0.006082\n",
      "train_f_rmse: 0.014361\n",
      "val_e/atom_mae: 0.004449\n",
      "val_e/atom_rmse: 0.007032\n",
      "val_f_mae: 0.005766\n",
      "val_f_rmse: 0.013453\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.2492, Val Loss: 0.2076\n",
      "train_e/atom_mae: 0.004537\n",
      "train_e/atom_rmse: 0.006964\n",
      "train_f_mae: 0.006052\n",
      "train_f_rmse: 0.014335\n",
      "val_e/atom_mae: 0.004380\n",
      "val_e/atom_rmse: 0.006927\n",
      "val_f_mae: 0.005796\n",
      "val_f_rmse: 0.013504\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.2490, Val Loss: 0.2078\n",
      "train_e/atom_mae: 0.004537\n",
      "train_e/atom_rmse: 0.006952\n",
      "train_f_mae: 0.006055\n",
      "train_f_rmse: 0.014341\n",
      "val_e/atom_mae: 0.004363\n",
      "val_e/atom_rmse: 0.006912\n",
      "val_f_mae: 0.005802\n",
      "val_f_rmse: 0.013519\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.2474, Val Loss: 0.2073\n",
      "train_e/atom_mae: 0.004542\n",
      "train_e/atom_rmse: 0.006958\n",
      "train_f_mae: 0.006029\n",
      "train_f_rmse: 0.014312\n",
      "val_e/atom_mae: 0.004417\n",
      "val_e/atom_rmse: 0.006961\n",
      "val_f_mae: 0.005764\n",
      "val_f_rmse: 0.013479\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.2506, Val Loss: 0.2073\n",
      "train_e/atom_mae: 0.004523\n",
      "train_e/atom_rmse: 0.006933\n",
      "train_f_mae: 0.006035\n",
      "train_f_rmse: 0.014327\n",
      "val_e/atom_mae: 0.004410\n",
      "val_e/atom_rmse: 0.006985\n",
      "val_f_mae: 0.005758\n",
      "val_f_rmse: 0.013467\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.2478, Val Loss: 0.2073\n",
      "train_e/atom_mae: 0.004520\n",
      "train_e/atom_rmse: 0.006941\n",
      "train_f_mae: 0.006019\n",
      "train_f_rmse: 0.014296\n",
      "val_e/atom_mae: 0.004382\n",
      "val_e/atom_rmse: 0.006910\n",
      "val_f_mae: 0.005780\n",
      "val_f_rmse: 0.013502\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.2465, Val Loss: 0.2073\n",
      "train_e/atom_mae: 0.004509\n",
      "train_e/atom_rmse: 0.006936\n",
      "train_f_mae: 0.006032\n",
      "train_f_rmse: 0.014286\n",
      "val_e/atom_mae: 0.004370\n",
      "val_e/atom_rmse: 0.006912\n",
      "val_f_mae: 0.005792\n",
      "val_f_rmse: 0.013501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.2472, Val Loss: 0.2069\n",
      "train_e/atom_mae: 0.004494\n",
      "train_e/atom_rmse: 0.006913\n",
      "train_f_mae: 0.005986\n",
      "train_f_rmse: 0.014271\n",
      "val_e/atom_mae: 0.004373\n",
      "val_e/atom_rmse: 0.006913\n",
      "val_f_mae: 0.005776\n",
      "val_f_rmse: 0.013483\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.2467, Val Loss: 0.2071\n",
      "train_e/atom_mae: 0.004532\n",
      "train_e/atom_rmse: 0.006944\n",
      "train_f_mae: 0.006022\n",
      "train_f_rmse: 0.014288\n",
      "val_e/atom_mae: 0.004416\n",
      "val_e/atom_rmse: 0.006980\n",
      "val_f_mae: 0.005760\n",
      "val_f_rmse: 0.013459\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.2476, Val Loss: 0.2070\n",
      "train_e/atom_mae: 0.004508\n",
      "train_e/atom_rmse: 0.006923\n",
      "train_f_mae: 0.005989\n",
      "train_f_rmse: 0.014275\n",
      "val_e/atom_mae: 0.004344\n",
      "val_e/atom_rmse: 0.006857\n",
      "val_f_mae: 0.005789\n",
      "val_f_rmse: 0.013515\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.2461, Val Loss: 0.2068\n",
      "train_e/atom_mae: 0.004512\n",
      "train_e/atom_rmse: 0.006928\n",
      "train_f_mae: 0.005977\n",
      "train_f_rmse: 0.014253\n",
      "val_e/atom_mae: 0.004404\n",
      "val_e/atom_rmse: 0.006961\n",
      "val_f_mae: 0.005758\n",
      "val_f_rmse: 0.013457\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.2467, Val Loss: 0.2070\n",
      "train_e/atom_mae: 0.004499\n",
      "train_e/atom_rmse: 0.006926\n",
      "train_f_mae: 0.005982\n",
      "train_f_rmse: 0.014249\n",
      "val_e/atom_mae: 0.004376\n",
      "val_e/atom_rmse: 0.006950\n",
      "val_f_mae: 0.005773\n",
      "val_f_rmse: 0.013470\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.2461, Val Loss: 0.2067\n",
      "train_e/atom_mae: 0.004496\n",
      "train_e/atom_rmse: 0.006929\n",
      "train_f_mae: 0.005977\n",
      "train_f_rmse: 0.014253\n",
      "val_e/atom_mae: 0.004372\n",
      "val_e/atom_rmse: 0.006913\n",
      "val_f_mae: 0.005777\n",
      "val_f_rmse: 0.013473\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.2456, Val Loss: 0.2067\n",
      "train_e/atom_mae: 0.004486\n",
      "train_e/atom_rmse: 0.006908\n",
      "train_f_mae: 0.005954\n",
      "train_f_rmse: 0.014226\n",
      "val_e/atom_mae: 0.004350\n",
      "val_e/atom_rmse: 0.006873\n",
      "val_f_mae: 0.005789\n",
      "val_f_rmse: 0.013493\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.2457, Val Loss: 0.2065\n",
      "train_e/atom_mae: 0.004497\n",
      "train_e/atom_rmse: 0.006925\n",
      "train_f_mae: 0.005952\n",
      "train_f_rmse: 0.014231\n",
      "val_e/atom_mae: 0.004419\n",
      "val_e/atom_rmse: 0.006986\n",
      "val_f_mae: 0.005742\n",
      "val_f_rmse: 0.013430\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-3, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 20, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss_2, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=400, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0daab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 7.6974, Val Loss: 5.6577\n",
      "train_e/atom_mae: 0.001703\n",
      "train_e/atom_rmse: 0.002236\n",
      "train_f_mae: 0.013134\n",
      "train_f_rmse: 0.025954\n",
      "val_e/atom_mae: 0.002180\n",
      "val_e/atom_rmse: 0.002359\n",
      "val_f_mae: 0.014842\n",
      "val_f_rmse: 0.027504\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 7.7391, Val Loss: 0.8131\n",
      "train_e/atom_mae: 0.002416\n",
      "train_e/atom_rmse: 0.002783\n",
      "train_f_mae: 0.012864\n",
      "train_f_rmse: 0.025716\n",
      "val_e/atom_mae: 0.000321\n",
      "val_e/atom_rmse: 0.000471\n",
      "val_f_mae: 0.013027\n",
      "val_f_rmse: 0.026173\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 6.0357, Val Loss: 0.7834\n",
      "train_e/atom_mae: 0.001956\n",
      "train_e/atom_rmse: 0.002317\n",
      "train_f_mae: 0.012828\n",
      "train_f_rmse: 0.025671\n",
      "val_e/atom_mae: 0.000292\n",
      "val_e/atom_rmse: 0.000424\n",
      "val_f_mae: 0.013030\n",
      "val_f_rmse: 0.026259\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 5.9646, Val Loss: 0.7772\n",
      "train_e/atom_mae: 0.002103\n",
      "train_e/atom_rmse: 0.002421\n",
      "train_f_mae: 0.012796\n",
      "train_f_rmse: 0.025699\n",
      "val_e/atom_mae: 0.000283\n",
      "val_e/atom_rmse: 0.000402\n",
      "val_f_mae: 0.013169\n",
      "val_f_rmse: 0.026452\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 1.9651, Val Loss: 0.7678\n",
      "train_e/atom_mae: 0.001459\n",
      "train_e/atom_rmse: 0.001812\n",
      "train_f_mae: 0.012836\n",
      "train_f_rmse: 0.025691\n",
      "val_e/atom_mae: 0.000285\n",
      "val_e/atom_rmse: 0.000411\n",
      "val_f_mae: 0.012957\n",
      "val_f_rmse: 0.026124\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 3.9707, Val Loss: 0.7558\n",
      "train_e/atom_mae: 0.001382\n",
      "train_e/atom_rmse: 0.001699\n",
      "train_f_mae: 0.012736\n",
      "train_f_rmse: 0.025612\n",
      "val_e/atom_mae: 0.000270\n",
      "val_e/atom_rmse: 0.000382\n",
      "val_f_mae: 0.013041\n",
      "val_f_rmse: 0.026298\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 3.8929, Val Loss: 0.7552\n",
      "train_e/atom_mae: 0.001468\n",
      "train_e/atom_rmse: 0.001770\n",
      "train_f_mae: 0.012680\n",
      "train_f_rmse: 0.025551\n",
      "val_e/atom_mae: 0.000275\n",
      "val_e/atom_rmse: 0.000387\n",
      "val_f_mae: 0.012932\n",
      "val_f_rmse: 0.026207\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 1.7225, Val Loss: 0.7499\n",
      "train_e/atom_mae: 0.001500\n",
      "train_e/atom_rmse: 0.001774\n",
      "train_f_mae: 0.012586\n",
      "train_f_rmse: 0.025483\n",
      "val_e/atom_mae: 0.000266\n",
      "val_e/atom_rmse: 0.000378\n",
      "val_f_mae: 0.012910\n",
      "val_f_rmse: 0.026217\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 3.1675, Val Loss: 0.7454\n",
      "train_e/atom_mae: 0.001281\n",
      "train_e/atom_rmse: 0.001543\n",
      "train_f_mae: 0.012625\n",
      "train_f_rmse: 0.025526\n",
      "val_e/atom_mae: 0.000258\n",
      "val_e/atom_rmse: 0.000369\n",
      "val_f_mae: 0.012952\n",
      "val_f_rmse: 0.026236\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 3.1645, Val Loss: 0.7448\n",
      "train_e/atom_mae: 0.001390\n",
      "train_e/atom_rmse: 0.001701\n",
      "train_f_mae: 0.012666\n",
      "train_f_rmse: 0.025505\n",
      "val_e/atom_mae: 0.000263\n",
      "val_e/atom_rmse: 0.000371\n",
      "val_f_mae: 0.012959\n",
      "val_f_rmse: 0.026170\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 2.6578, Val Loss: 0.7477\n",
      "train_e/atom_mae: 0.001282\n",
      "train_e/atom_rmse: 0.001506\n",
      "train_f_mae: 0.012594\n",
      "train_f_rmse: 0.025490\n",
      "val_e/atom_mae: 0.000262\n",
      "val_e/atom_rmse: 0.000375\n",
      "val_f_mae: 0.012905\n",
      "val_f_rmse: 0.026197\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 2.6805, Val Loss: 0.7440\n",
      "train_e/atom_mae: 0.001037\n",
      "train_e/atom_rmse: 0.001300\n",
      "train_f_mae: 0.012561\n",
      "train_f_rmse: 0.025459\n",
      "val_e/atom_mae: 0.000266\n",
      "val_e/atom_rmse: 0.000375\n",
      "val_f_mae: 0.012846\n",
      "val_f_rmse: 0.026124\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 1.7180, Val Loss: 0.7380\n",
      "train_e/atom_mae: 0.001112\n",
      "train_e/atom_rmse: 0.001328\n",
      "train_f_mae: 0.012588\n",
      "train_f_rmse: 0.025451\n",
      "val_e/atom_mae: 0.000254\n",
      "val_e/atom_rmse: 0.000359\n",
      "val_f_mae: 0.012924\n",
      "val_f_rmse: 0.026193\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 2.2646, Val Loss: 0.7352\n",
      "train_e/atom_mae: 0.001103\n",
      "train_e/atom_rmse: 0.001315\n",
      "train_f_mae: 0.012548\n",
      "train_f_rmse: 0.025425\n",
      "val_e/atom_mae: 0.000257\n",
      "val_e/atom_rmse: 0.000360\n",
      "val_f_mae: 0.012843\n",
      "val_f_rmse: 0.026139\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 1.9849, Val Loss: 0.7351\n",
      "train_e/atom_mae: 0.001024\n",
      "train_e/atom_rmse: 0.001224\n",
      "train_f_mae: 0.012469\n",
      "train_f_rmse: 0.025367\n",
      "val_e/atom_mae: 0.000261\n",
      "val_e/atom_rmse: 0.000364\n",
      "val_f_mae: 0.012825\n",
      "val_f_rmse: 0.026077\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 1.9837, Val Loss: 0.7321\n",
      "train_e/atom_mae: 0.001021\n",
      "train_e/atom_rmse: 0.001215\n",
      "train_f_mae: 0.012487\n",
      "train_f_rmse: 0.025371\n",
      "val_e/atom_mae: 0.000250\n",
      "val_e/atom_rmse: 0.000354\n",
      "val_f_mae: 0.012901\n",
      "val_f_rmse: 0.026132\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 1.0671, Val Loss: 0.7314\n",
      "train_e/atom_mae: 0.000654\n",
      "train_e/atom_rmse: 0.000837\n",
      "train_f_mae: 0.012514\n",
      "train_f_rmse: 0.025407\n",
      "val_e/atom_mae: 0.000248\n",
      "val_e/atom_rmse: 0.000349\n",
      "val_f_mae: 0.012906\n",
      "val_f_rmse: 0.026161\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 1.7028, Val Loss: 0.7308\n",
      "train_e/atom_mae: 0.000912\n",
      "train_e/atom_rmse: 0.001110\n",
      "train_f_mae: 0.012521\n",
      "train_f_rmse: 0.025415\n",
      "val_e/atom_mae: 0.000254\n",
      "val_e/atom_rmse: 0.000356\n",
      "val_f_mae: 0.012830\n",
      "val_f_rmse: 0.026075\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 0.9864, Val Loss: 0.7287\n",
      "train_e/atom_mae: 0.000743\n",
      "train_e/atom_rmse: 0.000941\n",
      "train_f_mae: 0.012484\n",
      "train_f_rmse: 0.025378\n",
      "val_e/atom_mae: 0.000254\n",
      "val_e/atom_rmse: 0.000355\n",
      "val_f_mae: 0.012771\n",
      "val_f_rmse: 0.026052\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 1.1041, Val Loss: 0.7273\n",
      "train_e/atom_mae: 0.000674\n",
      "train_e/atom_rmse: 0.000866\n",
      "train_f_mae: 0.012497\n",
      "train_f_rmse: 0.025385\n",
      "val_e/atom_mae: 0.000248\n",
      "val_e/atom_rmse: 0.000349\n",
      "val_f_mae: 0.012806\n",
      "val_f_rmse: 0.026092\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 1.3233, Val Loss: 0.7275\n",
      "train_e/atom_mae: 0.000680\n",
      "train_e/atom_rmse: 0.000848\n",
      "train_f_mae: 0.012491\n",
      "train_f_rmse: 0.025381\n",
      "val_e/atom_mae: 0.000242\n",
      "val_e/atom_rmse: 0.000345\n",
      "val_f_mae: 0.012855\n",
      "val_f_rmse: 0.026120\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.9426, Val Loss: 0.7258\n",
      "train_e/atom_mae: 0.000670\n",
      "train_e/atom_rmse: 0.000837\n",
      "train_f_mae: 0.012458\n",
      "train_f_rmse: 0.025351\n",
      "val_e/atom_mae: 0.000245\n",
      "val_e/atom_rmse: 0.000346\n",
      "val_f_mae: 0.012833\n",
      "val_f_rmse: 0.026095\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 1.2753, Val Loss: 0.7253\n",
      "train_e/atom_mae: 0.000637\n",
      "train_e/atom_rmse: 0.000802\n",
      "train_f_mae: 0.012406\n",
      "train_f_rmse: 0.025305\n",
      "val_e/atom_mae: 0.000250\n",
      "val_e/atom_rmse: 0.000350\n",
      "val_f_mae: 0.012790\n",
      "val_f_rmse: 0.026040\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 1.2510, Val Loss: 0.7252\n",
      "train_e/atom_mae: 0.000633\n",
      "train_e/atom_rmse: 0.000788\n",
      "train_f_mae: 0.012442\n",
      "train_f_rmse: 0.025327\n",
      "val_e/atom_mae: 0.000246\n",
      "val_e/atom_rmse: 0.000351\n",
      "val_f_mae: 0.012758\n",
      "val_f_rmse: 0.026011\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 1.0489, Val Loss: 0.7238\n",
      "train_e/atom_mae: 0.000587\n",
      "train_e/atom_rmse: 0.000739\n",
      "train_f_mae: 0.012458\n",
      "train_f_rmse: 0.025350\n",
      "val_e/atom_mae: 0.000240\n",
      "val_e/atom_rmse: 0.000342\n",
      "val_f_mae: 0.012843\n",
      "val_f_rmse: 0.026097\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 1.1780, Val Loss: 0.7237\n",
      "train_e/atom_mae: 0.000594\n",
      "train_e/atom_rmse: 0.000748\n",
      "train_f_mae: 0.012437\n",
      "train_f_rmse: 0.025345\n",
      "val_e/atom_mae: 0.000246\n",
      "val_e/atom_rmse: 0.000346\n",
      "val_f_mae: 0.012772\n",
      "val_f_rmse: 0.026050\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 1.0201, Val Loss: 0.7233\n",
      "train_e/atom_mae: 0.000566\n",
      "train_e/atom_rmse: 0.000720\n",
      "train_f_mae: 0.012428\n",
      "train_f_rmse: 0.025328\n",
      "val_e/atom_mae: 0.000243\n",
      "val_e/atom_rmse: 0.000344\n",
      "val_f_mae: 0.012816\n",
      "val_f_rmse: 0.026065\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 1.0771, Val Loss: 0.7228\n",
      "train_e/atom_mae: 0.000528\n",
      "train_e/atom_rmse: 0.000669\n",
      "train_f_mae: 0.012455\n",
      "train_f_rmse: 0.025338\n",
      "val_e/atom_mae: 0.000245\n",
      "val_e/atom_rmse: 0.000346\n",
      "val_f_mae: 0.012772\n",
      "val_f_rmse: 0.026026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 1.1063, Val Loss: 0.7224\n",
      "train_e/atom_mae: 0.000472\n",
      "train_e/atom_rmse: 0.000617\n",
      "train_f_mae: 0.012439\n",
      "train_f_rmse: 0.025341\n",
      "val_e/atom_mae: 0.000245\n",
      "val_e/atom_rmse: 0.000346\n",
      "val_f_mae: 0.012735\n",
      "val_f_rmse: 0.026016\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 1.0366, Val Loss: 0.7208\n",
      "train_e/atom_mae: 0.000484\n",
      "train_e/atom_rmse: 0.000621\n",
      "train_f_mae: 0.012420\n",
      "train_f_rmse: 0.025326\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000338\n",
      "val_f_mae: 0.012829\n",
      "val_f_rmse: 0.026080\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.9090, Val Loss: 0.7213\n",
      "train_e/atom_mae: 0.000435\n",
      "train_e/atom_rmse: 0.000567\n",
      "train_f_mae: 0.012425\n",
      "train_f_rmse: 0.025326\n",
      "val_e/atom_mae: 0.000242\n",
      "val_e/atom_rmse: 0.000342\n",
      "val_f_mae: 0.012782\n",
      "val_f_rmse: 0.026043\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.9647, Val Loss: 0.7213\n",
      "train_e/atom_mae: 0.000459\n",
      "train_e/atom_rmse: 0.000590\n",
      "train_f_mae: 0.012412\n",
      "train_f_rmse: 0.025317\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012802\n",
      "val_f_rmse: 0.026060\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.8210, Val Loss: 0.7205\n",
      "train_e/atom_mae: 0.000424\n",
      "train_e/atom_rmse: 0.000559\n",
      "train_f_mae: 0.012408\n",
      "train_f_rmse: 0.025301\n",
      "val_e/atom_mae: 0.000239\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012791\n",
      "val_f_rmse: 0.026038\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.9430, Val Loss: 0.7213\n",
      "train_e/atom_mae: 0.000430\n",
      "train_e/atom_rmse: 0.000562\n",
      "train_f_mae: 0.012436\n",
      "train_f_rmse: 0.025346\n",
      "val_e/atom_mae: 0.000236\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012789\n",
      "val_f_rmse: 0.026061\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.8440, Val Loss: 0.7195\n",
      "train_e/atom_mae: 0.000385\n",
      "train_e/atom_rmse: 0.000506\n",
      "train_f_mae: 0.012382\n",
      "train_f_rmse: 0.025292\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000338\n",
      "val_f_mae: 0.012790\n",
      "val_f_rmse: 0.026037\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.9702, Val Loss: 0.7203\n",
      "train_e/atom_mae: 0.000402\n",
      "train_e/atom_rmse: 0.000529\n",
      "train_f_mae: 0.012391\n",
      "train_f_rmse: 0.025309\n",
      "val_e/atom_mae: 0.000236\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012803\n",
      "val_f_rmse: 0.026074\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.8566, Val Loss: 0.7195\n",
      "train_e/atom_mae: 0.000376\n",
      "train_e/atom_rmse: 0.000496\n",
      "train_f_mae: 0.012388\n",
      "train_f_rmse: 0.025287\n",
      "val_e/atom_mae: 0.000243\n",
      "val_e/atom_rmse: 0.000342\n",
      "val_f_mae: 0.012755\n",
      "val_f_rmse: 0.025999\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.9279, Val Loss: 0.7195\n",
      "train_e/atom_mae: 0.000417\n",
      "train_e/atom_rmse: 0.000548\n",
      "train_f_mae: 0.012369\n",
      "train_f_rmse: 0.025282\n",
      "val_e/atom_mae: 0.000242\n",
      "val_e/atom_rmse: 0.000343\n",
      "val_f_mae: 0.012735\n",
      "val_f_rmse: 0.025996\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.8345, Val Loss: 0.7181\n",
      "train_e/atom_mae: 0.000357\n",
      "train_e/atom_rmse: 0.000477\n",
      "train_f_mae: 0.012386\n",
      "train_f_rmse: 0.025298\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012762\n",
      "val_f_rmse: 0.026030\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.8986, Val Loss: 0.7181\n",
      "train_e/atom_mae: 0.000439\n",
      "train_e/atom_rmse: 0.000577\n",
      "train_f_mae: 0.012396\n",
      "train_f_rmse: 0.025304\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012759\n",
      "val_f_rmse: 0.026024\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 6.4904, Val Loss: 4.5116\n",
      "train_e/atom_mae: 0.001233\n",
      "train_e/atom_rmse: 0.001610\n",
      "train_f_mae: 0.012498\n",
      "train_f_rmse: 0.025426\n",
      "val_e/atom_mae: 0.001861\n",
      "val_e/atom_rmse: 0.002147\n",
      "val_f_mae: 0.013131\n",
      "val_f_rmse: 0.026330\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 6.2301, Val Loss: 0.7370\n",
      "train_e/atom_mae: 0.002068\n",
      "train_e/atom_rmse: 0.002450\n",
      "train_f_mae: 0.012832\n",
      "train_f_rmse: 0.025709\n",
      "val_e/atom_mae: 0.000265\n",
      "val_e/atom_rmse: 0.000365\n",
      "val_f_mae: 0.012898\n",
      "val_f_rmse: 0.026119\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 5.0827, Val Loss: 0.7361\n",
      "train_e/atom_mae: 0.001924\n",
      "train_e/atom_rmse: 0.002233\n",
      "train_f_mae: 0.012639\n",
      "train_f_rmse: 0.025604\n",
      "val_e/atom_mae: 0.000254\n",
      "val_e/atom_rmse: 0.000349\n",
      "val_f_mae: 0.012971\n",
      "val_f_rmse: 0.026294\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 5.0351, Val Loss: 0.7344\n",
      "train_e/atom_mae: 0.001893\n",
      "train_e/atom_rmse: 0.002197\n",
      "train_f_mae: 0.012650\n",
      "train_f_rmse: 0.025564\n",
      "val_e/atom_mae: 0.000240\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.013005\n",
      "val_f_rmse: 0.026366\n",
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 4.1130, Val Loss: 0.7339\n",
      "train_e/atom_mae: 0.001689\n",
      "train_e/atom_rmse: 0.001982\n",
      "train_f_mae: 0.012676\n",
      "train_f_rmse: 0.025602\n",
      "val_e/atom_mae: 0.000249\n",
      "val_e/atom_rmse: 0.000344\n",
      "val_f_mae: 0.013007\n",
      "val_f_rmse: 0.026302\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 4.1006, Val Loss: 0.7372\n",
      "train_e/atom_mae: 0.001670\n",
      "train_e/atom_rmse: 0.001946\n",
      "train_f_mae: 0.012577\n",
      "train_f_rmse: 0.025500\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000347\n",
      "val_f_mae: 0.013052\n",
      "val_f_rmse: 0.026346\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 3.2477, Val Loss: 0.7407\n",
      "train_e/atom_mae: 0.001411\n",
      "train_e/atom_rmse: 0.001692\n",
      "train_f_mae: 0.012656\n",
      "train_f_rmse: 0.025555\n",
      "val_e/atom_mae: 0.000256\n",
      "val_e/atom_rmse: 0.000373\n",
      "val_f_mae: 0.012899\n",
      "val_f_rmse: 0.026081\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 3.2865, Val Loss: 0.7319\n",
      "train_e/atom_mae: 0.001121\n",
      "train_e/atom_rmse: 0.001397\n",
      "train_f_mae: 0.012757\n",
      "train_f_rmse: 0.025645\n",
      "val_e/atom_mae: 0.000251\n",
      "val_e/atom_rmse: 0.000357\n",
      "val_f_mae: 0.012836\n",
      "val_f_rmse: 0.026098\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 2.7748, Val Loss: 0.7275\n",
      "train_e/atom_mae: 0.001129\n",
      "train_e/atom_rmse: 0.001377\n",
      "train_f_mae: 0.012499\n",
      "train_f_rmse: 0.025404\n",
      "val_e/atom_mae: 0.000251\n",
      "val_e/atom_rmse: 0.000352\n",
      "val_f_mae: 0.012859\n",
      "val_f_rmse: 0.026054\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 1.4099, Val Loss: 0.7278\n",
      "train_e/atom_mae: 0.001151\n",
      "train_e/atom_rmse: 0.001385\n",
      "train_f_mae: 0.012519\n",
      "train_f_rmse: 0.025439\n",
      "val_e/atom_mae: 0.000252\n",
      "val_e/atom_rmse: 0.000347\n",
      "val_f_mae: 0.012863\n",
      "val_f_rmse: 0.026109\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 2.3738, Val Loss: 0.7259\n",
      "train_e/atom_mae: 0.000970\n",
      "train_e/atom_rmse: 0.001197\n",
      "train_f_mae: 0.012478\n",
      "train_f_rmse: 0.025367\n",
      "val_e/atom_mae: 0.000250\n",
      "val_e/atom_rmse: 0.000352\n",
      "val_f_mae: 0.012777\n",
      "val_f_rmse: 0.026038\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 2.3581, Val Loss: 0.7258\n",
      "train_e/atom_mae: 0.001166\n",
      "train_e/atom_rmse: 0.001378\n",
      "train_f_mae: 0.012496\n",
      "train_f_rmse: 0.025410\n",
      "val_e/atom_mae: 0.000247\n",
      "val_e/atom_rmse: 0.000350\n",
      "val_f_mae: 0.012838\n",
      "val_f_rmse: 0.026047\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 2.0500, Val Loss: 0.7299\n",
      "train_e/atom_mae: 0.000882\n",
      "train_e/atom_rmse: 0.001103\n",
      "train_f_mae: 0.012502\n",
      "train_f_rmse: 0.025405\n",
      "val_e/atom_mae: 0.000254\n",
      "val_e/atom_rmse: 0.000355\n",
      "val_f_mae: 0.012827\n",
      "val_f_rmse: 0.026079\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 2.0422, Val Loss: 0.7252\n",
      "train_e/atom_mae: 0.001034\n",
      "train_e/atom_rmse: 0.001233\n",
      "train_f_mae: 0.012474\n",
      "train_f_rmse: 0.025395\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012873\n",
      "val_f_rmse: 0.026138\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 1.8116, Val Loss: 0.7229\n",
      "train_e/atom_mae: 0.000940\n",
      "train_e/atom_rmse: 0.001125\n",
      "train_f_mae: 0.012455\n",
      "train_f_rmse: 0.025348\n",
      "val_e/atom_mae: 0.000247\n",
      "val_e/atom_rmse: 0.000347\n",
      "val_f_mae: 0.012829\n",
      "val_f_rmse: 0.026032\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 1.4328, Val Loss: 0.7229\n",
      "train_e/atom_mae: 0.000894\n",
      "train_e/atom_rmse: 0.001086\n",
      "train_f_mae: 0.012511\n",
      "train_f_rmse: 0.025419\n",
      "val_e/atom_mae: 0.000242\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012829\n",
      "val_f_rmse: 0.026100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 0.9963, Val Loss: 0.7221\n",
      "train_e/atom_mae: 0.000618\n",
      "train_e/atom_rmse: 0.000791\n",
      "train_f_mae: 0.012483\n",
      "train_f_rmse: 0.025382\n",
      "val_e/atom_mae: 0.000243\n",
      "val_e/atom_rmse: 0.000342\n",
      "val_f_mae: 0.012814\n",
      "val_f_rmse: 0.026067\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 1.6171, Val Loss: 0.7236\n",
      "train_e/atom_mae: 0.000747\n",
      "train_e/atom_rmse: 0.000936\n",
      "train_f_mae: 0.012455\n",
      "train_f_rmse: 0.025364\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012825\n",
      "val_f_rmse: 0.026111\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 1.4167, Val Loss: 0.7225\n",
      "train_e/atom_mae: 0.000764\n",
      "train_e/atom_rmse: 0.000940\n",
      "train_f_mae: 0.012475\n",
      "train_f_rmse: 0.025375\n",
      "val_e/atom_mae: 0.000247\n",
      "val_e/atom_rmse: 0.000345\n",
      "val_f_mae: 0.012809\n",
      "val_f_rmse: 0.026042\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 1.3169, Val Loss: 0.7206\n",
      "train_e/atom_mae: 0.000732\n",
      "train_e/atom_rmse: 0.000901\n",
      "train_f_mae: 0.012478\n",
      "train_f_rmse: 0.025393\n",
      "val_e/atom_mae: 0.000235\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012817\n",
      "val_f_rmse: 0.026110\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 1.2971, Val Loss: 0.7206\n",
      "train_e/atom_mae: 0.000639\n",
      "train_e/atom_rmse: 0.000804\n",
      "train_f_mae: 0.012475\n",
      "train_f_rmse: 0.025369\n",
      "val_e/atom_mae: 0.000235\n",
      "val_e/atom_rmse: 0.000333\n",
      "val_f_mae: 0.012864\n",
      "val_f_rmse: 0.026135\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 0.9167, Val Loss: 0.7206\n",
      "train_e/atom_mae: 0.000637\n",
      "train_e/atom_rmse: 0.000807\n",
      "train_f_mae: 0.012460\n",
      "train_f_rmse: 0.025363\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000340\n",
      "val_f_mae: 0.012808\n",
      "val_f_rmse: 0.026040\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 1.0662, Val Loss: 0.7202\n",
      "train_e/atom_mae: 0.000442\n",
      "train_e/atom_rmse: 0.000579\n",
      "train_f_mae: 0.012433\n",
      "train_f_rmse: 0.025334\n",
      "val_e/atom_mae: 0.000244\n",
      "val_e/atom_rmse: 0.000345\n",
      "val_f_mae: 0.012742\n",
      "val_f_rmse: 0.025986\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 0.9147, Val Loss: 0.7200\n",
      "train_e/atom_mae: 0.000524\n",
      "train_e/atom_rmse: 0.000678\n",
      "train_f_mae: 0.012432\n",
      "train_f_rmse: 0.025341\n",
      "val_e/atom_mae: 0.000242\n",
      "val_e/atom_rmse: 0.000344\n",
      "val_f_mae: 0.012747\n",
      "val_f_rmse: 0.025987\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 1.1037, Val Loss: 0.7201\n",
      "train_e/atom_mae: 0.000534\n",
      "train_e/atom_rmse: 0.000688\n",
      "train_f_mae: 0.012468\n",
      "train_f_rmse: 0.025370\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012819\n",
      "val_f_rmse: 0.026024\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 1.0538, Val Loss: 0.7183\n",
      "train_e/atom_mae: 0.000560\n",
      "train_e/atom_rmse: 0.000723\n",
      "train_f_mae: 0.012416\n",
      "train_f_rmse: 0.025329\n",
      "val_e/atom_mae: 0.000235\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012798\n",
      "val_f_rmse: 0.026052\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.9536, Val Loss: 0.7185\n",
      "train_e/atom_mae: 0.000532\n",
      "train_e/atom_rmse: 0.000686\n",
      "train_f_mae: 0.012439\n",
      "train_f_rmse: 0.025353\n",
      "val_e/atom_mae: 0.000231\n",
      "val_e/atom_rmse: 0.000334\n",
      "val_f_mae: 0.012847\n",
      "val_f_rmse: 0.026075\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 0.9245, Val Loss: 0.7183\n",
      "train_e/atom_mae: 0.000481\n",
      "train_e/atom_rmse: 0.000623\n",
      "train_f_mae: 0.012416\n",
      "train_f_rmse: 0.025321\n",
      "val_e/atom_mae: 0.000234\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012778\n",
      "val_f_rmse: 0.026024\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 1.0142, Val Loss: 0.7178\n",
      "train_e/atom_mae: 0.000494\n",
      "train_e/atom_rmse: 0.000641\n",
      "train_f_mae: 0.012395\n",
      "train_f_rmse: 0.025319\n",
      "val_e/atom_mae: 0.000234\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012752\n",
      "val_f_rmse: 0.026033\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.8987, Val Loss: 0.7181\n",
      "train_e/atom_mae: 0.000460\n",
      "train_e/atom_rmse: 0.000595\n",
      "train_f_mae: 0.012401\n",
      "train_f_rmse: 0.025312\n",
      "val_e/atom_mae: 0.000236\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012723\n",
      "val_f_rmse: 0.025977\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.9391, Val Loss: 0.7183\n",
      "train_e/atom_mae: 0.000409\n",
      "train_e/atom_rmse: 0.000538\n",
      "train_f_mae: 0.012399\n",
      "train_f_rmse: 0.025321\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012789\n",
      "val_f_rmse: 0.026046\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.9203, Val Loss: 0.7179\n",
      "train_e/atom_mae: 0.000421\n",
      "train_e/atom_rmse: 0.000549\n",
      "train_f_mae: 0.012416\n",
      "train_f_rmse: 0.025331\n",
      "val_e/atom_mae: 0.000229\n",
      "val_e/atom_rmse: 0.000331\n",
      "val_f_mae: 0.012824\n",
      "val_f_rmse: 0.026074\n",
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.8672, Val Loss: 0.7169\n",
      "train_e/atom_mae: 0.000451\n",
      "train_e/atom_rmse: 0.000593\n",
      "train_f_mae: 0.012420\n",
      "train_f_rmse: 0.025338\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000333\n",
      "val_f_mae: 0.012795\n",
      "val_f_rmse: 0.026043\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 1.1013, Val Loss: 0.7172\n",
      "train_e/atom_mae: 0.000422\n",
      "train_e/atom_rmse: 0.000553\n",
      "train_f_mae: 0.012405\n",
      "train_f_rmse: 0.025311\n",
      "val_e/atom_mae: 0.000241\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012759\n",
      "val_f_rmse: 0.025983\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.8698, Val Loss: 0.7162\n",
      "train_e/atom_mae: 0.000389\n",
      "train_e/atom_rmse: 0.000511\n",
      "train_f_mae: 0.012405\n",
      "train_f_rmse: 0.025333\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000331\n",
      "val_f_mae: 0.012802\n",
      "val_f_rmse: 0.026057\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.8897, Val Loss: 0.7162\n",
      "train_e/atom_mae: 0.000381\n",
      "train_e/atom_rmse: 0.000497\n",
      "train_f_mae: 0.012392\n",
      "train_f_rmse: 0.025315\n",
      "val_e/atom_mae: 0.000233\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012736\n",
      "val_f_rmse: 0.025996\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.8398, Val Loss: 0.7155\n",
      "train_e/atom_mae: 0.000356\n",
      "train_e/atom_rmse: 0.000473\n",
      "train_f_mae: 0.012390\n",
      "train_f_rmse: 0.025315\n",
      "val_e/atom_mae: 0.000231\n",
      "val_e/atom_rmse: 0.000331\n",
      "val_f_mae: 0.012793\n",
      "val_f_rmse: 0.026051\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.8373, Val Loss: 0.7158\n",
      "train_e/atom_mae: 0.000367\n",
      "train_e/atom_rmse: 0.000489\n",
      "train_f_mae: 0.012388\n",
      "train_f_rmse: 0.025325\n",
      "val_e/atom_mae: 0.000229\n",
      "val_e/atom_rmse: 0.000330\n",
      "val_f_mae: 0.012792\n",
      "val_f_rmse: 0.026065\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.8027, Val Loss: 0.7153\n",
      "train_e/atom_mae: 0.000344\n",
      "train_e/atom_rmse: 0.000459\n",
      "train_f_mae: 0.012399\n",
      "train_f_rmse: 0.025335\n",
      "val_e/atom_mae: 0.000234\n",
      "val_e/atom_rmse: 0.000335\n",
      "val_f_mae: 0.012734\n",
      "val_f_rmse: 0.025991\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.8258, Val Loss: 0.7158\n",
      "train_e/atom_mae: 0.000338\n",
      "train_e/atom_rmse: 0.000451\n",
      "train_f_mae: 0.012396\n",
      "train_f_rmse: 0.025316\n",
      "val_e/atom_mae: 0.000230\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012778\n",
      "val_f_rmse: 0.026024\n",
      "##### Step: 9 Learning rate: 0.001 #####\n",
      "Epoch 10, Train Loss: 2.1581, Val Loss: 1.9100\n",
      "train_e/atom_mae: 0.000737\n",
      "train_e/atom_rmse: 0.000948\n",
      "train_f_mae: 0.012414\n",
      "train_f_rmse: 0.025334\n",
      "val_e/atom_mae: 0.000955\n",
      "val_e/atom_rmse: 0.001254\n",
      "val_f_mae: 0.012371\n",
      "val_f_rmse: 0.025380\n",
      "##### Step: 19 Learning rate: 0.001 #####\n",
      "Epoch 20, Train Loss: 4.9393, Val Loss: 0.7315\n",
      "train_e/atom_mae: 0.001412\n",
      "train_e/atom_rmse: 0.001769\n",
      "train_f_mae: 0.012818\n",
      "train_f_rmse: 0.025730\n",
      "val_e/atom_mae: 0.000240\n",
      "val_e/atom_rmse: 0.000330\n",
      "val_f_mae: 0.013159\n",
      "val_f_rmse: 0.026328\n",
      "##### Step: 29 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 30, Train Loss: 2.1513, Val Loss: 0.7210\n",
      "train_e/atom_mae: 0.001112\n",
      "train_e/atom_rmse: 0.001367\n",
      "train_f_mae: 0.012536\n",
      "train_f_rmse: 0.025429\n",
      "val_e/atom_mae: 0.000240\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012821\n",
      "val_f_rmse: 0.026060\n",
      "##### Step: 39 Learning rate: 0.0009000000000000001 #####\n",
      "Epoch 40, Train Loss: 4.8408, Val Loss: 0.7588\n",
      "train_e/atom_mae: 0.001361\n",
      "train_e/atom_rmse: 0.001711\n",
      "train_f_mae: 0.012561\n",
      "train_f_rmse: 0.025463\n",
      "val_e/atom_mae: 0.000291\n",
      "val_e/atom_rmse: 0.000405\n",
      "val_f_mae: 0.012825\n",
      "val_f_rmse: 0.026041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 49 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 50, Train Loss: 3.7670, Val Loss: 0.7278\n",
      "train_e/atom_mae: 0.001540\n",
      "train_e/atom_rmse: 0.001842\n",
      "train_f_mae: 0.012642\n",
      "train_f_rmse: 0.025509\n",
      "val_e/atom_mae: 0.000244\n",
      "val_e/atom_rmse: 0.000346\n",
      "val_f_mae: 0.012886\n",
      "val_f_rmse: 0.026133\n",
      "##### Step: 59 Learning rate: 0.0008100000000000001 #####\n",
      "Epoch 60, Train Loss: 3.6620, Val Loss: 0.7372\n",
      "train_e/atom_mae: 0.001575\n",
      "train_e/atom_rmse: 0.001844\n",
      "train_f_mae: 0.012544\n",
      "train_f_rmse: 0.025454\n",
      "val_e/atom_mae: 0.000267\n",
      "val_e/atom_rmse: 0.000371\n",
      "val_f_mae: 0.012829\n",
      "val_f_rmse: 0.026039\n",
      "##### Step: 69 Learning rate: 0.000729 #####\n",
      "Epoch 70, Train Loss: 3.0787, Val Loss: 0.7251\n",
      "train_e/atom_mae: 0.001298\n",
      "train_e/atom_rmse: 0.001572\n",
      "train_f_mae: 0.012576\n",
      "train_f_rmse: 0.025477\n",
      "val_e/atom_mae: 0.000245\n",
      "val_e/atom_rmse: 0.000340\n",
      "val_f_mae: 0.012910\n",
      "val_f_rmse: 0.026150\n",
      "##### Step: 79 Learning rate: 0.000729 #####\n",
      "Epoch 80, Train Loss: 2.3225, Val Loss: 0.7248\n",
      "train_e/atom_mae: 0.001350\n",
      "train_e/atom_rmse: 0.001603\n",
      "train_f_mae: 0.012515\n",
      "train_f_rmse: 0.025411\n",
      "val_e/atom_mae: 0.000252\n",
      "val_e/atom_rmse: 0.000349\n",
      "val_f_mae: 0.012761\n",
      "val_f_rmse: 0.026061\n",
      "##### Step: 89 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 90, Train Loss: 1.3103, Val Loss: 0.7203\n",
      "train_e/atom_mae: 0.000811\n",
      "train_e/atom_rmse: 0.001028\n",
      "train_f_mae: 0.012500\n",
      "train_f_rmse: 0.025404\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012876\n",
      "val_f_rmse: 0.026113\n",
      "##### Step: 99 Learning rate: 0.0006561000000000001 #####\n",
      "Epoch 100, Train Loss: 2.6069, Val Loss: 0.7331\n",
      "train_e/atom_mae: 0.001222\n",
      "train_e/atom_rmse: 0.001461\n",
      "train_f_mae: 0.012565\n",
      "train_f_rmse: 0.025463\n",
      "val_e/atom_mae: 0.000266\n",
      "val_e/atom_rmse: 0.000364\n",
      "val_f_mae: 0.012799\n",
      "val_f_rmse: 0.026041\n",
      "##### Step: 109 Learning rate: 0.00059049 #####\n",
      "Epoch 110, Train Loss: 1.1910, Val Loss: 0.7219\n",
      "train_e/atom_mae: 0.000936\n",
      "train_e/atom_rmse: 0.001154\n",
      "train_f_mae: 0.012571\n",
      "train_f_rmse: 0.025491\n",
      "val_e/atom_mae: 0.000241\n",
      "val_e/atom_rmse: 0.000338\n",
      "val_f_mae: 0.012871\n",
      "val_f_rmse: 0.026109\n",
      "##### Step: 119 Learning rate: 0.00059049 #####\n",
      "Epoch 120, Train Loss: 2.2384, Val Loss: 0.7204\n",
      "train_e/atom_mae: 0.000854\n",
      "train_e/atom_rmse: 0.001072\n",
      "train_f_mae: 0.012537\n",
      "train_f_rmse: 0.025412\n",
      "val_e/atom_mae: 0.000239\n",
      "val_e/atom_rmse: 0.000343\n",
      "val_f_mae: 0.012781\n",
      "val_f_rmse: 0.026021\n",
      "##### Step: 129 Learning rate: 0.000531441 #####\n",
      "Epoch 130, Train Loss: 1.7532, Val Loss: 0.7212\n",
      "train_e/atom_mae: 0.000888\n",
      "train_e/atom_rmse: 0.001107\n",
      "train_f_mae: 0.012522\n",
      "train_f_rmse: 0.025452\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012871\n",
      "val_f_rmse: 0.026100\n",
      "##### Step: 139 Learning rate: 0.000531441 #####\n",
      "Epoch 140, Train Loss: 1.1200, Val Loss: 0.7185\n",
      "train_e/atom_mae: 0.000657\n",
      "train_e/atom_rmse: 0.000839\n",
      "train_f_mae: 0.012423\n",
      "train_f_rmse: 0.025350\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012812\n",
      "val_f_rmse: 0.026062\n",
      "##### Step: 149 Learning rate: 0.0004782969 #####\n",
      "Epoch 150, Train Loss: 1.6850, Val Loss: 0.7193\n",
      "train_e/atom_mae: 0.000803\n",
      "train_e/atom_rmse: 0.000994\n",
      "train_f_mae: 0.012449\n",
      "train_f_rmse: 0.025352\n",
      "val_e/atom_mae: 0.000235\n",
      "val_e/atom_rmse: 0.000340\n",
      "val_f_mae: 0.012764\n",
      "val_f_rmse: 0.026020\n",
      "##### Step: 159 Learning rate: 0.0004782969 #####\n",
      "Epoch 160, Train Loss: 1.6983, Val Loss: 0.7188\n",
      "train_e/atom_mae: 0.000859\n",
      "train_e/atom_rmse: 0.001059\n",
      "train_f_mae: 0.012453\n",
      "train_f_rmse: 0.025377\n",
      "val_e/atom_mae: 0.000238\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012787\n",
      "val_f_rmse: 0.026034\n",
      "##### Step: 169 Learning rate: 0.00043046721 #####\n",
      "Epoch 170, Train Loss: 1.4835, Val Loss: 0.7198\n",
      "train_e/atom_mae: 0.000771\n",
      "train_e/atom_rmse: 0.000949\n",
      "train_f_mae: 0.012444\n",
      "train_f_rmse: 0.025360\n",
      "val_e/atom_mae: 0.000239\n",
      "val_e/atom_rmse: 0.000341\n",
      "val_f_mae: 0.012775\n",
      "val_f_rmse: 0.026023\n",
      "##### Step: 179 Learning rate: 0.00043046721 #####\n",
      "Epoch 180, Train Loss: 1.5309, Val Loss: 0.7209\n",
      "train_e/atom_mae: 0.000640\n",
      "train_e/atom_rmse: 0.000819\n",
      "train_f_mae: 0.012443\n",
      "train_f_rmse: 0.025354\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000340\n",
      "val_f_mae: 0.012817\n",
      "val_f_rmse: 0.026050\n",
      "##### Step: 189 Learning rate: 0.000387420489 #####\n",
      "Epoch 190, Train Loss: 1.1532, Val Loss: 0.7180\n",
      "train_e/atom_mae: 0.000723\n",
      "train_e/atom_rmse: 0.000890\n",
      "train_f_mae: 0.012433\n",
      "train_f_rmse: 0.025345\n",
      "val_e/atom_mae: 0.000234\n",
      "val_e/atom_rmse: 0.000335\n",
      "val_f_mae: 0.012798\n",
      "val_f_rmse: 0.026058\n",
      "##### Step: 199 Learning rate: 0.000387420489 #####\n",
      "Epoch 200, Train Loss: 1.3690, Val Loss: 0.7193\n",
      "train_e/atom_mae: 0.000601\n",
      "train_e/atom_rmse: 0.000766\n",
      "train_f_mae: 0.012394\n",
      "train_f_rmse: 0.025308\n",
      "val_e/atom_mae: 0.000236\n",
      "val_e/atom_rmse: 0.000340\n",
      "val_f_mae: 0.012755\n",
      "val_f_rmse: 0.026009\n",
      "##### Step: 209 Learning rate: 0.0003486784401 #####\n",
      "Epoch 210, Train Loss: 1.6299, Val Loss: 0.7175\n",
      "train_e/atom_mae: 0.000689\n",
      "train_e/atom_rmse: 0.000870\n",
      "train_f_mae: 0.012431\n",
      "train_f_rmse: 0.025347\n",
      "val_e/atom_mae: 0.000235\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012760\n",
      "val_f_rmse: 0.026029\n",
      "##### Step: 219 Learning rate: 0.0003486784401 #####\n",
      "Epoch 220, Train Loss: 1.2639, Val Loss: 0.7179\n",
      "train_e/atom_mae: 0.000659\n",
      "train_e/atom_rmse: 0.000820\n",
      "train_f_mae: 0.012440\n",
      "train_f_rmse: 0.025341\n",
      "val_e/atom_mae: 0.000229\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012851\n",
      "val_f_rmse: 0.026073\n",
      "##### Step: 229 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 230, Train Loss: 1.1426, Val Loss: 0.7185\n",
      "train_e/atom_mae: 0.000589\n",
      "train_e/atom_rmse: 0.000740\n",
      "train_f_mae: 0.012424\n",
      "train_f_rmse: 0.025340\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012795\n",
      "val_f_rmse: 0.026039\n",
      "##### Step: 239 Learning rate: 0.00031381059609000004 #####\n",
      "Epoch 240, Train Loss: 1.1184, Val Loss: 0.7184\n",
      "train_e/atom_mae: 0.000533\n",
      "train_e/atom_rmse: 0.000688\n",
      "train_f_mae: 0.012403\n",
      "train_f_rmse: 0.025318\n",
      "val_e/atom_mae: 0.000233\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012763\n",
      "val_f_rmse: 0.026030\n",
      "##### Step: 249 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 250, Train Loss: 1.0554, Val Loss: 0.7164\n",
      "train_e/atom_mae: 0.000513\n",
      "train_e/atom_rmse: 0.000662\n",
      "train_f_mae: 0.012413\n",
      "train_f_rmse: 0.025326\n",
      "val_e/atom_mae: 0.000239\n",
      "val_e/atom_rmse: 0.000339\n",
      "val_f_mae: 0.012719\n",
      "val_f_rmse: 0.025978\n",
      "##### Step: 259 Learning rate: 0.00028242953648100003 #####\n",
      "Epoch 260, Train Loss: 0.9306, Val Loss: 0.7152\n",
      "train_e/atom_mae: 0.000469\n",
      "train_e/atom_rmse: 0.000608\n",
      "train_f_mae: 0.012445\n",
      "train_f_rmse: 0.025351\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012827\n",
      "val_f_rmse: 0.026041\n",
      "##### Step: 269 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 270, Train Loss: 0.9389, Val Loss: 0.7169\n",
      "train_e/atom_mae: 0.000457\n",
      "train_e/atom_rmse: 0.000591\n",
      "train_f_mae: 0.012385\n",
      "train_f_rmse: 0.025313\n",
      "val_e/atom_mae: 0.000240\n",
      "val_e/atom_rmse: 0.000342\n",
      "val_f_mae: 0.012686\n",
      "val_f_rmse: 0.025935\n",
      "##### Step: 279 Learning rate: 0.00025418658283290005 #####\n",
      "Epoch 280, Train Loss: 1.0499, Val Loss: 0.7167\n",
      "train_e/atom_mae: 0.000527\n",
      "train_e/atom_rmse: 0.000705\n",
      "train_f_mae: 0.012447\n",
      "train_f_rmse: 0.025347\n",
      "val_e/atom_mae: 0.000234\n",
      "val_e/atom_rmse: 0.000336\n",
      "val_f_mae: 0.012765\n",
      "val_f_rmse: 0.026000\n",
      "##### Step: 289 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 290, Train Loss: 0.9746, Val Loss: 0.7149\n",
      "train_e/atom_mae: 0.000451\n",
      "train_e/atom_rmse: 0.000580\n",
      "train_f_mae: 0.012402\n",
      "train_f_rmse: 0.025312\n",
      "val_e/atom_mae: 0.000233\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012782\n",
      "val_f_rmse: 0.026017\n",
      "##### Step: 299 Learning rate: 0.00022876792454961005 #####\n",
      "Epoch 300, Train Loss: 0.8117, Val Loss: 0.7157\n",
      "train_e/atom_mae: 0.000412\n",
      "train_e/atom_rmse: 0.000540\n",
      "train_f_mae: 0.012420\n",
      "train_f_rmse: 0.025332\n",
      "val_e/atom_mae: 0.000233\n",
      "val_e/atom_rmse: 0.000335\n",
      "val_f_mae: 0.012756\n",
      "val_f_rmse: 0.025991\n",
      "##### Step: 309 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 310, Train Loss: 0.9132, Val Loss: 0.7139\n",
      "train_e/atom_mae: 0.000477\n",
      "train_e/atom_rmse: 0.000624\n",
      "train_f_mae: 0.012419\n",
      "train_f_rmse: 0.025330\n",
      "val_e/atom_mae: 0.000237\n",
      "val_e/atom_rmse: 0.000333\n",
      "val_f_mae: 0.012738\n",
      "val_f_rmse: 0.025999\n",
      "##### Step: 319 Learning rate: 0.00020589113209464906 #####\n",
      "Epoch 320, Train Loss: 0.9128, Val Loss: 0.7153\n",
      "train_e/atom_mae: 0.000426\n",
      "train_e/atom_rmse: 0.000558\n",
      "train_f_mae: 0.012392\n",
      "train_f_rmse: 0.025320\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012766\n",
      "val_f_rmse: 0.026022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 329 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 330, Train Loss: 0.8198, Val Loss: 0.7153\n",
      "train_e/atom_mae: 0.000397\n",
      "train_e/atom_rmse: 0.000525\n",
      "train_f_mae: 0.012424\n",
      "train_f_rmse: 0.025342\n",
      "val_e/atom_mae: 0.000228\n",
      "val_e/atom_rmse: 0.000331\n",
      "val_f_mae: 0.012818\n",
      "val_f_rmse: 0.026028\n",
      "##### Step: 339 Learning rate: 0.00018530201888518417 #####\n",
      "Epoch 340, Train Loss: 0.8793, Val Loss: 0.7145\n",
      "train_e/atom_mae: 0.000382\n",
      "train_e/atom_rmse: 0.000501\n",
      "train_f_mae: 0.012391\n",
      "train_f_rmse: 0.025316\n",
      "val_e/atom_mae: 0.000233\n",
      "val_e/atom_rmse: 0.000337\n",
      "val_f_mae: 0.012694\n",
      "val_f_rmse: 0.025955\n",
      "##### Step: 349 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 350, Train Loss: 0.8396, Val Loss: 0.7139\n",
      "train_e/atom_mae: 0.000372\n",
      "train_e/atom_rmse: 0.000492\n",
      "train_f_mae: 0.012383\n",
      "train_f_rmse: 0.025293\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000333\n",
      "val_f_mae: 0.012737\n",
      "val_f_rmse: 0.025984\n",
      "##### Step: 359 Learning rate: 0.00016677181699666576 #####\n",
      "Epoch 360, Train Loss: 0.8212, Val Loss: 0.7143\n",
      "train_e/atom_mae: 0.000357\n",
      "train_e/atom_rmse: 0.000471\n",
      "train_f_mae: 0.012375\n",
      "train_f_rmse: 0.025293\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000334\n",
      "val_f_mae: 0.012705\n",
      "val_f_rmse: 0.025977\n",
      "##### Step: 369 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 370, Train Loss: 0.7987, Val Loss: 0.7138\n",
      "train_e/atom_mae: 0.000342\n",
      "train_e/atom_rmse: 0.000452\n",
      "train_f_mae: 0.012405\n",
      "train_f_rmse: 0.025327\n",
      "val_e/atom_mae: 0.000230\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012744\n",
      "val_f_rmse: 0.026003\n",
      "##### Step: 379 Learning rate: 0.0001500946352969992 #####\n",
      "Epoch 380, Train Loss: 0.8378, Val Loss: 0.7138\n",
      "train_e/atom_mae: 0.000346\n",
      "train_e/atom_rmse: 0.000459\n",
      "train_f_mae: 0.012392\n",
      "train_f_rmse: 0.025314\n",
      "val_e/atom_mae: 0.000230\n",
      "val_e/atom_rmse: 0.000330\n",
      "val_f_mae: 0.012778\n",
      "val_f_rmse: 0.026024\n",
      "##### Step: 389 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 390, Train Loss: 0.8132, Val Loss: 0.7135\n",
      "train_e/atom_mae: 0.000343\n",
      "train_e/atom_rmse: 0.000459\n",
      "train_f_mae: 0.012341\n",
      "train_f_rmse: 0.025278\n",
      "val_e/atom_mae: 0.000232\n",
      "val_e/atom_rmse: 0.000332\n",
      "val_f_mae: 0.012734\n",
      "val_f_rmse: 0.025996\n",
      "##### Step: 399 Learning rate: 0.0001350851717672993 #####\n",
      "Epoch 400, Train Loss: 0.8110, Val Loss: 0.7139\n",
      "train_e/atom_mae: 0.000334\n",
      "train_e/atom_rmse: 0.000449\n",
      "train_f_mae: 0.012372\n",
      "train_f_rmse: 0.025299\n",
      "val_e/atom_mae: 0.000228\n",
      "val_e/atom_rmse: 0.000331\n",
      "val_f_mae: 0.012747\n",
      "val_f_rmse: 0.026002\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-3, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 20, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss_3, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=400, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aa72469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 9 Learning rate: 0.0001 #####\n",
      "Epoch 10, Train Loss: 2.0277, Val Loss: 1.2304\n",
      "train_e/atom_mae: 0.000215\n",
      "train_e/atom_rmse: 0.000294\n",
      "train_f_mae: 0.014397\n",
      "train_f_rmse: 0.027374\n",
      "val_e/atom_mae: 0.000188\n",
      "val_e/atom_rmse: 0.000241\n",
      "val_f_mae: 0.015173\n",
      "val_f_rmse: 0.028556\n",
      "##### Step: 19 Learning rate: 0.0001 #####\n",
      "Epoch 20, Train Loss: 1.6945, Val Loss: 1.0554\n",
      "train_e/atom_mae: 0.000232\n",
      "train_e/atom_rmse: 0.000303\n",
      "train_f_mae: 0.014969\n",
      "train_f_rmse: 0.028004\n",
      "val_e/atom_mae: 0.000130\n",
      "val_e/atom_rmse: 0.000186\n",
      "val_f_mae: 0.015252\n",
      "val_f_rmse: 0.028624\n",
      "##### Step: 29 Learning rate: 9e-05 #####\n",
      "Epoch 30, Train Loss: 1.5359, Val Loss: 1.0508\n",
      "train_e/atom_mae: 0.000222\n",
      "train_e/atom_rmse: 0.000291\n",
      "train_f_mae: 0.015012\n",
      "train_f_rmse: 0.028067\n",
      "val_e/atom_mae: 0.000129\n",
      "val_e/atom_rmse: 0.000183\n",
      "val_f_mae: 0.015283\n",
      "val_f_rmse: 0.028668\n",
      "##### Step: 39 Learning rate: 9e-05 #####\n",
      "Epoch 40, Train Loss: 1.5039, Val Loss: 1.0501\n",
      "train_e/atom_mae: 0.000210\n",
      "train_e/atom_rmse: 0.000275\n",
      "train_f_mae: 0.015048\n",
      "train_f_rmse: 0.028111\n",
      "val_e/atom_mae: 0.000127\n",
      "val_e/atom_rmse: 0.000181\n",
      "val_f_mae: 0.015319\n",
      "val_f_rmse: 0.028698\n",
      "##### Step: 49 Learning rate: 8.1e-05 #####\n",
      "Epoch 50, Train Loss: 1.3952, Val Loss: 1.0457\n",
      "train_e/atom_mae: 0.000208\n",
      "train_e/atom_rmse: 0.000273\n",
      "train_f_mae: 0.015062\n",
      "train_f_rmse: 0.028139\n",
      "val_e/atom_mae: 0.000125\n",
      "val_e/atom_rmse: 0.000179\n",
      "val_f_mae: 0.015320\n",
      "val_f_rmse: 0.028713\n",
      "##### Step: 59 Learning rate: 8.1e-05 #####\n",
      "Epoch 60, Train Loss: 1.6021, Val Loss: 1.0418\n",
      "train_e/atom_mae: 0.000205\n",
      "train_e/atom_rmse: 0.000267\n",
      "train_f_mae: 0.015080\n",
      "train_f_rmse: 0.028169\n",
      "val_e/atom_mae: 0.000124\n",
      "val_e/atom_rmse: 0.000177\n",
      "val_f_mae: 0.015356\n",
      "val_f_rmse: 0.028748\n",
      "##### Step: 69 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 70, Train Loss: 1.2792, Val Loss: 1.0352\n",
      "train_e/atom_mae: 0.000199\n",
      "train_e/atom_rmse: 0.000265\n",
      "train_f_mae: 0.015085\n",
      "train_f_rmse: 0.028178\n",
      "val_e/atom_mae: 0.000124\n",
      "val_e/atom_rmse: 0.000177\n",
      "val_f_mae: 0.015330\n",
      "val_f_rmse: 0.028742\n",
      "##### Step: 79 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 80, Train Loss: 1.7968, Val Loss: 1.0358\n",
      "train_e/atom_mae: 0.000200\n",
      "train_e/atom_rmse: 0.000263\n",
      "train_f_mae: 0.015118\n",
      "train_f_rmse: 0.028217\n",
      "val_e/atom_mae: 0.000124\n",
      "val_e/atom_rmse: 0.000175\n",
      "val_f_mae: 0.015383\n",
      "val_f_rmse: 0.028807\n",
      "##### Step: 89 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 90, Train Loss: 1.2517, Val Loss: 1.0328\n",
      "train_e/atom_mae: 0.000177\n",
      "train_e/atom_rmse: 0.000236\n",
      "train_f_mae: 0.015122\n",
      "train_f_rmse: 0.028233\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000175\n",
      "val_f_mae: 0.015358\n",
      "val_f_rmse: 0.028781\n",
      "##### Step: 99 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 100, Train Loss: 1.3304, Val Loss: 1.0321\n",
      "train_e/atom_mae: 0.000180\n",
      "train_e/atom_rmse: 0.000239\n",
      "train_f_mae: 0.015123\n",
      "train_f_rmse: 0.028234\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000174\n",
      "val_f_mae: 0.015365\n",
      "val_f_rmse: 0.028793\n",
      "##### Step: 109 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 110, Train Loss: 1.3063, Val Loss: 1.0305\n",
      "train_e/atom_mae: 0.000178\n",
      "train_e/atom_rmse: 0.000236\n",
      "train_f_mae: 0.015165\n",
      "train_f_rmse: 0.028289\n",
      "val_e/atom_mae: 0.000122\n",
      "val_e/atom_rmse: 0.000172\n",
      "val_f_mae: 0.015432\n",
      "val_f_rmse: 0.028872\n",
      "##### Step: 119 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 120, Train Loss: 1.2349, Val Loss: 1.0323\n",
      "train_e/atom_mae: 0.000184\n",
      "train_e/atom_rmse: 0.000244\n",
      "train_f_mae: 0.015171\n",
      "train_f_rmse: 0.028296\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000173\n",
      "val_f_mae: 0.015400\n",
      "val_f_rmse: 0.028831\n",
      "##### Step: 129 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 130, Train Loss: 1.2491, Val Loss: 1.0294\n",
      "train_e/atom_mae: 0.000170\n",
      "train_e/atom_rmse: 0.000227\n",
      "train_f_mae: 0.015147\n",
      "train_f_rmse: 0.028270\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000172\n",
      "val_f_mae: 0.015398\n",
      "val_f_rmse: 0.028841\n",
      "##### Step: 139 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 140, Train Loss: 1.3529, Val Loss: 1.0295\n",
      "train_e/atom_mae: 0.000166\n",
      "train_e/atom_rmse: 0.000223\n",
      "train_f_mae: 0.015159\n",
      "train_f_rmse: 0.028292\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000171\n",
      "val_f_mae: 0.015421\n",
      "val_f_rmse: 0.028872\n",
      "##### Step: 149 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 150, Train Loss: 1.2090, Val Loss: 1.0291\n",
      "train_e/atom_mae: 0.000169\n",
      "train_e/atom_rmse: 0.000226\n",
      "train_f_mae: 0.015173\n",
      "train_f_rmse: 0.028305\n",
      "val_e/atom_mae: 0.000122\n",
      "val_e/atom_rmse: 0.000171\n",
      "val_f_mae: 0.015455\n",
      "val_f_rmse: 0.028906\n",
      "##### Step: 159 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 160, Train Loss: 1.1925, Val Loss: 1.0316\n",
      "train_e/atom_mae: 0.000168\n",
      "train_e/atom_rmse: 0.000224\n",
      "train_f_mae: 0.015174\n",
      "train_f_rmse: 0.028302\n",
      "val_e/atom_mae: 0.000122\n",
      "val_e/atom_rmse: 0.000171\n",
      "val_f_mae: 0.015457\n",
      "val_f_rmse: 0.028891\n",
      "##### Step: 169 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 170, Train Loss: 1.2684, Val Loss: 1.0278\n",
      "train_e/atom_mae: 0.000163\n",
      "train_e/atom_rmse: 0.000219\n",
      "train_f_mae: 0.015168\n",
      "train_f_rmse: 0.028304\n",
      "val_e/atom_mae: 0.000123\n",
      "val_e/atom_rmse: 0.000171\n",
      "val_f_mae: 0.015413\n",
      "val_f_rmse: 0.028864\n",
      "##### Step: 179 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 180, Train Loss: 1.1887, Val Loss: 1.0244\n",
      "train_e/atom_mae: 0.000161\n",
      "train_e/atom_rmse: 0.000218\n",
      "train_f_mae: 0.015187\n",
      "train_f_rmse: 0.028326\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000170\n",
      "val_f_mae: 0.015415\n",
      "val_f_rmse: 0.028876\n",
      "##### Step: 189 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 190, Train Loss: 1.1755, Val Loss: 1.0255\n",
      "train_e/atom_mae: 0.000156\n",
      "train_e/atom_rmse: 0.000211\n",
      "train_f_mae: 0.015172\n",
      "train_f_rmse: 0.028313\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000169\n",
      "val_f_mae: 0.015450\n",
      "val_f_rmse: 0.028921\n",
      "##### Step: 199 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 200, Train Loss: 1.1939, Val Loss: 1.0262\n",
      "train_e/atom_mae: 0.000154\n",
      "train_e/atom_rmse: 0.000209\n",
      "train_f_mae: 0.015182\n",
      "train_f_rmse: 0.028322\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000171\n",
      "val_f_mae: 0.015401\n",
      "val_f_rmse: 0.028856\n",
      "##### Step: 209 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 210, Train Loss: 1.1542, Val Loss: 1.0239\n",
      "train_e/atom_mae: 0.000154\n",
      "train_e/atom_rmse: 0.000209\n",
      "train_f_mae: 0.015190\n",
      "train_f_rmse: 0.028337\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000170\n",
      "val_f_mae: 0.015436\n",
      "val_f_rmse: 0.028899\n",
      "##### Step: 219 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 220, Train Loss: 1.2358, Val Loss: 1.0232\n",
      "train_e/atom_mae: 0.000155\n",
      "train_e/atom_rmse: 0.000210\n",
      "train_f_mae: 0.015185\n",
      "train_f_rmse: 0.028328\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000170\n",
      "val_f_mae: 0.015440\n",
      "val_f_rmse: 0.028909\n",
      "##### Step: 229 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 230, Train Loss: 1.1867, Val Loss: 1.0252\n",
      "train_e/atom_mae: 0.000152\n",
      "train_e/atom_rmse: 0.000206\n",
      "train_f_mae: 0.015183\n",
      "train_f_rmse: 0.028333\n",
      "val_e/atom_mae: 0.000122\n",
      "val_e/atom_rmse: 0.000170\n",
      "val_f_mae: 0.015434\n",
      "val_f_rmse: 0.028902\n",
      "##### Step: 239 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 240, Train Loss: 1.1710, Val Loss: 1.0227\n",
      "train_e/atom_mae: 0.000148\n",
      "train_e/atom_rmse: 0.000201\n",
      "train_f_mae: 0.015197\n",
      "train_f_rmse: 0.028353\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015482\n",
      "val_f_rmse: 0.028954\n",
      "##### Step: 249 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 250, Train Loss: 1.1408, Val Loss: 1.0241\n",
      "train_e/atom_mae: 0.000145\n",
      "train_e/atom_rmse: 0.000197\n",
      "train_f_mae: 0.015215\n",
      "train_f_rmse: 0.028365\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015472\n",
      "val_f_rmse: 0.028940\n",
      "##### Step: 259 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 260, Train Loss: 1.1692, Val Loss: 1.0267\n",
      "train_e/atom_mae: 0.000150\n",
      "train_e/atom_rmse: 0.000203\n",
      "train_f_mae: 0.015213\n",
      "train_f_rmse: 0.028369\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000169\n",
      "val_f_mae: 0.015459\n",
      "val_f_rmse: 0.028924\n",
      "##### Step: 269 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 270, Train Loss: 1.1726, Val Loss: 1.0179\n",
      "train_e/atom_mae: 0.000142\n",
      "train_e/atom_rmse: 0.000195\n",
      "train_f_mae: 0.015205\n",
      "train_f_rmse: 0.028363\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000169\n",
      "val_f_mae: 0.015445\n",
      "val_f_rmse: 0.028938\n",
      "##### Step: 279 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 280, Train Loss: 1.1511, Val Loss: 1.0231\n",
      "train_e/atom_mae: 0.000142\n",
      "train_e/atom_rmse: 0.000194\n",
      "train_f_mae: 0.015201\n",
      "train_f_rmse: 0.028358\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015479\n",
      "val_f_rmse: 0.028954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 289 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 290, Train Loss: 1.1648, Val Loss: 1.0237\n",
      "train_e/atom_mae: 0.000143\n",
      "train_e/atom_rmse: 0.000196\n",
      "train_f_mae: 0.015227\n",
      "train_f_rmse: 0.028385\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015491\n",
      "val_f_rmse: 0.028972\n",
      "##### Step: 299 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 300, Train Loss: 1.1316, Val Loss: 1.0253\n",
      "train_e/atom_mae: 0.000145\n",
      "train_e/atom_rmse: 0.000198\n",
      "train_f_mae: 0.015201\n",
      "train_f_rmse: 0.028354\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015457\n",
      "val_f_rmse: 0.028925\n",
      "##### Step: 309 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 310, Train Loss: 1.1235, Val Loss: 1.0208\n",
      "train_e/atom_mae: 0.000138\n",
      "train_e/atom_rmse: 0.000190\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028364\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015456\n",
      "val_f_rmse: 0.028935\n",
      "##### Step: 319 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 320, Train Loss: 1.1381, Val Loss: 1.0211\n",
      "train_e/atom_mae: 0.000139\n",
      "train_e/atom_rmse: 0.000191\n",
      "train_f_mae: 0.015198\n",
      "train_f_rmse: 0.028352\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015453\n",
      "val_f_rmse: 0.028922\n",
      "##### Step: 329 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 330, Train Loss: 1.1295, Val Loss: 1.0238\n",
      "train_e/atom_mae: 0.000138\n",
      "train_e/atom_rmse: 0.000191\n",
      "train_f_mae: 0.015198\n",
      "train_f_rmse: 0.028354\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015456\n",
      "val_f_rmse: 0.028932\n",
      "##### Step: 339 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 340, Train Loss: 1.1208, Val Loss: 1.0241\n",
      "train_e/atom_mae: 0.000137\n",
      "train_e/atom_rmse: 0.000189\n",
      "train_f_mae: 0.015211\n",
      "train_f_rmse: 0.028370\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015488\n",
      "val_f_rmse: 0.028963\n",
      "##### Step: 349 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 350, Train Loss: 1.1245, Val Loss: 1.0224\n",
      "train_e/atom_mae: 0.000137\n",
      "train_e/atom_rmse: 0.000190\n",
      "train_f_mae: 0.015203\n",
      "train_f_rmse: 0.028357\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015472\n",
      "val_f_rmse: 0.028948\n",
      "##### Step: 359 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 360, Train Loss: 1.1287, Val Loss: 1.0232\n",
      "train_e/atom_mae: 0.000136\n",
      "train_e/atom_rmse: 0.000188\n",
      "train_f_mae: 0.015213\n",
      "train_f_rmse: 0.028372\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015474\n",
      "val_f_rmse: 0.028954\n",
      "##### Step: 369 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 370, Train Loss: 1.1084, Val Loss: 1.0224\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015221\n",
      "train_f_rmse: 0.028382\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015494\n",
      "val_f_rmse: 0.028984\n",
      "##### Step: 379 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 380, Train Loss: 1.1351, Val Loss: 1.0228\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015198\n",
      "train_f_rmse: 0.028360\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015464\n",
      "val_f_rmse: 0.028946\n",
      "##### Step: 389 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 390, Train Loss: 1.1249, Val Loss: 1.0209\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028394\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015471\n",
      "val_f_rmse: 0.028954\n",
      "##### Step: 399 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 400, Train Loss: 1.1205, Val Loss: 1.0226\n",
      "train_e/atom_mae: 0.000134\n",
      "train_e/atom_rmse: 0.000186\n",
      "train_f_mae: 0.015209\n",
      "train_f_rmse: 0.028371\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015462\n",
      "val_f_rmse: 0.028937\n",
      "##### Step: 9 Learning rate: 0.0001 #####\n",
      "Epoch 10, Train Loss: 1.4951, Val Loss: 1.2875\n",
      "train_e/atom_mae: 0.000167\n",
      "train_e/atom_rmse: 0.000225\n",
      "train_f_mae: 0.015231\n",
      "train_f_rmse: 0.028397\n",
      "val_e/atom_mae: 0.000186\n",
      "val_e/atom_rmse: 0.000247\n",
      "val_f_mae: 0.015526\n",
      "val_f_rmse: 0.029058\n",
      "##### Step: 19 Learning rate: 0.0001 #####\n",
      "Epoch 20, Train Loss: 1.5436, Val Loss: 1.0203\n",
      "train_e/atom_mae: 0.000218\n",
      "train_e/atom_rmse: 0.000282\n",
      "train_f_mae: 0.015246\n",
      "train_f_rmse: 0.028413\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015490\n",
      "val_f_rmse: 0.028983\n",
      "##### Step: 29 Learning rate: 9e-05 #####\n",
      "Epoch 30, Train Loss: 1.4606, Val Loss: 1.0192\n",
      "train_e/atom_mae: 0.000197\n",
      "train_e/atom_rmse: 0.000257\n",
      "train_f_mae: 0.015220\n",
      "train_f_rmse: 0.028387\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015471\n",
      "val_f_rmse: 0.028964\n",
      "##### Step: 39 Learning rate: 9e-05 #####\n",
      "Epoch 40, Train Loss: 1.4754, Val Loss: 1.0207\n",
      "train_e/atom_mae: 0.000217\n",
      "train_e/atom_rmse: 0.000282\n",
      "train_f_mae: 0.015214\n",
      "train_f_rmse: 0.028382\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015462\n",
      "val_f_rmse: 0.028942\n",
      "##### Step: 49 Learning rate: 8.1e-05 #####\n",
      "Epoch 50, Train Loss: 1.3795, Val Loss: 1.0222\n",
      "train_e/atom_mae: 0.000194\n",
      "train_e/atom_rmse: 0.000258\n",
      "train_f_mae: 0.015224\n",
      "train_f_rmse: 0.028388\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015514\n",
      "val_f_rmse: 0.028999\n",
      "##### Step: 59 Learning rate: 8.1e-05 #####\n",
      "Epoch 60, Train Loss: 1.3621, Val Loss: 1.0205\n",
      "train_e/atom_mae: 0.000187\n",
      "train_e/atom_rmse: 0.000245\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028404\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015483\n",
      "val_f_rmse: 0.028973\n",
      "##### Step: 69 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 70, Train Loss: 1.2816, Val Loss: 1.0189\n",
      "train_e/atom_mae: 0.000208\n",
      "train_e/atom_rmse: 0.000272\n",
      "train_f_mae: 0.015200\n",
      "train_f_rmse: 0.028366\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015476\n",
      "val_f_rmse: 0.028958\n",
      "##### Step: 79 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 80, Train Loss: 1.7201, Val Loss: 1.0227\n",
      "train_e/atom_mae: 0.000191\n",
      "train_e/atom_rmse: 0.000252\n",
      "train_f_mae: 0.015216\n",
      "train_f_rmse: 0.028379\n",
      "val_e/atom_mae: 0.000121\n",
      "val_e/atom_rmse: 0.000168\n",
      "val_f_mae: 0.015470\n",
      "val_f_rmse: 0.028958\n",
      "##### Step: 89 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 90, Train Loss: 1.3042, Val Loss: 1.0159\n",
      "train_e/atom_mae: 0.000183\n",
      "train_e/atom_rmse: 0.000241\n",
      "train_f_mae: 0.015212\n",
      "train_f_rmse: 0.028383\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015519\n",
      "val_f_rmse: 0.029024\n",
      "##### Step: 99 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 100, Train Loss: 1.2349, Val Loss: 1.0198\n",
      "train_e/atom_mae: 0.000170\n",
      "train_e/atom_rmse: 0.000224\n",
      "train_f_mae: 0.015223\n",
      "train_f_rmse: 0.028394\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015473\n",
      "val_f_rmse: 0.028960\n",
      "##### Step: 109 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 110, Train Loss: 1.3652, Val Loss: 1.0193\n",
      "train_e/atom_mae: 0.000181\n",
      "train_e/atom_rmse: 0.000239\n",
      "train_f_mae: 0.015218\n",
      "train_f_rmse: 0.028389\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015493\n",
      "val_f_rmse: 0.028990\n",
      "##### Step: 119 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 120, Train Loss: 1.2053, Val Loss: 1.0186\n",
      "train_e/atom_mae: 0.000172\n",
      "train_e/atom_rmse: 0.000229\n",
      "train_f_mae: 0.015217\n",
      "train_f_rmse: 0.028388\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000167\n",
      "val_f_mae: 0.015461\n",
      "val_f_rmse: 0.028958\n",
      "##### Step: 129 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 130, Train Loss: 1.1860, Val Loss: 1.0174\n",
      "train_e/atom_mae: 0.000159\n",
      "train_e/atom_rmse: 0.000214\n",
      "train_f_mae: 0.015200\n",
      "train_f_rmse: 0.028368\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015464\n",
      "val_f_rmse: 0.028966\n",
      "##### Step: 139 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 140, Train Loss: 1.2274, Val Loss: 1.0177\n",
      "train_e/atom_mae: 0.000164\n",
      "train_e/atom_rmse: 0.000219\n",
      "train_f_mae: 0.015209\n",
      "train_f_rmse: 0.028391\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015464\n",
      "val_f_rmse: 0.028978\n",
      "##### Step: 149 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 150, Train Loss: 1.2885, Val Loss: 1.0180\n",
      "train_e/atom_mae: 0.000164\n",
      "train_e/atom_rmse: 0.000218\n",
      "train_f_mae: 0.015217\n",
      "train_f_rmse: 0.028401\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015484\n",
      "val_f_rmse: 0.028992\n",
      "##### Step: 159 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 160, Train Loss: 1.2109, Val Loss: 1.0196\n",
      "train_e/atom_mae: 0.000162\n",
      "train_e/atom_rmse: 0.000215\n",
      "train_f_mae: 0.015239\n",
      "train_f_rmse: 0.028415\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015517\n",
      "val_f_rmse: 0.029015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 169 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 170, Train Loss: 1.2627, Val Loss: 1.0162\n",
      "train_e/atom_mae: 0.000152\n",
      "train_e/atom_rmse: 0.000206\n",
      "train_f_mae: 0.015221\n",
      "train_f_rmse: 0.028404\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015452\n",
      "val_f_rmse: 0.028955\n",
      "##### Step: 179 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 180, Train Loss: 1.2040, Val Loss: 1.0154\n",
      "train_e/atom_mae: 0.000158\n",
      "train_e/atom_rmse: 0.000212\n",
      "train_f_mae: 0.015210\n",
      "train_f_rmse: 0.028387\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015468\n",
      "val_f_rmse: 0.028971\n",
      "##### Step: 189 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 190, Train Loss: 1.2389, Val Loss: 1.0149\n",
      "train_e/atom_mae: 0.000153\n",
      "train_e/atom_rmse: 0.000206\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028387\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015473\n",
      "val_f_rmse: 0.028978\n",
      "##### Step: 199 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 200, Train Loss: 1.2147, Val Loss: 1.0186\n",
      "train_e/atom_mae: 0.000152\n",
      "train_e/atom_rmse: 0.000205\n",
      "train_f_mae: 0.015215\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015473\n",
      "val_f_rmse: 0.028973\n",
      "##### Step: 209 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 210, Train Loss: 1.1749, Val Loss: 1.0156\n",
      "train_e/atom_mae: 0.000150\n",
      "train_e/atom_rmse: 0.000203\n",
      "train_f_mae: 0.015223\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015450\n",
      "val_f_rmse: 0.028944\n",
      "##### Step: 219 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 220, Train Loss: 1.1478, Val Loss: 1.0219\n",
      "train_e/atom_mae: 0.000150\n",
      "train_e/atom_rmse: 0.000203\n",
      "train_f_mae: 0.015211\n",
      "train_f_rmse: 0.028394\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015487\n",
      "val_f_rmse: 0.028985\n",
      "##### Step: 229 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 230, Train Loss: 1.1633, Val Loss: 1.0139\n",
      "train_e/atom_mae: 0.000147\n",
      "train_e/atom_rmse: 0.000199\n",
      "train_f_mae: 0.015223\n",
      "train_f_rmse: 0.028404\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015499\n",
      "val_f_rmse: 0.029008\n",
      "##### Step: 239 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 240, Train Loss: 1.1349, Val Loss: 1.0145\n",
      "train_e/atom_mae: 0.000143\n",
      "train_e/atom_rmse: 0.000195\n",
      "train_f_mae: 0.015223\n",
      "train_f_rmse: 0.028403\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015472\n",
      "val_f_rmse: 0.028976\n",
      "##### Step: 249 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 250, Train Loss: 1.1399, Val Loss: 1.0143\n",
      "train_e/atom_mae: 0.000141\n",
      "train_e/atom_rmse: 0.000193\n",
      "train_f_mae: 0.015203\n",
      "train_f_rmse: 0.028386\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015445\n",
      "val_f_rmse: 0.028944\n",
      "##### Step: 259 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 260, Train Loss: 1.1481, Val Loss: 1.0183\n",
      "train_e/atom_mae: 0.000141\n",
      "train_e/atom_rmse: 0.000193\n",
      "train_f_mae: 0.015201\n",
      "train_f_rmse: 0.028379\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015499\n",
      "val_f_rmse: 0.028995\n",
      "##### Step: 269 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 270, Train Loss: 1.1281, Val Loss: 1.0152\n",
      "train_e/atom_mae: 0.000140\n",
      "train_e/atom_rmse: 0.000192\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028409\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015500\n",
      "val_f_rmse: 0.029002\n",
      "##### Step: 279 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 280, Train Loss: 1.1583, Val Loss: 1.0169\n",
      "train_e/atom_mae: 0.000142\n",
      "train_e/atom_rmse: 0.000194\n",
      "train_f_mae: 0.015218\n",
      "train_f_rmse: 0.028397\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015480\n",
      "val_f_rmse: 0.028977\n",
      "##### Step: 289 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 290, Train Loss: 1.1487, Val Loss: 1.0155\n",
      "train_e/atom_mae: 0.000138\n",
      "train_e/atom_rmse: 0.000189\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028416\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015490\n",
      "val_f_rmse: 0.029004\n",
      "##### Step: 299 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 300, Train Loss: 1.1343, Val Loss: 1.0180\n",
      "train_e/atom_mae: 0.000144\n",
      "train_e/atom_rmse: 0.000196\n",
      "train_f_mae: 0.015190\n",
      "train_f_rmse: 0.028372\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015463\n",
      "val_f_rmse: 0.028964\n",
      "##### Step: 309 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 310, Train Loss: 1.1187, Val Loss: 1.0145\n",
      "train_e/atom_mae: 0.000137\n",
      "train_e/atom_rmse: 0.000189\n",
      "train_f_mae: 0.015221\n",
      "train_f_rmse: 0.028407\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015488\n",
      "val_f_rmse: 0.029002\n",
      "##### Step: 319 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 320, Train Loss: 1.1216, Val Loss: 1.0170\n",
      "train_e/atom_mae: 0.000138\n",
      "train_e/atom_rmse: 0.000189\n",
      "train_f_mae: 0.015231\n",
      "train_f_rmse: 0.028411\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015504\n",
      "val_f_rmse: 0.029011\n",
      "##### Step: 329 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 330, Train Loss: 1.1281, Val Loss: 1.0160\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000186\n",
      "train_f_mae: 0.015213\n",
      "train_f_rmse: 0.028399\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015477\n",
      "val_f_rmse: 0.028978\n",
      "##### Step: 339 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 340, Train Loss: 1.1112, Val Loss: 1.0132\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000186\n",
      "train_f_mae: 0.015223\n",
      "train_f_rmse: 0.028411\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015493\n",
      "val_f_rmse: 0.029007\n",
      "##### Step: 349 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 350, Train Loss: 1.1129, Val Loss: 1.0129\n",
      "train_e/atom_mae: 0.000136\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015218\n",
      "train_f_rmse: 0.028410\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015484\n",
      "val_f_rmse: 0.029005\n",
      "##### Step: 359 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 360, Train Loss: 1.1452, Val Loss: 1.0143\n",
      "train_e/atom_mae: 0.000136\n",
      "train_e/atom_rmse: 0.000188\n",
      "train_f_mae: 0.015202\n",
      "train_f_rmse: 0.028388\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015494\n",
      "val_f_rmse: 0.029016\n",
      "##### Step: 369 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 370, Train Loss: 1.1092, Val Loss: 1.0157\n",
      "train_e/atom_mae: 0.000132\n",
      "train_e/atom_rmse: 0.000183\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028410\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015503\n",
      "val_f_rmse: 0.029013\n",
      "##### Step: 379 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 380, Train Loss: 1.1086, Val Loss: 1.0146\n",
      "train_e/atom_mae: 0.000134\n",
      "train_e/atom_rmse: 0.000185\n",
      "train_f_mae: 0.015220\n",
      "train_f_rmse: 0.028397\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015488\n",
      "val_f_rmse: 0.028997\n",
      "##### Step: 389 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 390, Train Loss: 1.1221, Val Loss: 1.0140\n",
      "train_e/atom_mae: 0.000131\n",
      "train_e/atom_rmse: 0.000182\n",
      "train_f_mae: 0.015224\n",
      "train_f_rmse: 0.028416\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015476\n",
      "val_f_rmse: 0.028987\n",
      "##### Step: 399 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 400, Train Loss: 1.1003, Val Loss: 1.0148\n",
      "train_e/atom_mae: 0.000131\n",
      "train_e/atom_rmse: 0.000182\n",
      "train_f_mae: 0.015210\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015469\n",
      "val_f_rmse: 0.028977\n",
      "##### Step: 9 Learning rate: 0.0001 #####\n",
      "Epoch 10, Train Loss: 1.5460, Val Loss: 1.1520\n",
      "train_e/atom_mae: 0.000171\n",
      "train_e/atom_rmse: 0.000227\n",
      "train_f_mae: 0.015214\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000167\n",
      "val_e/atom_rmse: 0.000216\n",
      "val_f_mae: 0.015249\n",
      "val_f_rmse: 0.028814\n",
      "##### Step: 19 Learning rate: 0.0001 #####\n",
      "Epoch 20, Train Loss: 1.4242, Val Loss: 1.0093\n",
      "train_e/atom_mae: 0.000212\n",
      "train_e/atom_rmse: 0.000273\n",
      "train_f_mae: 0.015194\n",
      "train_f_rmse: 0.028380\n",
      "val_e/atom_mae: 0.000120\n",
      "val_e/atom_rmse: 0.000166\n",
      "val_f_mae: 0.015424\n",
      "val_f_rmse: 0.028955\n",
      "##### Step: 29 Learning rate: 9e-05 #####\n",
      "Epoch 30, Train Loss: 1.2922, Val Loss: 1.0098\n",
      "train_e/atom_mae: 0.000212\n",
      "train_e/atom_rmse: 0.000278\n",
      "train_f_mae: 0.015222\n",
      "train_f_rmse: 0.028419\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015482\n",
      "val_f_rmse: 0.029004\n",
      "##### Step: 39 Learning rate: 9e-05 #####\n",
      "Epoch 40, Train Loss: 1.4572, Val Loss: 1.0137\n",
      "train_e/atom_mae: 0.000199\n",
      "train_e/atom_rmse: 0.000259\n",
      "train_f_mae: 0.015220\n",
      "train_f_rmse: 0.028413\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015484\n",
      "val_f_rmse: 0.029002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 49 Learning rate: 8.1e-05 #####\n",
      "Epoch 50, Train Loss: 1.2736, Val Loss: 1.0109\n",
      "train_e/atom_mae: 0.000210\n",
      "train_e/atom_rmse: 0.000276\n",
      "train_f_mae: 0.015201\n",
      "train_f_rmse: 0.028395\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015470\n",
      "val_f_rmse: 0.028982\n",
      "##### Step: 59 Learning rate: 8.1e-05 #####\n",
      "Epoch 60, Train Loss: 1.3757, Val Loss: 1.0148\n",
      "train_e/atom_mae: 0.000191\n",
      "train_e/atom_rmse: 0.000249\n",
      "train_f_mae: 0.015209\n",
      "train_f_rmse: 0.028399\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015482\n",
      "val_f_rmse: 0.029009\n",
      "##### Step: 69 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 70, Train Loss: 1.3692, Val Loss: 1.0132\n",
      "train_e/atom_mae: 0.000193\n",
      "train_e/atom_rmse: 0.000255\n",
      "train_f_mae: 0.015231\n",
      "train_f_rmse: 0.028425\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015489\n",
      "val_f_rmse: 0.029004\n",
      "##### Step: 79 Learning rate: 7.290000000000001e-05 #####\n",
      "Epoch 80, Train Loss: 1.2484, Val Loss: 1.0127\n",
      "train_e/atom_mae: 0.000183\n",
      "train_e/atom_rmse: 0.000238\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028405\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015454\n",
      "val_f_rmse: 0.028968\n",
      "##### Step: 89 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 90, Train Loss: 1.2278, Val Loss: 1.0117\n",
      "train_e/atom_mae: 0.000172\n",
      "train_e/atom_rmse: 0.000226\n",
      "train_f_mae: 0.015211\n",
      "train_f_rmse: 0.028401\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015492\n",
      "val_f_rmse: 0.029007\n",
      "##### Step: 99 Learning rate: 6.561000000000002e-05 #####\n",
      "Epoch 100, Train Loss: 1.2650, Val Loss: 1.0118\n",
      "train_e/atom_mae: 0.000178\n",
      "train_e/atom_rmse: 0.000235\n",
      "train_f_mae: 0.015204\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000165\n",
      "val_f_mae: 0.015455\n",
      "val_f_rmse: 0.028975\n",
      "##### Step: 109 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 110, Train Loss: 1.2300, Val Loss: 1.0120\n",
      "train_e/atom_mae: 0.000181\n",
      "train_e/atom_rmse: 0.000238\n",
      "train_f_mae: 0.015205\n",
      "train_f_rmse: 0.028398\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015482\n",
      "val_f_rmse: 0.029009\n",
      "##### Step: 119 Learning rate: 5.904900000000002e-05 #####\n",
      "Epoch 120, Train Loss: 1.2689, Val Loss: 1.0113\n",
      "train_e/atom_mae: 0.000174\n",
      "train_e/atom_rmse: 0.000231\n",
      "train_f_mae: 0.015198\n",
      "train_f_rmse: 0.028395\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015476\n",
      "val_f_rmse: 0.028988\n",
      "##### Step: 129 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 130, Train Loss: 1.2609, Val Loss: 1.0117\n",
      "train_e/atom_mae: 0.000163\n",
      "train_e/atom_rmse: 0.000218\n",
      "train_f_mae: 0.015220\n",
      "train_f_rmse: 0.028414\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015494\n",
      "val_f_rmse: 0.029013\n",
      "##### Step: 139 Learning rate: 5.314410000000002e-05 #####\n",
      "Epoch 140, Train Loss: 1.2104, Val Loss: 1.0099\n",
      "train_e/atom_mae: 0.000167\n",
      "train_e/atom_rmse: 0.000221\n",
      "train_f_mae: 0.015225\n",
      "train_f_rmse: 0.028424\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015486\n",
      "val_f_rmse: 0.029013\n",
      "##### Step: 149 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 150, Train Loss: 1.2152, Val Loss: 1.0110\n",
      "train_e/atom_mae: 0.000155\n",
      "train_e/atom_rmse: 0.000208\n",
      "train_f_mae: 0.015218\n",
      "train_f_rmse: 0.028414\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015471\n",
      "val_f_rmse: 0.028982\n",
      "##### Step: 159 Learning rate: 4.782969000000002e-05 #####\n",
      "Epoch 160, Train Loss: 1.2785, Val Loss: 1.0103\n",
      "train_e/atom_mae: 0.000158\n",
      "train_e/atom_rmse: 0.000210\n",
      "train_f_mae: 0.015217\n",
      "train_f_rmse: 0.028413\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015495\n",
      "val_f_rmse: 0.029023\n",
      "##### Step: 169 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 170, Train Loss: 1.2693, Val Loss: 1.0135\n",
      "train_e/atom_mae: 0.000158\n",
      "train_e/atom_rmse: 0.000211\n",
      "train_f_mae: 0.015221\n",
      "train_f_rmse: 0.028416\n",
      "val_e/atom_mae: 0.000119\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015493\n",
      "val_f_rmse: 0.029009\n",
      "##### Step: 179 Learning rate: 4.304672100000002e-05 #####\n",
      "Epoch 180, Train Loss: 1.1884, Val Loss: 1.0123\n",
      "train_e/atom_mae: 0.000157\n",
      "train_e/atom_rmse: 0.000210\n",
      "train_f_mae: 0.015203\n",
      "train_f_rmse: 0.028399\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015495\n",
      "val_f_rmse: 0.029014\n",
      "##### Step: 189 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 190, Train Loss: 1.1463, Val Loss: 1.0093\n",
      "train_e/atom_mae: 0.000151\n",
      "train_e/atom_rmse: 0.000202\n",
      "train_f_mae: 0.015235\n",
      "train_f_rmse: 0.028436\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015505\n",
      "val_f_rmse: 0.029031\n",
      "##### Step: 199 Learning rate: 3.874204890000002e-05 #####\n",
      "Epoch 200, Train Loss: 1.1527, Val Loss: 1.0095\n",
      "train_e/atom_mae: 0.000153\n",
      "train_e/atom_rmse: 0.000206\n",
      "train_f_mae: 0.015210\n",
      "train_f_rmse: 0.028402\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015472\n",
      "val_f_rmse: 0.028983\n",
      "##### Step: 209 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 210, Train Loss: 1.1544, Val Loss: 1.0073\n",
      "train_e/atom_mae: 0.000146\n",
      "train_e/atom_rmse: 0.000197\n",
      "train_f_mae: 0.015201\n",
      "train_f_rmse: 0.028395\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015490\n",
      "val_f_rmse: 0.029014\n",
      "##### Step: 219 Learning rate: 3.4867844010000016e-05 #####\n",
      "Epoch 220, Train Loss: 1.1671, Val Loss: 1.0114\n",
      "train_e/atom_mae: 0.000145\n",
      "train_e/atom_rmse: 0.000196\n",
      "train_f_mae: 0.015215\n",
      "train_f_rmse: 0.028413\n",
      "val_e/atom_mae: 0.000115\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015516\n",
      "val_f_rmse: 0.029037\n",
      "##### Step: 229 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 230, Train Loss: 1.1382, Val Loss: 1.0094\n",
      "train_e/atom_mae: 0.000144\n",
      "train_e/atom_rmse: 0.000196\n",
      "train_f_mae: 0.015203\n",
      "train_f_rmse: 0.028398\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015474\n",
      "val_f_rmse: 0.028999\n",
      "##### Step: 239 Learning rate: 3.138105960900002e-05 #####\n",
      "Epoch 240, Train Loss: 1.1360, Val Loss: 1.0120\n",
      "train_e/atom_mae: 0.000145\n",
      "train_e/atom_rmse: 0.000197\n",
      "train_f_mae: 0.015211\n",
      "train_f_rmse: 0.028407\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015495\n",
      "val_f_rmse: 0.029013\n",
      "##### Step: 249 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 250, Train Loss: 1.1264, Val Loss: 1.0109\n",
      "train_e/atom_mae: 0.000140\n",
      "train_e/atom_rmse: 0.000192\n",
      "train_f_mae: 0.015207\n",
      "train_f_rmse: 0.028406\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015490\n",
      "val_f_rmse: 0.029011\n",
      "##### Step: 259 Learning rate: 2.8242953648100018e-05 #####\n",
      "Epoch 260, Train Loss: 1.1462, Val Loss: 1.0104\n",
      "train_e/atom_mae: 0.000142\n",
      "train_e/atom_rmse: 0.000194\n",
      "train_f_mae: 0.015205\n",
      "train_f_rmse: 0.028406\n",
      "val_e/atom_mae: 0.000118\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015478\n",
      "val_f_rmse: 0.029006\n",
      "##### Step: 269 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 270, Train Loss: 1.1402, Val Loss: 1.0111\n",
      "train_e/atom_mae: 0.000138\n",
      "train_e/atom_rmse: 0.000189\n",
      "train_f_mae: 0.015212\n",
      "train_f_rmse: 0.028414\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015497\n",
      "val_f_rmse: 0.029023\n",
      "##### Step: 279 Learning rate: 2.5418658283290016e-05 #####\n",
      "Epoch 280, Train Loss: 1.1143, Val Loss: 1.0077\n",
      "train_e/atom_mae: 0.000142\n",
      "train_e/atom_rmse: 0.000194\n",
      "train_f_mae: 0.015216\n",
      "train_f_rmse: 0.028418\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015478\n",
      "val_f_rmse: 0.028999\n",
      "##### Step: 289 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 290, Train Loss: 1.1121, Val Loss: 1.0108\n",
      "train_e/atom_mae: 0.000136\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015213\n",
      "train_f_rmse: 0.028415\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015490\n",
      "val_f_rmse: 0.029012\n",
      "##### Step: 299 Learning rate: 2.2876792454961016e-05 #####\n",
      "Epoch 300, Train Loss: 1.1385, Val Loss: 1.0108\n",
      "train_e/atom_mae: 0.000137\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015216\n",
      "train_f_rmse: 0.028418\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015498\n",
      "val_f_rmse: 0.029018\n",
      "##### Step: 309 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 310, Train Loss: 1.1372, Val Loss: 1.0113\n",
      "train_e/atom_mae: 0.000136\n",
      "train_e/atom_rmse: 0.000187\n",
      "train_f_mae: 0.015203\n",
      "train_f_rmse: 0.028405\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015476\n",
      "val_f_rmse: 0.028996\n",
      "##### Step: 319 Learning rate: 2.0589113209464913e-05 #####\n",
      "Epoch 320, Train Loss: 1.1186, Val Loss: 1.0118\n",
      "train_e/atom_mae: 0.000134\n",
      "train_e/atom_rmse: 0.000185\n",
      "train_f_mae: 0.015217\n",
      "train_f_rmse: 0.028416\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015471\n",
      "val_f_rmse: 0.028987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 329 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 330, Train Loss: 1.1058, Val Loss: 1.0091\n",
      "train_e/atom_mae: 0.000132\n",
      "train_e/atom_rmse: 0.000183\n",
      "train_f_mae: 0.015214\n",
      "train_f_rmse: 0.028418\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015481\n",
      "val_f_rmse: 0.029011\n",
      "##### Step: 339 Learning rate: 1.8530201888518422e-05 #####\n",
      "Epoch 340, Train Loss: 1.1036, Val Loss: 1.0086\n",
      "train_e/atom_mae: 0.000133\n",
      "train_e/atom_rmse: 0.000184\n",
      "train_f_mae: 0.015202\n",
      "train_f_rmse: 0.028400\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015473\n",
      "val_f_rmse: 0.028997\n",
      "##### Step: 349 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 350, Train Loss: 1.1049, Val Loss: 1.0093\n",
      "train_e/atom_mae: 0.000135\n",
      "train_e/atom_rmse: 0.000186\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028410\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015498\n",
      "val_f_rmse: 0.029026\n",
      "##### Step: 359 Learning rate: 1.667718169966658e-05 #####\n",
      "Epoch 360, Train Loss: 1.1135, Val Loss: 1.0120\n",
      "train_e/atom_mae: 0.000133\n",
      "train_e/atom_rmse: 0.000184\n",
      "train_f_mae: 0.015208\n",
      "train_f_rmse: 0.028405\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015474\n",
      "val_f_rmse: 0.028990\n",
      "##### Step: 369 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 370, Train Loss: 1.1097, Val Loss: 1.0090\n",
      "train_e/atom_mae: 0.000132\n",
      "train_e/atom_rmse: 0.000183\n",
      "train_f_mae: 0.015204\n",
      "train_f_rmse: 0.028403\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015474\n",
      "val_f_rmse: 0.028996\n",
      "##### Step: 379 Learning rate: 1.5009463529699922e-05 #####\n",
      "Epoch 380, Train Loss: 1.0968, Val Loss: 1.0110\n",
      "train_e/atom_mae: 0.000131\n",
      "train_e/atom_rmse: 0.000181\n",
      "train_f_mae: 0.015222\n",
      "train_f_rmse: 0.028422\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015496\n",
      "val_f_rmse: 0.029007\n",
      "##### Step: 389 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 390, Train Loss: 1.1009, Val Loss: 1.0063\n",
      "train_e/atom_mae: 0.000131\n",
      "train_e/atom_rmse: 0.000181\n",
      "train_f_mae: 0.015192\n",
      "train_f_rmse: 0.028392\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015449\n",
      "val_f_rmse: 0.028987\n",
      "##### Step: 399 Learning rate: 1.350851717672993e-05 #####\n",
      "Epoch 400, Train Loss: 1.0940, Val Loss: 1.0103\n",
      "train_e/atom_mae: 0.000129\n",
      "train_e/atom_rmse: 0.000180\n",
      "train_f_mae: 0.015209\n",
      "train_f_rmse: 0.028407\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015486\n",
      "val_f_rmse: 0.029003\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "optimizer_args = {'lr': 1e-4, 'amsgrad': True}  # step 1\n",
    "#optimizer_args = {'lr': 1e-3, 'amsgrad': True} # step 2\n",
    "scheduler_args = {'step_size': 20, 'gamma': 0.9}  # \n",
    "#scheduler_args = {'mode': 'min', 'factor': 0.8, 'patience': 10}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    task = TrainingTask(\n",
    "        model=combo_p,\n",
    "        losses=[energy_loss_4, force_loss],\n",
    "        metrics=[e_metric, f_metric],\n",
    "        device=device,\n",
    "        #optimizer_cls=torch.optim.SGD,\n",
    "        optimizer_args=optimizer_args, \n",
    "        scheduler_cls=torch.optim.lr_scheduler.StepLR, \n",
    "        #scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "        scheduler_args=scheduler_args,\n",
    "        max_grad_norm=10,\n",
    "        ema=True,\n",
    "        ema_start=10,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    task.fit(train_loader, valid_loader, epochs=400, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "970a95f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Step: 409 Learning rate: 1.2157665459056937e-05 #####\n",
      "Epoch 10, Train Loss: 1.0993, Val Loss: 1.0086\n",
      "train_e/atom_mae: 0.000129\n",
      "train_e/atom_rmse: 0.000179\n",
      "train_f_mae: 0.015212\n",
      "train_f_rmse: 0.028416\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015485\n",
      "val_f_rmse: 0.029012\n",
      "##### Step: 419 Learning rate: 1.2157665459056937e-05 #####\n",
      "Epoch 20, Train Loss: 1.1011, Val Loss: 1.0092\n",
      "train_e/atom_mae: 0.000130\n",
      "train_e/atom_rmse: 0.000180\n",
      "train_f_mae: 0.015210\n",
      "train_f_rmse: 0.028414\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015512\n",
      "val_f_rmse: 0.029045\n",
      "##### Step: 429 Learning rate: 1.0941898913151244e-05 #####\n",
      "Epoch 30, Train Loss: 1.1022, Val Loss: 1.0112\n",
      "train_e/atom_mae: 0.000130\n",
      "train_e/atom_rmse: 0.000180\n",
      "train_f_mae: 0.015199\n",
      "train_f_rmse: 0.028393\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015486\n",
      "val_f_rmse: 0.028999\n",
      "##### Step: 439 Learning rate: 1.0941898913151244e-05 #####\n",
      "Epoch 40, Train Loss: 1.1064, Val Loss: 1.0111\n",
      "train_e/atom_mae: 0.000130\n",
      "train_e/atom_rmse: 0.000181\n",
      "train_f_mae: 0.015222\n",
      "train_f_rmse: 0.028424\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015506\n",
      "val_f_rmse: 0.029032\n",
      "##### Step: 449 Learning rate: 9.84770902183612e-06 #####\n",
      "Epoch 50, Train Loss: 1.0920, Val Loss: 1.0097\n",
      "train_e/atom_mae: 0.000127\n",
      "train_e/atom_rmse: 0.000178\n",
      "train_f_mae: 0.015202\n",
      "train_f_rmse: 0.028407\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015474\n",
      "val_f_rmse: 0.029004\n",
      "##### Step: 459 Learning rate: 9.84770902183612e-06 #####\n",
      "Epoch 60, Train Loss: 1.0927, Val Loss: 1.0080\n",
      "train_e/atom_mae: 0.000128\n",
      "train_e/atom_rmse: 0.000179\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028405\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015496\n",
      "val_f_rmse: 0.029029\n",
      "##### Step: 469 Learning rate: 8.862938119652508e-06 #####\n",
      "Epoch 70, Train Loss: 1.0912, Val Loss: 1.0072\n",
      "train_e/atom_mae: 0.000127\n",
      "train_e/atom_rmse: 0.000178\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028406\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015473\n",
      "val_f_rmse: 0.029000\n",
      "##### Step: 479 Learning rate: 8.862938119652508e-06 #####\n",
      "Epoch 80, Train Loss: 1.0826, Val Loss: 1.0104\n",
      "train_e/atom_mae: 0.000127\n",
      "train_e/atom_rmse: 0.000177\n",
      "train_f_mae: 0.015204\n",
      "train_f_rmse: 0.028405\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000164\n",
      "val_f_mae: 0.015457\n",
      "val_f_rmse: 0.028970\n",
      "##### Step: 489 Learning rate: 7.976644307687257e-06 #####\n",
      "Epoch 90, Train Loss: 1.0863, Val Loss: 1.0082\n",
      "train_e/atom_mae: 0.000126\n",
      "train_e/atom_rmse: 0.000177\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028408\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015460\n",
      "val_f_rmse: 0.028982\n",
      "##### Step: 499 Learning rate: 7.976644307687257e-06 #####\n",
      "Epoch 100, Train Loss: 1.0950, Val Loss: 1.0078\n",
      "train_e/atom_mae: 0.000128\n",
      "train_e/atom_rmse: 0.000179\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028408\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015507\n",
      "val_f_rmse: 0.029036\n",
      "##### Step: 509 Learning rate: 7.1789798769185315e-06 #####\n",
      "Epoch 110, Train Loss: 1.0877, Val Loss: 1.0087\n",
      "train_e/atom_mae: 0.000126\n",
      "train_e/atom_rmse: 0.000176\n",
      "train_f_mae: 0.015218\n",
      "train_f_rmse: 0.028419\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015463\n",
      "val_f_rmse: 0.028985\n",
      "##### Step: 519 Learning rate: 7.1789798769185315e-06 #####\n",
      "Epoch 120, Train Loss: 1.0888, Val Loss: 1.0100\n",
      "train_e/atom_mae: 0.000125\n",
      "train_e/atom_rmse: 0.000175\n",
      "train_f_mae: 0.015207\n",
      "train_f_rmse: 0.028409\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015502\n",
      "val_f_rmse: 0.029020\n",
      "##### Step: 529 Learning rate: 6.461081889226678e-06 #####\n",
      "Epoch 130, Train Loss: 1.0800, Val Loss: 1.0080\n",
      "train_e/atom_mae: 0.000125\n",
      "train_e/atom_rmse: 0.000176\n",
      "train_f_mae: 0.015213\n",
      "train_f_rmse: 0.028417\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015484\n",
      "val_f_rmse: 0.029009\n",
      "##### Step: 539 Learning rate: 6.461081889226678e-06 #####\n",
      "Epoch 140, Train Loss: 1.0870, Val Loss: 1.0085\n",
      "train_e/atom_mae: 0.000126\n",
      "train_e/atom_rmse: 0.000176\n",
      "train_f_mae: 0.015206\n",
      "train_f_rmse: 0.028411\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015482\n",
      "val_f_rmse: 0.029008\n",
      "##### Step: 549 Learning rate: 5.81497370030401e-06 #####\n",
      "Epoch 150, Train Loss: 1.0762, Val Loss: 1.0077\n",
      "train_e/atom_mae: 0.000125\n",
      "train_e/atom_rmse: 0.000176\n",
      "train_f_mae: 0.015202\n",
      "train_f_rmse: 0.028403\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015488\n",
      "val_f_rmse: 0.029014\n",
      "##### Step: 559 Learning rate: 5.81497370030401e-06 #####\n",
      "Epoch 160, Train Loss: 1.0778, Val Loss: 1.0083\n",
      "train_e/atom_mae: 0.000125\n",
      "train_e/atom_rmse: 0.000175\n",
      "train_f_mae: 0.015211\n",
      "train_f_rmse: 0.028413\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015475\n",
      "val_f_rmse: 0.028995\n",
      "##### Step: 569 Learning rate: 5.23347633027361e-06 #####\n",
      "Epoch 170, Train Loss: 1.0799, Val Loss: 1.0075\n",
      "train_e/atom_mae: 0.000125\n",
      "train_e/atom_rmse: 0.000175\n",
      "train_f_mae: 0.015208\n",
      "train_f_rmse: 0.028410\n",
      "val_e/atom_mae: 0.000117\n",
      "val_e/atom_rmse: 0.000163\n",
      "val_f_mae: 0.015469\n",
      "val_f_rmse: 0.028994\n",
      "##### Step: 579 Learning rate: 5.23347633027361e-06 #####\n",
      "Epoch 180, Train Loss: 1.0853, Val Loss: 1.0070\n",
      "train_e/atom_mae: 0.000124\n",
      "train_e/atom_rmse: 0.000175\n",
      "train_f_mae: 0.015205\n",
      "train_f_rmse: 0.028406\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015478\n",
      "val_f_rmse: 0.029005\n",
      "##### Step: 589 Learning rate: 4.710128697246249e-06 #####\n",
      "Epoch 190, Train Loss: 1.0877, Val Loss: 1.0080\n",
      "train_e/atom_mae: 0.000124\n",
      "train_e/atom_rmse: 0.000174\n",
      "train_f_mae: 0.015207\n",
      "train_f_rmse: 0.028411\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015487\n",
      "val_f_rmse: 0.029014\n",
      "##### Step: 599 Learning rate: 4.710128697246249e-06 #####\n",
      "Epoch 200, Train Loss: 1.0841, Val Loss: 1.0072\n",
      "train_e/atom_mae: 0.000124\n",
      "train_e/atom_rmse: 0.000175\n",
      "train_f_mae: 0.015207\n",
      "train_f_rmse: 0.028412\n",
      "val_e/atom_mae: 0.000116\n",
      "val_e/atom_rmse: 0.000162\n",
      "val_f_mae: 0.015480\n",
      "val_f_rmse: 0.029015\n"
     ]
    }
   ],
   "source": [
    "task.fit(train_loader, valid_loader, epochs=200, screen_nan=False, val_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acfc67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.save_model('model-tmp.pth')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deaf4a73",
   "metadata": {},
   "source": [
    "torch.save(combo_p, 'model-tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93e7b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = cace.tasks.EvaluateTask(model_path='model-tmp.pth', device='cpu',\n",
    "                                    energy_key='CACE_energy', #'ewald_potential',\n",
    "                                    forces_key='CACE_forces',\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62ad042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ase_xyz = read(train_xyz_dir,':')\n",
    "pred_train = evaluator(train_ase_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37ed3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_energy = np.array([ a.info['energy'] for a in train_ase_xyz ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62f4b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_true  = np.array([ xyz.get_array('forces') for xyz in train_ase_xyz]).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d18d8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(3, 3))\n",
    "\n",
    "ax1.plot(train_f_true[:,1], pred_train['forces'][:,1], '.', color='blue', label='Train')\n",
    "\n",
    "ax1.set_xlabel('Forces [$eV/\\mathrm{\\AA}$]')\n",
    "\n",
    "ax1.set_ylabel('MLP-LR Forces [$eV/\\mathrm{\\AA}$]')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14b4f9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(3, 3))\n",
    "ax1.plot(train_energy, pred_train['energy'], '.', color='blue', label='Train')\n",
    "\n",
    "ax1.set_xlabel('Energy [$eV$]')\n",
    "\n",
    "ax1.set_ylabel('MLP-LR energy [$eV$]')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9bba4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
